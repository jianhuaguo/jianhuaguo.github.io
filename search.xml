<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PointSIFT:A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation</title>
      <link href="/2019/11/12/PointSIFT%20A%20SIFT-like%20Network%20Module%20for%203D%20Point%20Cloud%20Semantic/"/>
      <url>/2019/11/12/PointSIFT%20A%20SIFT-like%20Network%20Module%20for%203D%20Point%20Cloud%20Semantic/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>这篇论文是针对语义分割，借鉴二维的SIFT提取特征的方法，对三维局部的点有针对性的提取特征，称为OE（Orientation-encoding ）模块，另外提出用不同的数量堆叠来学习尺寸不变性。</p><a id="more"></a><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>点云</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PointSIFT%20A%20SIFT-like%20Network%20Module%20for%203D%20Point%20Cloud%20Semantic/1.png" alt="1"></p><p>如上图所示，这个框架主要由三种模块组成，Set abstraction (SA) 和feature propagation (FP)是PointNet++里的模块，本文提出的PointSIFT，主要由两个部分组成，Orientation-encoding和Scale-awareness。</p><h4 id="PointSIFT"><a href="#PointSIFT" class="headerlink" title="PointSIFT"></a>PointSIFT</h4><h5 id="Orientation-encoding"><a href="#Orientation-encoding" class="headerlink" title="Orientation-encoding"></a>Orientation-encoding</h5><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PointSIFT%20A%20SIFT-like%20Network%20Module%20for%203D%20Point%20Cloud%20Semantic/2.png" alt="2"></p><p>提取每一个点的八个象限里距离最近的点，如果超出一定范围则取自己。这样就得到$2\times 2 \times 2 \times d$的特征，再沿着xyz轴三个方向做卷积，最终得到一个$1\times 1 \times 1\times d$的特征。</p><h5 id="Scale-awareness"><a href="#Scale-awareness" class="headerlink" title="Scale-awareness"></a>Scale-awareness</h5><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PointSIFT%20A%20SIFT-like%20Network%20Module%20for%203D%20Point%20Cloud%20Semantic/3.png" alt="3"></p><p>为了学习到多尺寸的特征，堆叠不同数目的OE模块，并且最后融合。</p><h4 id="SA-Set-abstraction"><a href="#SA-Set-abstraction" class="headerlink" title="SA(Set abstraction)"></a>SA(Set abstraction)</h4><p>利用聚类得到K个中心点，将中心点周围的点赋值给中心点并计算特征，比如输入$N \times d$的特征，输出$N^{\prime} \times d^{\prime}$，N是点云所有的点数目，$N^{\prime}$是中心点的数目</p><h4 id="FP-Feature-propagation"><a href="#FP-Feature-propagation" class="headerlink" title="FP(Feature propagation)"></a>FP(Feature propagation)</h4><p>基于得到的中心点特征，上采样恢复到本来的点的数目，具体方法是线性插值</p><h5 id><a href="#" class="headerlink" title=" "></a> </h5><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PointSIFT%20A%20SIFT-like%20Network%20Module%20for%203D%20Point%20Cloud%20Semantic/4.png" alt="4"></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><p><a href="https://github.com/MVIG-SJTU/pointSIFT" target="_blank" rel="noopener">代码</a></p><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>主要想要为三维检测做预处理，但是好像没有大的三维语义分割的户外的数据库，所以还是得从二维分割入手</p><h3 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PointSIFT%20A%20SIFT-like%20Network%20Module%20for%203D%20Point%20Cloud%20Semantic/PointSIFT%20A%20SIFT-like%20Network%20Module%20for%203D%20Point%20Cloud%20Semantic.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维语义分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SEG-VoxelNet for 3D Vehicle Detection from RGB and LiDAR Data</title>
      <link href="/2019/10/24/SEG-VoxelNet%20for%203D%20Vehicle%20Detection%20from%20RGB%20and%20LiDAR%20Data/"/>
      <url>/2019/10/24/SEG-VoxelNet%20for%203D%20Vehicle%20Detection%20from%20RGB%20and%20LiDAR%20Data/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>这篇文章是对voxelnet的另一种改进，增加语义分割的信息。效果比FPN还好一点，不过FPN用的框架是PointPillar，这篇文章用的是voxelnet原来的框架。</p><a id="more"></a><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>点云和二维图片</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SEG-VoxelNet%20for%203D%20Vehicle%20Detection%20from%20RGB%20and%20LiDAR%20Data/1.png" alt="图片1"></p><p>该网络框架主要由三部分组成，二维语义分割，二维像素点和点云的匹配，体素的检测组成。</p><h4 id="SEG-Net"><a href="#SEG-Net" class="headerlink" title="SEG-Net"></a>SEG-Net</h4><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SEG-VoxelNet%20for%203D%20Vehicle%20Detection%20from%20RGB%20and%20LiDAR%20Data/2.png" alt="图片2"></p><p>用的LightNet的变种，主要有两个模块。</p><p>SCSE（Spatial Channel Squeeze and Excitation  ），大概是压缩特征的通道，增大有用信息的权重。</p><p>RFB（The Receptive Field Block ）,不同步长和卷积核的大小的空洞卷积，就是得到不同大小感受野的特征。</p><p>最后得到的结果是每一个点对于每一个类的可能性，$\left(p_{\text {car }}, p_{\text {person}}, p_{\text {other}}\right)$</p><h4 id="Alignment"><a href="#Alignment" class="headerlink" title="Alignment"></a>Alignment</h4><p>$w=\mathrm{K}[\mathrm{R} \mathbf{t}] \mathbf{p}$，w是二维上的点，k是相机的内参，R和t是物体的旋转矩阵和移动矩阵</p><h4 id="Improved-VoxelNet"><a href="#Improved-VoxelNet" class="headerlink" title="Improved-VoxelNet"></a>Improved-VoxelNet</h4><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SEG-VoxelNet%20for%203D%20Vehicle%20Detection%20from%20RGB%20and%20LiDAR%20Data/3.png" alt="图片3"></p><p>和正常的Voxel主要在提取特征不一样，多加了个语义分割的特征。每一个点的特征都是十维的，$\begin{aligned} \overline{\mathbf{p}}_{i}=&amp;\left[x_{i}, y_{i}, z_{i}, r_{i}, x_{i}-x_{c}, y_{i}-y_{c}, z_{i}-z_{c}，p_{i, c a r}, p_{i, p e r s o n}, p_{i, o t h e r}\right]^{T} \end{aligned}$，然后每一个FCN完之后都加上分割特征，可能是为了增强重要性。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>语义分割的损失网络为$L_{s e g}\left(w_{1}\right)=-\sum_{x=1}^{m} \sum_{y=1}^{n} 1\left\{S_{(x, y)}=l\right\} \log \left(S_{(x, y)}\right)$，整个网络架构除了voxelnet的损失还要考虑语义分割的损失，所以自然就成了$L_{\text {total}}(w)=\lambda L_{\text {seg}}\left(w_{1}\right)+L_{\text {voxel}}\left(w_{2}\right)$。但是由于KITTI没有分割数据集，所以只能分开训练，先训练二维分割，然后固定二维分割网络。</p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><p>用CityScapes先训练SegNet，再用Kitti的几百个分割微调SegNet，然后固定二维分割网络。</p><p>最后使用随机梯度下降法，训练70epochs.</p><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SEG-VoxelNet%20for%203D%20Vehicle%20Detection%20from%20RGB%20and%20LiDAR%20Data/4.png" alt="4"></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><p>无</p><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>直接用三维分割，和上次看多层次的结合</p><h3 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SEG-VoxelNet%20for%203D%20Vehicle%20Detection%20from%20RGB%20and%20LiDAR%20Data/1.png" target="_blank" rel="noopener">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Voxel-FPN:multi-scale voxel feature aggregation in 3D object detection from point clouds</title>
      <link href="/2019/10/09/Voxel-FPN/"/>
      <url>/2019/10/09/Voxel-FPN/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>这篇文章是对Voxnet的创新，通过划分体素时选取不同网格大小，得到不同特征，模仿二维中FPN的操作，对不同层次的特征分别检测。</p><a id="more"></a><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>点云</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Voxel-FPN%20multi-scale%20voxel%20feature%20aggregation%20in%203D%20object%20detection%20from%20point%20clouds/1.png" alt></p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Voxel-FPN%20multi-scale%20voxel%20feature%20aggregation%20in%203D%20object%20detection%20from%20point%20clouds/2.png" alt></p><p>整个网络架构主要如第一图所示，由三个部分组成，体素特征提取，多层特征融合和最后基于多个层次特征的RPN.</p><h4 id="Voxel-frature-extraction"><a href="#Voxel-frature-extraction" class="headerlink" title="Voxel frature extraction"></a>Voxel frature extraction</h4><p>和voxnet一样的方法，提取体素中每一个点的特征，经过全连接层，最大池化和融合之后，得到一个体素的特征。</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Voxel-FPN%20multi-scale%20voxel%20feature%20aggregation%20in%203D%20object%20detection%20from%20point%20clouds/3.png" alt></p><h4 id="Multi-scale-feature-aggregation"><a href="#Multi-scale-feature-aggregation" class="headerlink" title="Multi-scale feature aggregation"></a>Multi-scale feature aggregation</h4><p>得到体素的的特征之后，经过卷积提取高层的语义，同时分辨率也会降低到和下一个尺寸一样，同时将它和分辨率比较低的特征相融合，成为较低层的特征。这就是多尺寸的特征聚集层</p><h4 id><a href="#" class="headerlink" title=" "></a> </h4><h4 id="Region-Proposal-Network"><a href="#Region-Proposal-Network" class="headerlink" title="Region Proposal Network"></a>Region Proposal Network</h4><p>由于有多个层次的特征，所以每个层次的特征都可以用来检测。经过聚集的特征都可以通过卷积然后用来检测。只是不知道这个上采样的含义是什么，回头可以去掉试一试。</p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><p>Adam训练，训练160个epochs.整体训练，这是one-stage的。</p><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p>KITTI数据集</p><p>消融实验</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Voxel-FPN%20multi-scale%20voxel%20feature%20aggregation%20in%203D%20object%20detection%20from%20point%20clouds/4.png" alt="4"></p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Voxel-FPN%20multi-scale%20voxel%20feature%20aggregation%20in%203D%20object%20detection%20from%20point%20clouds/5.png" alt="5"></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><p>无</p><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>关于大小可以改变，那么初始的位置的改变是不是也会使特征更加多样化。稍微移动半个像素，重新组建特征。</p><h3 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Voxel-FPN%20multi-scale%20voxel%20feature%20aggregation%20in%203D%20object%20detection%20from%20point%20clouds/Voxel-FPN%20multi-scale%20voxel%20feature%20aggregation%20in%203D%20object%20detection%20from%20point%20clouds.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSD代码要点</title>
      <link href="/2019/08/20/SSD%E4%BB%A3%E7%A0%81%E8%A6%81%E7%82%B9/"/>
      <url>/2019/08/20/SSD%E4%BB%A3%E7%A0%81%E8%A6%81%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>本文是对SSD: Single Shot MultiBox Detector 一文的pytorch代码解读,主要工作在github的代码加了注释</p><h3 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h3><p><a href="https://github.com/amdegroot/ssd.pytorch" target="_blank" rel="noopener">https://github.com/amdegroot/ssd.pytorch</a></p><a id="more"></a><h3 id="代码架构"><a href="#代码架构" class="headerlink" title="代码架构"></a>代码架构</h3><ul><li>data<ul><li>scripts （这里有三个自动下载数据库并组装数据库的脚本）</li><li>coco.py  （组装好的coco数据库，因为暂时用不到这个数据库，就没看）</li><li>config.py   (一些超参数，包括基本路径，各个数据库的参数等)</li><li>voc0712.py （组装好的voc数据库，因为暂时用不到这个数据库，就没看）</li></ul></li><li>demo</li><li>doc</li><li>layers<ul><li>functions<ul><li>detection.py （检测时直接得到相应的框和得分）</li><li>prior_box.py （最后的结果是生成特征图上候选框在原图上相对位置）</li></ul></li><li>modules<ul><li>l2norm.py   （L2正则化）</li><li>multibox_loss.py （损失函数，很重要的一部分，包括如何匹配）</li></ul></li><li>box_utils.py（工具类，包括nms,match等）</li></ul></li><li>utils<ul><li>augmentations.py  （各种数据增强）</li></ul></li><li>eval.py</li><li>ssd.py  （搭建基本框架）</li><li>test.py</li><li>train.py  （各个流程聚合地）</li></ul><h3 id="语法点"><a href="#语法点" class="headerlink" title="语法点"></a>语法点</h3><ul><li><p>_ _new _ _()在类中用以创建实例，_ _init_ _()初始化一个实例，为了将一个类实例当做函数调用，我们需要在类中实现<strong>call_ _()方法。如A是类，a=A(),写了__call\</strong>()就可以调用a(),那为什么不直接调用函数呢，因为一般来说函数不能存储数值。<a href="https://blog.csdn.net/ll845876425/article/details/87307237" target="_blank" rel="noopener">参考链接</a></p></li><li><p>cv2.cvtColor()  颜色空间转换，如image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)，这里hsv是另一种颜色空间，h是色调，s是饱和度，v是透明度</p></li><li><p>itertools.product(*iterables[, repeat])，创建一个迭代器，repeat是一个关键字参数，指定重复生成序列的次数。如product(range(2), repeat=3) —&gt; 000 001 010 011 100 101 110 111</p></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>总的来说，代码不算复杂。损失函数那里没有细看，了解了每一段的代码，看了两天代码，觉得还是要自己写才能感触深一点，明天开始看Pointpillar的代码。我把写了注释的代码传到github上了，地址在下面，不过应该是不能跑，这个我是远程服务器代码在本地的拷贝，数据库什么也没有一起传上去。只有这些注释仅供参考。</p><h3 id="加了注释的代码"><a href="#加了注释的代码" class="headerlink" title="加了注释的代码"></a>加了注释的代码</h3><p><a href="https://github.com/jianhuaguo/SSD-annotation" target="_blank" rel="noopener">代码</a></p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SECOND:Sparsely Embedded Convolutional Detection</title>
      <link href="/2019/08/17/SECOND%20Sparsely%20Embedded%20Convolutional%20Detection/"/>
      <url>/2019/08/17/SECOND%20Sparsely%20Embedded%20Convolutional%20Detection/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>这篇文章主要是对voxel的改进，主要的贡献点在三个方面，一是导入sparse convolution将voxelnet的三维卷积改进为二维卷积，提高速度，二是提出一个角度损失函数，三是提出一个新的数据增强方式。最大的改进点应该是导入sparse convolution。</p><a id="more"></a><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>点云</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SECOND%3A%20Sparsely%20Embedded%20Convolutional%20Detection/1.png" alt="1"></p><p>这个架构主要有三个部分，和voxelnet基本差不多，就是将三维卷积改成sparse convolution，实现了实时检测。第一部分将点云划分成体素，第二部分将体素的特征进行卷积，第三个部分就是常用的RPN,RPN之后的损失增加了一项，用以方向的检测矫正</p><h4 id="Point-Cloud-Grouping"><a href="#Point-Cloud-Grouping" class="headerlink" title="Point Cloud Grouping"></a>Point Cloud Grouping</h4><p>首先预设体素数目，遍历点，不断往体素里添加点，直到体素里的点到达超参数。最后可以得到体素的坐标，体素里的点的数目。体素的大小预设是唯一的，$0.4 \times 0.2 \times 0.2$</p><p>接着用VoxelNet的VFE提取体素层面特征</p><h4 id="Sparse-Convolutional-Middle-Extractor"><a href="#Sparse-Convolutional-Middle-Extractor" class="headerlink" title="Sparse Convolutional Middle Extractor"></a>Sparse Convolutional Middle Extractor</h4><p>Sparse Convolution的思想是有很多空结点会影响计算效率，所以讲空点直接赋为零，也不考虑后面的bias。</p><p>除了常规的Sparse convolution，作者采用了submainfold convolution，进一步节省了时间，由于我没看过这两篇论文，只能按照这篇论文里的解释大概了解一下。</p><p>主要是将三维的数据压缩成一维卷积的存储，然后利用算法将输入特征的索引和输出特征的索引相对应起来</p><h4 id="Region-Proposal-Network"><a href="#Region-Proposal-Network" class="headerlink" title="Region Proposal Network"></a>Region Proposal Network</h4><p>用的常用的类似SSD的二维检测方式，不过因为对方向重新编码，最后要多得到一个关于方向的得分</p><h4 id="关于角度的损失函数"><a href="#关于角度的损失函数" class="headerlink" title="关于角度的损失函数"></a>关于角度的损失函数</h4><p>直接用弧度差的话，对于0和π实际上是同一个框，却有很大的误差，所以用了以下新的损失函数</p><p>$L_{\theta}=\operatorname{SmoothL} 1\left(\sin \left(\theta_{p}-\theta_{t}\right)\right)$，但是这么定义损失函数的话，对于方向相反的框损失函数为零，所以额外加入一个方向分类器，用以判断这个方向是正还是负。</p><p>这个有点没办法理解，因为对于方向相反的框而言，IOU和GT应该是一样的，为什么要加个分类器呢。</p><p>是不是因为计算得分的时候也需要计算具体方向</p><h4 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h4><p>为了解决正负样本不均衡的问题，用了Focal Loss ，具体为$F L\left(p_{t}\right)=-\alpha_{t}\left(1-p_{t}\right)^{\gamma} \log \left(p_{t}\right)$，α和γ是超参数，pt是分类的可能性大小</p><h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p>有一个比较新颖的方法，Sample Ground Truths from the Database ，就是从数据库找一些正样本随机加到点云去训练，同时也检测是否出现不合理的情况。</p><p>还有一些点云的翻转以及全部翻转等等</p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><p>one-stage，所以速度很快，学习率为2*10-4，adam下降方法</p><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SECOND%3A%20Sparsely%20Embedded%20Convolutional%20Detection/2.png" alt></p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SECOND%3A%20Sparsely%20Embedded%20Convolutional%20Detection/3.png" alt></p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SECOND%3A%20Sparsely%20Embedded%20Convolutional%20Detection/4.png" alt></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><p>pytorch</p><p>(<a href="https://github.com/nutonomy/second.pytorch" target="_blank" rel="noopener">https://github.com/nutonomy/second.pytorch</a>) </p><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>这篇的离散卷积看得不太明白，大概就是排除掉0，当成一维卷积吧。</p><p>接下来要考虑如何用图结构做三维检测，或者加入图片，做个深层次的融合</p><h3 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SECOND%3A%20Sparsely%20Embedded%20Convolutional%20Detection/SECOND%20Sparsely%20Embedded.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PointPillars:Fast Encoders for Object Detection from Point Clouds</title>
      <link href="/2019/08/14/PointPillars%20Fast%20Encoders%20for%20Object%20Detection%20from%20Point%20Clouds/"/>
      <url>/2019/08/14/PointPillars%20Fast%20Encoders%20for%20Object%20Detection%20from%20Point%20Clouds/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>这篇文章类似于VoxelNet，不过不是一般的处理成体素，而是将点云转换成柱状体，再转换成二维的，紧接着正常的二维检测方法，将三维独有的高度和z位置当成回归。</p><p>另一个贡献点是，这篇论文是单阶段的，双翼速度很快而且准确度也不错。</p><a id="more"></a><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>点云</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PointPillars%20Fast%20Encoders%20for%20Object%20Detection%20from%20Point%20Clouds/1.png" alt="1"></p><p>这个架构主要有三个部分，第一部分是将点云进行组织和学习形成一个二维多通道的东西，二是将上一阶段的特征进行提取更高层次的，并且在不同阶段提取特征进行融合，有点类似SSD，第三阶段就是用变形的二维检测方法来检测和回归框。</p><h4 id="Pointcloud-to-Pseudo-Image-伪图像"><a href="#Pointcloud-to-Pseudo-Image-伪图像" class="headerlink" title="Pointcloud to Pseudo-Image(伪图像)"></a>Pointcloud to Pseudo-Image(伪图像)</h4><p>将点云在xy平面上均分成各个网格$H \times W$，对网格中的每一个点可以用九维来表示（$x, y, z,r,x_{c}, y_{c}, z_{c}, x_{p} ,y_{p}$）,分别是点的坐标，反射率，点到柱状体的坐标均值的距离，点到柱状体中点的距离。比voxelnet多了后面两个，实验证明会增加0.5的map，我猜是这种人工划分的要多一点属性或者权重来描述内部的关系。</p><p>对一个点云取P个非空柱状体，对每一个柱体里取固定数目的N个点。如果多了就随机采样，少了就零填充。然后将这些点通过Pointnet，生成$C \times P \times N$的向量，C是现在每一个点的特征长度，然后在每一个柱体去做最大池化，变成$C \times P$的向量，再将这些向量送回原来的网格，整个点云这时候就可以用$H \times W \times C$的伪图像表示。</p><h4 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h4><p>如图1所示，这个部分主要有两个子部分，从上到下的生成更高级的特征，二是同时上采样，级联到最后的特征里去，主要框架和VoxelNet的那部分有点类似。</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PointPillars%20Fast%20Encoders%20for%20Object%20Detection%20from%20Point%20Clouds/2.png" alt="voxelnet的"></p><h4 id="Detection-head"><a href="#Detection-head" class="headerlink" title="Detection head"></a>Detection head</h4><p>文章指这部分用的是SSD做的主要框架，因为是二维预测框架，所以将三维框架的高度和Z坐标当成一个回归的目标。虽然不太详细，不过这篇论文有给出代码。</p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><p>one-stage，所以速度很快，学习率为2*10-4，adam下降方法</p><p>训练时候用了SECOND的数据增强方法，将GroundTruth的一些物体的子点云集，随机加到现在的点云中去。还有一些翻转之类的。</p><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PointPillars%20Fast%20Encoders%20for%20Object%20Detection%20from%20Point%20Clouds/3.png" alt="3"></p><p>在实际训练中，在KITTI上进行训练，三个类分别只有一种类型的预设框，而且车是一个网络，行人和骑车的人是一个网络，可能是由于两类目标大小相差较大。</p><p>此外还研究方格画大一点之后对速度和准确度的影响。</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PointPillars%20Fast%20Encoders%20for%20Object%20Detection%20from%20Point%20Clouds/4.png" alt="4"></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><p>pytorch</p><p>(<a href="https://github.com/nutonomy/second.pytorch" target="_blank" rel="noopener">https://github.com/nutonomy/second.pytorch</a>) </p><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>这篇论文应该算是Voxelnet的继承和改进，体素的表达方式虽然不会再局限于二维检测的成功率，但是手动的划分会不会也导致信息的丢失。我想到的改进点，</p><p>一是柱子的均匀分割入手，按照点密度有不同大小的柱子，装成伪图像的时候，可以有几个像素点的值相同。</p><p>二是柱状体再最大化不能完全表征这个柱状体，用平均或者中位数是不是会更好的描述这个柱状体的形状。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PointPillars%20Fast%20Encoders%20for%20Object%20Detection%20from%20Point%20Clouds/PointPillars%20Fast%20Encoders%20for%20Object%20Detection%20from%20Point%20Clouds.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PointFusion:Deep Sensor Fusion for 3D Bounding Box Estimation</title>
      <link href="/2019/08/13/PointFusion%20Deep%20Sensor%20Fusion%20for%203D%20Bounding%20Box%20Estimation/"/>
      <url>/2019/08/13/PointFusion%20Deep%20Sensor%20Fusion%20for%203D%20Bounding%20Box%20Estimation/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>​    本篇文章将图片特征和点云特征直接进行级联，在之后的预测步骤中框中将点云当成锚点，预测和框的偏差。实际上，还是利用了二维的检测，先限定了大概的范围。和FPointnet比，多了颜色信息，但是效果比FPointnet差很多。</p><a id="more"></a><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>RGB图片和点云</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CVPR2018-PointFusion-%20Deep%20Sensor%20Fusion%20for%203D%20Bounding%20Box%20Estimation/1.png" alt="1"></p><p>这个架构主要有三个部分，输入的数据是二维检测的框和相应的点云数据。一个Pointnet提取点云特征，一个resnet提取二维图片的特征，之后将这两种特征融合在一起的网络。</p><h4 id="Point-Cloud-Network"><a href="#Point-Cloud-Network" class="headerlink" title="Point Cloud Network"></a>Point Cloud Network</h4><p>提取点云中每一个点的特征和全局特征，基本上是pointnet的框架做了两个改变。</p><p>第一个是去掉所有的batchnorm，在实验中发现去掉batchnorm会提高准确度，作者认为归一化会对框的回归有所影响。</p><p>第二个是去掉Pointnet中受Spatial Transformer Network(STN)启发引入的T-Net模块，本来大概是解决平移不变性。作者提出直接用相机内参将候选框的中心旋转到相机坐标的Z轴。大概就是将下图的黑线移动到和蓝线平行，候选框内的点云也会发生一定的矫正</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CVPR2018-PointFusion-%20Deep%20Sensor%20Fusion%20for%203D%20Bounding%20Box%20Estimation/2.png" alt="2"></p><h4 id="Fusion-Network"><a href="#Fusion-Network" class="headerlink" title="Fusion Network"></a>Fusion Network</h4><p>直接将两个特征级联之后，提出了两种预测网络。一是图1中的D，直接回归网络，叫做Global fusion network ；二是图1中的C,称为Dense fusion network ，将各个点算出一个预测框和置信度，预测框的预测是用这个点到八个角的相对坐标，也就是3*8个数，取置信度最高的点的预测框。</p><p>第二种是最后采用的架构，损失函数为$L=\frac{1}{N} \sum_{i} \operatorname{smoothL} 1\left(\mathbf{x}_{\text { offset }}^{\mathrm{i} *}, \mathbf{x}_{\text { offset }}^{\mathrm{i}}\right)+L_{\text { score }}+L_{\mathrm{stn}}$，$L_{stn}$是正则化的损失，下面来分析$L_{score}$,</p><p>我们即希望当前置信度最高的点在框内，但是他的置信度又导致他的误差比其他点的误差大，这是一个相互矛盾的点。所以最后的损失函数设计如下</p><p>$L=\frac{1}{N} \sum\left(L_{\text { offset }}^{i} \cdot c_{i}-w \cdot \log \left(c_{i}\right)\right)+L_{\text { stn }}$，$c_i$就是当前点的置信度，w是协调的一个超参数</p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><p>在二维检测时候图片裁剪为224*224,并且采用一定的数据增强。一个框里最多取400个点。</p><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CVPR2018-PointFusion-%20Deep%20Sensor%20Fusion%20for%203D%20Bounding%20Box%20Estimation/3.png" alt="3"></p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CVPR2018-PointFusion-%20Deep%20Sensor%20Fusion%20for%203D%20Bounding%20Box%20Estimation/4.png" alt="4"></p><p>还做了一些对照试验，直接回归还是密集预测，损失函数是监督还是非监督，有没有运用运用rgb图片，已经采样的点云为多少比较合适。</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CVPR2018-PointFusion-%20Deep%20Sensor%20Fusion%20for%203D%20Bounding%20Box%20Estimation/5.png" alt></p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CVPR2018-PointFusion-%20Deep%20Sensor%20Fusion%20for%203D%20Bounding%20Box%20Estimation/6.png" alt="6"></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>想法其实很简单，主要创新点就是考虑点云中每一个点和包围框的关系。作者认为可以把二维检测的框架和这个融合在一起，形成一个端到端。</p><p>我觉得这个框的微调的方法值得借鉴</p><h3 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CVPR2018-PointFusion-%20Deep%20Sensor%20Fusion%20for%203D%20Bounding%20Box%20Estimation/CVPR2018-PointFusion-%20Deep%20Sensor%20Fusion%20for%203D%20Bounding%20Box%20Estimation.pdf" target="_blank" rel="noopener">https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CVPR2018-PointFusion-%20Deep%20Sensor%20Fusion%20for%203D%20Bounding%20Box%20Estimation/CVPR2018-PointFusion-%20Deep%20Sensor%20Fusion%20for%203D%20Bounding%20Box%20Estimation.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
            <tag> 三维深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2D-Driven 3D Object Detection in RGB-D Images</title>
      <link href="/2019/08/11/2D-Driven%203D%20Object%20Detection%20in%20RGB-D%20Images/"/>
      <url>/2019/08/11/2D-Driven%203D%20Object%20Detection%20in%20RGB-D%20Images/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>这篇论文比frustum pointnet还早了一点，也是利用二维检测的框投影到三维世界中，利用二维检测的结果缩小甚至是直接限定三维中的搜索空间。之后用法线和Manhattan Frame Estimation(MFE)来估计方向，用各个密度直方图来回归框，最后用了因子图微调各个框的得分，每一个步骤都是一种思路，挺多做法我都是第一次看见，当然也是因为我看的论文不多。</p><p>最后效果一般，论文突出了速度快。</p><a id="more"></a><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>二维图片和RGB-D 图片</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2D-Driven%203D%20Object%20Detection%20in%20RGB-D%20Images/1.png" alt="1"></p><p>整个网络架构一共四个部分，第一个是用二维的faster-rcnn先检测出来可能的框，然后拓展成一个椎体。第二个模块是给每一个物体分配一个方向，第三个是多层感知机进行回归框，最后一个模块是利用上下文信息（主要是共同出现的概率和类和类之间的距离）进行微调，下面来详细讲一下这四个模块。</p><h4 id="二维检测模块"><a href="#二维检测模块" class="headerlink" title="二维检测模块"></a>二维检测模块</h4><p>使用的是以VGG-16为基本框架的Faster R-CNN方法，然后拓展成椎体，只要三维的点的投影在二维平面的框内，这个三维的点自然就在这个椎体之内，或者说相机中心和二维的框组成的三维椎体中就包含我们想要的物体。</p><h4 id="估计三维物体的方向"><a href="#估计三维物体的方向" class="headerlink" title="估计三维物体的方向"></a>估计三维物体的方向</h4><p>提出曼哈顿结构，找到资料不多，大概就是正正方方的物体，利用Manhattan Frame Estimation (MFE)  独立的估计每一个椎体里的方向。由于是室内检测，假设每一个物体的方向调整只在z轴上旋转，角度$R$可以估计为</p><p>$\min _{\mathbf{R}, \mathbf{X}} \frac{1}{2}|\mathbf{X}-\mathbf{R} \mathbf{N}|_{F}^{2}+\lambda|\mathbf{X}|_{1,1}$</p><p>其中X是松弛变量，N为椎体中每一个点的法线的矩阵，λ是常数。</p><h4 id="包围框回归"><a href="#包围框回归" class="headerlink" title="包围框回归"></a>包围框回归</h4><p>已经得到椎体和物体的方向，以点云的中心点为原点，方向为坐标轴方向建立直角坐标系。统计xyz每一个方向的点的密度直方图（如图二所示）,对每一个类的每一个维度训练一个mlp,作为多层感知机的输入，输出包围框的大小.</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2D-Driven%203D%20Object%20Detection%20in%20RGB-D%20Images/2.png" alt="2"></p><p>一旦得到三维回归框，要给三维框一个得分，二维框的得分和三维密度得分的线性组合，三维密度得分是在所有类的点云密度上训练SVM.(The 3D point density score is computed by training a linear SVM classifier on the 3D point cloud density of the 3D cuboids for all classes. )</p><h4 id="利用上下文微调"><a href="#利用上下文微调" class="headerlink" title="利用上下文微调"></a>利用上下文微调</h4><p>假设所有的候选框的标签是符合玻尔兹曼分布(Gibbs distribution)的离散变量，就可以用这些变量当做变量节点和两个候选框之间组成因子节点，画出一个因子图，如下图。</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2D-Driven%203D%20Object%20Detection%20in%20RGB-D%20Images/3.png" alt="3"></p><p>就可以有式子，$P_{U, B}(l) \propto \exp \left(\sum_{i=1}^{n_{b}} U_{i}\left(l_{i}\right)+\sum_{(i, j) \in P} B\left(l_{i}, l_{j}\right)\right)$，nb是框的数目，U和B分别是一元和二元函数，然后要求得一个标签序列L,使得</p><p>$L=\arg \max _{l} P_{U, B}(l)$</p><p>这个式子可以引进局部边缘变量$p_u和p_b$来解决，其中$p_u和p_b$表达式如下，u代表unary,b代表binary</p><p>$\left(p_{u}, p_{b}\right)=\arg \max \sum_{i=1}^{n_{b}} \mathbb{E}_{p_{u}}\left[U_{i}\left(l_{i}\right)\right]+\sum_{(i, j) \in P} \mathbb{E}_{p_{b}}\left[B\left(l_{i}, l_{j}\right)\right]$</p><p>$p_u$表示某个框i为某标签的概率，用SVM训练，在这个实验中共有十个标签加上背景，训练的特征包括几何特征和深度特征，几何特征包括长宽高体积等，深度特征是fast-rcnn的全连接层的特征，将svm的分类得分当成先验得分。</p><p>$p_b$表示一个框为标签i，另一个框为标签j的可能性。一共有两部分组成，一个是共同出现的概率$p_0$，另一个是两个共同出现之间距离的概率$p_d$,$p_0$就是在训练集中简单的计数，$p_d$是在一对框的Hausdorff distance 下使用核密度估计，最后$p_{b}=p_{o}^{\alpha} p_{d}^{1-\alpha}$，α取0.1。</p><p>微调之后标签和原来一样的，加百分二十的分，不然减百分二十的分</p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><p>fast-rcnn用在imagenet预训练过的</p><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p>SUN RGB-D数据库中的十类</p><p>做了三个消融实验，分别是二三四三个模块去掉，结果如下图</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2D-Driven%203D%20Object%20Detection%20in%20RGB-D%20Images/4.png" alt="4"></p><p>和其他两种方法的比较，在准确度没有下降很多的情况下，速度的确快很多</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2D-Driven%203D%20Object%20Detection%20in%20RGB-D%20Images/5.png" alt="5"></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><p>引用量比较少，也没有找到代码。论文中说用的faster-rcnn代码用的caffe,之后的都是用matlab.</p><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>以2D检测为基本还是会受限于2D检测的诸多弊病，这篇论文是2017年的，之后有篇frustum也是在二维检测的结果上做，不过由于用的POINTNET，map从这篇的45升到54。</p><h3 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2D-Driven%203D%20Object%20Detection%20in%20RGB-D%20Images/CVPR2017-2D-Driven%203D%20Object%20Detection%20in%20RGB-D%20Images.pdf" target="_blank" rel="noopener">论文</a></p><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2D-Driven%203D%20Object%20Detection%20in%20RGB-D%20Images/2D-Driven%203D%20Object%20Detection%20in%20RGB-D%20Images_supp.pdf" target="_blank" rel="noopener">附录</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
            <tag> 三维深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>美亚柏科夏令营心得</title>
      <link href="/2019/08/05/%E7%BE%8E%E4%BA%9A%E6%9F%8F%E7%A7%91%E5%A4%8F%E4%BB%A4%E8%90%A5%E5%BF%83%E5%BE%97/"/>
      <url>/2019/08/05/%E7%BE%8E%E4%BA%9A%E6%9F%8F%E7%A7%91%E5%A4%8F%E4%BB%A4%E8%90%A5%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h3 id="心路总结"><a href="#心路总结" class="headerlink" title="心路总结"></a>心路总结</h3><p>​    美亚柏科一个月的实习结束了，写篇文章大概总结一下这次实习。实际上也不大算得上实习，因为我们做的东西还是没有脱离学校学的那些管理系统，做的订餐管理系统相当于一个课设。并没有从事到公司真正的业务层面，算是一个遗憾吧。让我印象最为深刻的就是那一群人，每一个人都乐观向上，会议室整天笑声都此起彼伏，一起写代码也算是有了战友情。</p><a id="more"></a><p>​    之前一直在说Spring框架，这是第一次用它，Spring的东西真的太多了，就更不用说拓展，怪不得那么多人在看Spring源码。距离明年秋招大概只有一年了，不知道这一年能不能完成学术的同时，在开发上有所作为，实习应该是暂时不能去了，学术方向还没有走上道。</p><p>​    暑假还剩下一个月，先把这个方向的十来篇论文看完，代码能力还是太差了，pytorch一个月没用，感觉又忘得差不多，加油一点吧，马上23岁了。开发暂时缓一缓，或者每天晚上看一点。先把核心技术看完，然后正式学习Spring。尽量全心全意一点吧，黄金时间就这几年了。</p><h3 id="技术总结"><a href="#技术总结" class="headerlink" title="技术总结"></a>技术总结</h3><ul><li><p>maven利用一个中央信息片段管理一个项目的构建、报告和文档等步骤，是一个项目管理工具，可以对Java项目进行构建、依赖管理。有点像python的conda。</p></li><li><p>MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以对配置和原生Map使用简单的 XML 或注解，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录.简单来说，是一个实现了java持久化接口的用来连接数据库并进行一定操作的开源框架，底层封装的就是JDBC的组件，容易上手。</p></li><li><h5 id="Spring"><a href="#Spring" class="headerlink" title="Spring"></a>Spring</h5><p>​    Spring是一个JAVA开发框架，目的是为了简化应用程序的开发。主要有两点特性，一个是IOC,一个是AOP</p><p>​    IOC，控制反转，Java是面向对象的开发，但是对对象之间的依赖和生存周期的管理常常很麻烦，Spring提出由Spring来负责控制对象的生命周期和对象间的关系，所有类的创建、销毁都有spring来控制，控制对象生存周期的不再是引用它的对象，而是spring。这就叫控制反转</p><p>​    AOP,面向切面编程，将业务逻辑和通用逻辑的代码分离，便于维护和升级，降低了业务逻辑和通用逻辑的耦合。但是也不是简单的整合，即可以通过在编译期间、装载期间或运行期间实现在不修改源代码的情况下给程序动态添加功能的一种技术。通俗点说就是把可重用的功能提取出来，然后将这些通用功能在合适的时候织入到应用程序中。</p><p>​    SpringMVC首先是一个mvc框架，就是把model,view,controller分离，springmvc是spring下的子框架.</p><p>​    不过最后使用了SpringBoot</p></li><li><p>vue,三大前端框架之一，目的让用户不再操作DOM元素，让程序员有更多的时间去关注业务逻辑</p></li></ul><p>  框架和库的区别</p><p>  ​    框架是一套完整的技术解决方案，对项目的侵入性较大，更换框架需要重新架构整个项目。</p><p>  ​    库提供一个小功能，如果这个框不能解决就可以换一个框</p><p>  MVC和mvvm</p><p>  mvc是后端的分层思想，mvvm是前端的分层思想。</p><p>  把每一个页面分成了，m,v和vm。其中vm是v和m之间的调度者，也是mvvm思想的核心。</p><p>  m是每个页面中单独的数据，v是html中的页面代码，vm是调度者，分隔了m和v，由vm提供了数据的双向绑定</p><p>用的大框架主要就这些了，还有一些短信验证码，阿里云上传图片，E-charts，分页等插件接口之类。</p><p><a href="https://github.com/jianhuaguo/meiya_vue" target="_blank" rel="noopener">前端代码</a></p><p><a href="https://github.com/jianhuaguo/meiya_springboot" target="_blank" rel="noopener">后端代码</a></p>]]></content>
      
      
      <categories>
          
          <category> 自白与反思 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Frustum PointNets for 3D Object Detection from RGB-D Data</title>
      <link href="/2019/06/29/Frustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data/"/>
      <url>/2019/06/29/Frustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>本文提出了一种结构叫做Frustum PointNets ，利用二维图片和点云数据进行三维检测，先在二维图片检测框，利用锥截体反投影到点云上。得到点云上的锥截体，接着用Pointnet变形进行语义分割，排除掉一些不用的点，解决阻塞和扰乱等问题。接着将得到的点使用Pointnet的另一变形进行回归。</p><p>为如何结合二维图片和三维点云进行点云提供了一种思路，中间加上实例分割的子网络也是实用又易于理解。</p><a id="more"></a><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Frustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data/4.png" alt="图片4"></p><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>二维图片和RGB-D数据</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Frustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data/1.png" alt="图片1"></p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Frustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data/3.png" alt="图片3"></p><p>整个网络框架主要有三个部分，锥截体生成（frustum proposal),三维实例分割(3D instance<br>segmentation)和三维包围框估计( 3D amodal bounding box estimation)</p><p>在整个网络框架中，多次转换坐标中心，对最后的结果有着重要的影响</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Frustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data/2.png" alt="图片2"></p><h4 id="frustum-proposal"><a href="#frustum-proposal" class="headerlink" title="frustum proposal"></a>frustum proposal</h4><p>利用二维检测的框和分类结构，以及相机转换的投影结果等得到一个在点云上的锥截体，旋转所有锥截体的轴使它正交于相机的坐标系，收集锥截体中的所有点</p><h4 id="3D-instance-segmentation"><a href="#3D-instance-segmentation" class="headerlink" title="3D instance segmentation"></a>3D instance segmentation</h4><p>首先更换坐标中心，这个网络的功能是读入锥截体内的点云，判断每一个是不是属于这个物体。</p><h4 id="3D-amodal-bounding-box-estimation"><a href="#3D-amodal-bounding-box-estimation" class="headerlink" title="3D amodal bounding box estimation"></a>3D amodal bounding box estimation</h4><p>得到上一个网络给的具体的点，这个子网络需要回归包围框</p><p>先用一个T-NET再次回归这个物体的中心点。接下来的网络结构相似于PointNet，不过最好输出的是回归的参数，不是分类的结果。回归的中心点利用残差的思想，就是多个网络来拟合，如下式</p><p>$C_{\text {pred}}=C_{\text {mask}}+\Delta C_{t-n e t}+\Delta C_{b o x-n e t}$</p><p>回归大小和角度的时候，事先有NS大小和NH个角度的模板，先预测预定义类的大小和角度，再预测当前的长宽高和角度，大小和角度直接用预设的选项里选一个，应该是最接近谁用谁。关于为什么损失函数还有考虑这个类的大小和角度，应该是为了健壮性，同类别的大小比较相似。（没有看代码，自己猜的），最后输出$3+4 \times N S+2 \times N H$个参数，预定义类的是size,而当前物体是长宽高，所以是3+1。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>$\begin{aligned} L_{\text {multi-task}}=&amp; L_{\text {seg}}+\lambda(L_{c 1-r e g}+L_{c 2-r e g}+L_{h-c l s}+\\ &amp; L_{h-r e g}+L_{s-c l s}+L_{s-r e g}+\gamma L_{\text {corner}} ) \end{aligned}$</p><p>λ里的顺序分别如下，T-net的回归损失，box estimation的回归损失，角度的回归损失和分类损失（因为预定义的应该是分类损失，当前物体的是回归损失），大小的分类损失和回归损失。最后一个拐角损失形式如下</p><p>$L_{\text {corner}}=\sum_{i=1}^{N S} \sum_{j=1}^{N H} \delta_{i j} \min \left\{\sum_{k=1}^{8}\left|P_{k}^{i j}-P_{k}^{<em>}\right|, \sum_{i=1}^{8}\left|P_{k}^{i j}-P_{k}^{</em> *}\right|\right\}$，加上拐角的损失</p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><p>不同数据库不一样的指标，在附录里有，太过具体。有需要再看看。</p><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p>在KITTI中做了检测和定位的实验，（定位是指在鸟瞰图的二维图中的检测结果）</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Frustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data/5.png" alt="图片5"></p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Frustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data/6.png" alt="图片6"></p><p>在SUn-RGBD中的检测结果</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Frustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data/7.png" alt="图片7"></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><p>python,tensorflow</p><p><a href="https://github.com/charlesq34/frustum-pointnets" target="_blank" rel="noopener">代码</a></p><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>有些地方点太少不能进行估计，作者提出用图片辅助。假想每一个锥截体里只有一个物体，如果有多个就会混乱，作者觉得可以提出对截锥体里多个物体进行检测的网络。</p><p>我觉得这篇论文很好的利用二维图片，又不会显得人工特征太多。而且用上了pointnet，利用体素或者投影人工特征之类都会损失信息。</p><h3 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Frustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data/CVPR2018-Frustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
            <tag> 三维深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RoarNet:A Robust 3D Object Detection based on RegiOn Approximation Refinement</title>
      <link href="/2019/06/26/RoarNet%20A%20Robust%203D%20Object%20Detection%20based%20on%20RegiOn%20Approximation%20Refinement/"/>
      <url>/2019/06/26/RoarNet%20A%20Robust%203D%20Object%20Detection%20based%20on%20RegiOn%20Approximation%20Refinement/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>这篇论文提出了RoarNet结构，利用二维图片找到三维物体的框，再进一步进行微调。由于取样多个候选框，所以对图片采样和点云采样的同步性拥有一定的鲁棒性。</p><p>实际上多次微调预测定位应该是这篇论文的关键</p><p>有点新颖的点是Geometric agreement search ，通过poseCNN和投影等原理来通过二维中的框来确定三维中的框</p><a id="more"></a><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>点云和二维图片</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RoarNet%20A%20Robust%203D%20Object%20Detection%20based%20on%20RegiOn%20Approximation%20Refinement/1.png" alt="图片1"></p><p>如上图所示，整个网络架构由两个子网络组成，RoarNet_2D和RoarNet_3D，RoarNet_3D又由RPN(region<br>proposal network )和BRN(box regression network )两个更小的网络组成。</p><p>对于一个图片，预测二维的框和三维的姿态，将二维的框，利用geometric agreement search得到三维的框，三维的候选框是圆柱体的形状，考虑到误差的存在，每一个三维物体都会有多个候选框。每一个候选框经过再次训练会得到一个新的坐标和物体得分，将这个新坐标送入下一个子网络，再进行训练最终得到回归框</p><h4 id="RoarNet-2D"><a href="#RoarNet-2D" class="headerlink" title="RoarNet_2D"></a>RoarNet_2D</h4><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RoarNet%20A%20Robust%203D%20Object%20Detection%20based%20on%20RegiOn%20Approximation%20Refinement/2.png" alt="图片2"></p><p>该子网络读入二维图片，先学习到二维框，再找到物体在三维中的框。同时还有类别</p><h5 id="geometric-agreement-search"><a href="#geometric-agreement-search" class="headerlink" title="geometric agreement search"></a>geometric agreement search</h5><p>思想是找到一个三维的框，使它在二维平面上投影与二维平面上学习到的框最接近。</p><p>一个三维的框可以由七元组决定，（$X, Y, Z, W, H, L, \Theta$），分别是坐标的三个点，长宽高以及偏转角度。而一个物体可以通过网络学习到$W, H, L, \Theta$,于是有$b_{3 D}^{c}=B\left(W, H, L, \Theta ; c, b_{2 D}\right)$，只要有参数c,就可以得到三维的框，将该三维框投影到二维平面，与已知的二维框最接近的C就是我们想要的参数C。这里的B是受约束的线性方程组。</p><p>$\begin{aligned} b_{P R O J}^{c} &amp;=T\left(b_{3 D}^{c} ; P\right) \\ c^{*} &amp;=\underset{c \in C}{\arg \max } \operatorname{IoU}\left(b_{2 D}, b_{P R O J}^{c}\right) \end{aligned}$</p><p>这个思想是PoseCNN的， PoseCNN是什么？</p><p>PoseCNN是一种用于6D物体姿态估计的新型卷积神经网络，通过二维图片估计物体的三维姿态</p><h5 id="Spatial-scattering"><a href="#Spatial-scattering" class="headerlink" title="Spatial scattering"></a>Spatial scattering</h5><p>为了提高召回率，增加候选框。对长宽高进行缩放，代入上一部分的式子，从而坐标也会变。在两个坐标之间取等长的m个，那么一个物体就会有m+1个候选框。</p><p>$\begin{aligned} b_{3 D}^{c^{<em>}} s m a l l &amp;=B\left((1-s) W,(1-s) H,(1-s) L, \Theta ; c^{</em>}, b_{2 D}\right) \\ b_{3 D}^{c^{<em>}} \operatorname{large} &amp;=B\left((1+s) W,(1+s) H,(1+s) L, \Theta ; c^{</em>}, b_{2 D}\right) \end{aligned}$</p><h4 id="RoarNet-3D"><a href="#RoarNet-3D" class="headerlink" title="RoarNet_3D"></a>RoarNet_3D</h4><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RoarNet%20A%20Robust%203D%20Object%20Detection%20based%20on%20RegiOn%20Approximation%20Refinement/3.png" alt="图片3"></p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RoarNet%20A%20Robust%203D%20Object%20Detection%20based%20on%20RegiOn%20Approximation%20Refinement/4.png" alt="图片4"></p><p>该子网络是对一个框一个框进行处理的，直接对框内的点云进行处理。在训练时候对框内随机取256个点，测试的时候取512个点。</p><p>RoarNet_3D由RPN(regionproposal network )和BRN(box regression network )两个更小的网络组成，两个子网络是两级训练的。两个子网络都是由pointnet的架构组成，只是输出不一样。RPN只预测定位和物体得分，BRN针对不一样的类回归了定位旋转角，和大小。具体的网络输出如下图</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RoarNet%20A%20Robust%203D%20Object%20Detection%20based%20on%20RegiOn%20Approximation%20Refinement/5.png" alt="图片5"></p><p>2$N_R$和4$N_C$分别是$\left(t_{r_{-} \operatorname{cls}(i)}, t_{r_{-} \operatorname{reg}(i)}\right)_{i=1}^{N_{R}}$和$\left(t_{\text {size-cls}(i)}, t_{h(i)}, t_{w(i)}, t_{l(i)}\right)_{i=1}^{N_{C}}$，针对每一类求误差，$N_R和N_C$分别是分别把π分成$N_R$个和把大小聚类成$N_C$个，变成one-hot形式。实际上每一个物体进来，应该只考虑当前类有关的参数，具体损失函数如下</p><h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><p>$L_{\mathrm{RPN}}=\lambda_{\mathrm{obj}} * L_{\mathrm{obj}}+\mathbb{1}^{\mathrm{obj}}\left[L_{\mathrm{loc}}\right]$</p><p>$L_{\mathrm{BRN}}=\mathbb{1}^{3 \mathrm{D} \mathrm{IoU}&lt;0.8}\left[L_{\mathrm{loc}}+L_{\mathrm{rot-cls}}+\mathbb{1}^{\mathrm{rot-cls}}\left[L_{\mathrm{rot}-\mathrm{reg}}\right]\right.$<br>$+L_{\mathrm{size}-\mathrm{cls}}+\mathbb{1}^{\mathrm{size-cls}}\left[L_{\mathrm{size}-\mathrm{reg}}\right] ]$</p><p>$L_{\mathrm{loc}}, L_{\text { rot-reg }},$ 和 $L_{\text { size-reg }}$是考虑回归损失，具体是huber loss</p><p>$L_{\mathrm{obj}}, L_{\mathrm{rot}-\mathrm{cls}},$ 和 $L_{\mathrm{size}-\mathrm{cls}}$是考虑分类损失，具体是交叉熵损失</p><p>对于RPN,考虑物体的损失，只有包含物体再考虑定位损失</p><p>对于BRN,对于IOU小于0.8的,考虑定位损失和当前类的损失，这个类的预测应该是之前2D网络生成的，只有确定是当前类，再考虑回归损失。</p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><p>对每一个网络512的batch训练，迭代50万次。初始10万次学习速率为5e-3，其余步骤学习速率为5e-4</p><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p>KITTI数据集中的车</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RoarNet%20A%20Robust%203D%20Object%20Detection%20based%20on%20RegiOn%20Approximation%20Refinement/6.png" alt="图片6"></p><p>因为这篇文章突出对不同传感器不同步的健壮性，所以还做了不同步的实验</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RoarNet%20A%20Robust%203D%20Object%20Detection%20based%20on%20RegiOn%20Approximation%20Refinement/7.png" alt="图片7"></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>作者认为：时间不同步可以用以对视频帧的理解</p><p>对于混合模型，如何将在二维图片投影到三维上是一个很重要的问题，这篇使用了Geometric agreement search（虽然也是借鉴别人的）是个创新点，而且效果还不错</p><h4 id="注解过的论文"><a href="#注解过的论文" class="headerlink" title="注解过的论文"></a>注解过的论文</h4><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RoarNet%20A%20Robust%203D%20Object%20Detection%20based%20on%20RegiOn%20Approximation%20Refinement/RoarNet%20A%20Robust%203D%20Object%20Detection%20based%20on%20RegiOn%20Approximation%20Refinement.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
            <tag> 三维深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images</title>
      <link href="/2019/06/21/Deep%20Sliding%20Shapes%20for%20Amodal%203D%20Object%20Detection%20in%20RGB-D%20Images/"/>
      <url>/2019/06/21/Deep%20Sliding%20Shapes%20for%20Amodal%203D%20Object%20Detection%20in%20RGB-D%20Images/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>这篇文章提出了两个网络用以三维检测</p><p>一个是RPN（region proposal network）,目标生成框；RPN有两种不一样的尺寸。输入是体素，输出候选框</p><p>第二个是ORN(object recognition network),提取并融合二维和三维的特征用以回归框和分类，从二维提取颜色特征，从三维提取几何特征</p><p>这个不是端到端的网络框架</p><a id="more"></a><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>深度图和二维图片，深度图处理成体素</p><h4 id="如何将深度图处理成三维网络的输入"><a href="#如何将深度图处理成三维网络的输入" class="headerlink" title="如何将深度图处理成三维网络的输入"></a>如何将深度图处理成三维网络的输入</h4><p>采用directional Truncated Signed Distance Function ，有方向的截断信号距离函数。</p><p>将三维空间转换成体素，每一个体素的值定义为体素中心到表面最小的值c，同时用dx,dy,dz来记录三个方向离表面最近的值，δ为体素的大小，可以用c-2δ来表示，这个体素在表面内还是在表面外.</p><p>为了计算，TSND用投影的方法计算，不使用精确的算法，虽然准确度会下降</p><h3 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h3><p>这个dxdydz在哪里用到了？</p><p>没有directional的是不是直接用中点到表面的距离，而有方向的是求得最小的三个方向？</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><h4 id="RPN（region-proposal-network）"><a href="#RPN（region-proposal-network）" class="headerlink" title="RPN（region proposal network）"></a>RPN（region proposal network）</h4><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Deep%20Sliding%20Shapes%20for%20Amodal%203D%20Object%20Detection%20in%20RGB-D%20Images/1.png" alt="图片1"></p><p>如图所示每一个特征图都会有19个锚点，第二个卷积层上。然后在接下面两层分别完成这十九个预测。再将这19个预测反投影到原图上去，找到IOU最大的GT当成GT。再判断是正样本负样本还是不考虑加入训练</p><p>训练取样是IOU大于0.35就是正，小于0.15为负样本。随机取样256个锚点，全取不会收敛。</p><p>回归的损失误差就是一个六维的元素，分别是锚和GT的中心位置和长宽高的差</p><p>损失函数也是多任务损失，分类和回归，这里的分类就只是两类，是物体或者不是物体。</p><h4 id="ORN-object-recognition-network"><a href="#ORN-object-recognition-network" class="headerlink" title="ORN(object recognition network)"></a>ORN(object recognition network)</h4><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Deep%20Sliding%20Shapes%20for%20Amodal%203D%20Object%20Detection%20in%20RGB-D%20Images/2.png" alt="图片2"></p><p>三维识别子网络</p><p> 也是用之前的TSDF当做特征，将RPN给的候选框分成$30\times 30 \times 30$的体素，经过一个深度网络得到3维的特征。</p><p>二维识别子网络</p><p>将RPN给的候选框投影到2维空间，然后就是常规的二维空间的特征提取</p><p>之后将这两个子网络进行融合</p><p>损失函数同样是分类损失和回归损失，这里的分类是二十类</p><p>还有大小的调整，如果一个锚的大小和同类的大小分布相差太多，就会降低它的权重</p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><p>多步训练，以上两个子网络是分开训练的</p><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p>评估了三维区域提议和物体识别两项指标，在NYUv2和SUN RGB-D上进行了以上两种评估，只要IOU大于0.25就算正样本</p><p>在NYUv2上进行提议评估结果如下,其中ABO是average best overlap ratio ，2D to 3D 和3D Selective Search 是其他两种获得锚的方法</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Deep%20Sliding%20Shapes%20for%20Amodal%203D%20Object%20Detection%20in%20RGB-D%20Images/3.png" alt="图片3"></p><p>在NYU v2上进行检测评估结果如下，dxdydz是有方向的TSDF size是指去掉这类不合群的大小 hha是HHA即将深度图像转换为三种不同的通道(水平差异，对地高度以及表面法向量的角度)</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Deep%20Sliding%20Shapes%20for%20Amodal%203D%20Object%20Detection%20in%20RGB-D%20Images/4.png" alt="图片4"></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>拓展深度图重要特征的学习</p><h3 id="带备注的论文"><a href="#带备注的论文" class="headerlink" title="带备注的论文"></a>带备注的论文</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Deep%20Sliding%20Shapes%20for%20Amodal%203D%20Object%20Detection%20in%20RGB-D%20Images/CVPR2016-Deep%20Sliding%20Shapes%20for%20Amodal%203D%20Object%20Detection%20in%20RGB-D%20Images.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
            <tag> 三维深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VoxelNet:End-to-End Learning for Point Cloud Based 3D Object Detection</title>
      <link href="/2019/06/19/VoxelNet%20End-to-End%20Learning%20for%20Point%20Cloud%20Based%203D%20Object%20Detection/"/>
      <url>/2019/06/19/VoxelNet%20End-to-End%20Learning%20for%20Point%20Cloud%20Based%203D%20Object%20Detection/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>检测很重要的点就是为了生成候选框，最近看的几篇论文大多学习了faster-rcnn的rpn,这就是需要获取特征，但是稀疏的点云和二维图像处理起来不太一样，在百度的那篇MV3D，是将点云投影到俯瞰图在提取特征，这篇文章是生成体素，然后提取特征。</p><p>VoxelNet的贡献点，提出了体素网络的框架，用以目标检测</p><a id="more"></a><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>输入的是点云</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p>网络架构一共有三层，最新颖的应该就是第一层</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/VoxelNet%3A%20End-to-End%20Learning%20for%20Point%20Cloud%20Based%203D%20Object%20Detection/1.png" alt="图片1"></p><h4 id="Feature-learning-network"><a href="#Feature-learning-network" class="headerlink" title="Feature learning network"></a>Feature learning network</h4><p>group:先将点云均匀划分成若干体素,将D,H,W的点云空间变成$D’<em>H’</em>W’$个体素</p><p>random sampling:设一个超参数T，对体素内有超过T点的体素，随机抽取T个点</p><p>Stacked Voxel Feature Encoding ： 对体素的点的特征的表示是这个网络的重点</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/VoxelNet%3A%20End-to-End%20Learning%20for%20Point%20Cloud%20Based%203D%20Object%20Detection/2.png" alt="图片2"></p><p>对一个体素内的各个点用一个七维向量进行表示，前四维是自身的坐标和反射率，后三维是该点与该体素中的中心点的偏差，送入全连接层，输出的特征变成m个,记做特征f。通过将一个体素内所有的点的最大池化，得到这个体素的m维特征 $f^1$。在将$f^1$级联到每一个f的后面，记做$f^2$,每一个体素的体素就可以用t个（t就是体素内点的个数，t&lt;=T)$f^2$表示。这就是一个VFE（Voxel Feature Encoding）的工作流程，该网络级联了好几层的VFE.</p><p>将VFE的最后输出送入全连接层，再经过池化层，得到这个体素的特征，长为C。</p><h4 id="Convolutional-middle-layers"><a href="#Convolutional-middle-layers" class="headerlink" title="Convolutional middle layers"></a>Convolutional middle layers</h4><p>常见的卷积层，将基于体素的特征拓展了感受野，增加了更多的信息</p><h4 id="Region-proposal-network"><a href="#Region-proposal-network" class="headerlink" title="Region proposal network"></a>Region proposal network</h4><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/VoxelNet%3A%20End-to-End%20Learning%20for%20Point%20Cloud%20Based%203D%20Object%20Detection/3.png" alt="图片3"></p><p>通过一串的卷积，上采样和级联等，得到一个768维的$H‘/2和W’/2$的特征图，也就是产生$H‘/2和W’/2$个锚点。和faster-rcnn不一样的是对同一类只使用固定大小的锚，在特征图上的锚可以反投影到点云上，利用固定的锚的大小和最近的groud truth之间的IOU，将各个锚定义成正样本，负样本或者不训练，然后可以得到正样本分类损失，负样本分类损失和回归损失。</p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><p>对每一类的物体的锚大小和IOU都略有不同</p><p>具体训练是 用随机梯度下降法</p><p>前150个epoch的学习率是0.01 最后10个0。001</p><p>batchsize为16</p><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p>KITTI数据库的某几类 车 行人和骑车的人</p><p>没有验证集的标签，所以和其他人一样把训练集分成验证集和训练集</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/VoxelNet%3A%20End-to-End%20Learning%20for%20Point%20Cloud%20Based%203D%20Object%20Detection/4.png" alt="图片4"></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>作者认为可以增加图片，多个模型的输入学习，向MV3D学习</p><p>我觉得这个还是难免有人为特征的因素，二维图片的rgb值是天然的特征，三维点云各个点的坐标是不够的，增加与中心点的距离什么也是人工特征，增加人工特征就自然失去一些潜在特征</p><h3 id="论文（自己加一些注解）"><a href="#论文（自己加一些注解）" class="headerlink" title="论文（自己加一些注解）"></a>论文（自己加一些注解）</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/VoxelNet%3A%20End-to-End%20Learning%20for%20Point%20Cloud%20Based%203D%20Object%20Detection/VoxelNet-End-to-End%20Learning%20for%20Point%20Cloud%20Based%203D%20Object%20Detection.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
            <tag> 三维深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-View 3D Object Detection Network for Autonomous Driving论文理解</title>
      <link href="/2019/06/16/Multi-View%203D%20Object%20Detection%20Network%20for%20Autonomous%20Driving%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3/"/>
      <url>/2019/06/16/Multi-View%203D%20Object%20Detection%20Network%20for%20Autonomous%20Driving%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>本文提出了一种新的架构（MV3D),用以自动驾驶中的目标检测，输入点云和RGB图片，输出预测框。整个架构主要有两个部分，一个是基于点云的鸟瞰图生成的提议框的网络，第二个是将这个提议框套在点云和RGB图像得到的特征进行融合的网络。将融合之后的特征用以计算分类误差和回归误差</p><a id="more"></a><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>数据是点云和RGB图像，然后从点云得到它的鸟瞰图（离散化的分辨率为0.1米）和前视图（front view map),处理之后如下图</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Multi-View%203D%20Object%20Detection%20Network%20for%20Autonomous%20Driving/2.png" alt="图片2"></p><p>鸟瞰图由三个特征图组成。高度图是划分成若干单元，每一个单元中最高的点；强度图是每一个单元中最高的点的反射度，密度是每一个单元的点的个数。由于高度图有很多信息，又将高度水平切成M片，每一片计算高度特征</p><p>由于点云一般很稀疏，前景图不是将它投影到平面上，而且投到柱面上。具体对应坐标如下，其中$△θ和△Φ$分别是激光束的水平和垂直分辨率。同样的前景图也取高度特征，强度特征，密度特征</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Multi-View%203D%20Object%20Detection%20Network%20for%20Autonomous%20Driving/1.png" alt="图片1"></p><p>网络的卷积层基本是用vgg16，移除了一些池化层和全连接层。由于物体经过卷积之后过小，所以在多个地方用到了双线性插值，上采样。</p><h4 id="3D-Proposal-Network-（三维候选网络）"><a href="#3D-Proposal-Network-（三维候选网络）" class="headerlink" title="3D Proposal Network （三维候选网络）"></a>3D Proposal Network （三维候选网络）</h4><p>由于鸟瞰图的诸多优点，候选框只在鸟瞰图中生成，候选框一般由六个参数（中点的位置和长宽高），中点的位置xy随机生成，长宽高是设定好的，共有四个搭配，位置z通过物体高度和相机高度（这是借助了RGB图？）。</p><p>损失函数由分类的交叉熵损失和回归损失组成</p><h4 id="Region-based-Fusion-Network"><a href="#Region-based-Fusion-Network" class="headerlink" title="Region-based Fusion Network"></a>Region-based Fusion Network</h4><p>一个融合多个特征的网络，来分类候选框和方向回归。因为不一样模块的特征分辨率不一样，采用ROI池化来得到一样长度的特征（类似金字塔）。</p><p>一般融合有三种方式，early fusion、late fusion和deep fusion.示意图如下，论文采用的是第三个，融合之后又分别经过不一样的层，再融合。</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Multi-View%203D%20Object%20Detection%20Network%20for%20Autonomous%20Driving/3.png" alt="图片3"></p><p>还采用两种策略来正则化这个融合子网络，drop-path和auxiliary loss,前者类似于drop out，以一定概率丢弃掉一些信息进行训练。后者是为了加强每一个特征的表征能力，因为前一种方法随机丢弃，所以加了辅助的网络，辅助网络的架构和权重都和主网络一样，损失和训练也是一样。测试的时候去掉，如题如下图（训练的时候有四条路径）</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Multi-View%203D%20Object%20Detection%20Network%20for%20Autonomous%20Driving/4.png" alt="图片4"></p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><p>以0.001的学习率训练10万次，再以0.0001训练20万次，</p><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p>KITTI数据库上，7481训练，7518测试，因为测试集上只有2D检测，将训练集划分成真的训练集和验证集。本论文只用了KITTI数据库的车辆部分。</p><p>一共比较了三种指标，将召回率作为评判标准，$AP_{loc}$在鸟瞰图中框的定位准确度，$AP_{3D}$框在点云中的定位准确度，$AP_{2D}$是投影到二维中的准确度。然后IOU有三种取值，分别是0.25，0.5，0.7,以下为表现</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Multi-View%203D%20Object%20Detection%20Network%20for%20Autonomous%20Driving/5.png" alt="图片5"></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><p>没有找到代码，网上有帖子说进入了商用，不能公开？</p><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>这篇论文的鸟瞰图和前景图也是手动选择的，有没有可能直接将整个点云输入进行，结合图片进行检测呢</p><h3 id="自己带备注的论文"><a href="#自己带备注的论文" class="headerlink" title="自己带备注的论文"></a>自己带备注的论文</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Multi-View%203D%20Object%20Detection%20Network%20for%20Autonomous%20Driving/Multi-View_3D_Object_CVPR_2017_paper.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
            <tag> 三维深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Object Detection in 3D Scenes Using CNNs in Multi-view Images论文解读</title>
      <link href="/2019/06/13/Object%20Detection%20in%203D%20Scenes%20Using%20CNNs%20in%20Multi-view%20Images/"/>
      <url>/2019/06/13/Object%20Detection%20in%203D%20Scenes%20Using%20CNNs%20in%20Multi-view%20Images/</url>
      
        <content type="html"><![CDATA[<h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>本篇论文对三维物体各个视点得到的二维图进行检测，再将检测结果反投射到三维空间的体素，经过后续的校准得到具体的语义分割的结果，具体输出是语义分割热力图。</p><a id="more"></a><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>3D场景的各个视角的RGB-D图</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Object%20Detection%20in%203D%20Scenes%20Using%20CNNs%20in%20Multi-view%20Images/1.png" alt="图片1"></p><p>先在RGB图片上使用二维检测的得到检测框和得分，由于我们有深度图和相机的位置、倾向度等因素，可以将这些点得到三维空间中的坐标。</p><h4 id="如何从二维图片上的点得到三维世界中的坐标"><a href="#如何从二维图片上的点得到三维世界中的坐标" class="headerlink" title="如何从二维图片上的点得到三维世界中的坐标"></a>如何从二维图片上的点得到三维世界中的坐标</h4><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Object%20Detection%20in%203D%20Scenes%20Using%20CNNs%20in%20Multi-view%20Images/2.png" alt="图片2"></p><p>这里的u和v是在二维空间上的坐标，uc和vc是图像中心的坐标（因为相机的光心穿过图像中心）。z是点在深度图中的深度，f是相机的焦距，D是从深度图的深度到三维世界中深度的转变。</p><h4 id="将三维空间转换为体素"><a href="#将三维空间转换为体素" class="headerlink" title="将三维空间转换为体素"></a>将三维空间转换为体素</h4><p>为了离散化和方便表示三维空间，把三维空间划分成了体素。以下公式为（i,j,k)体素属于C类的可能性。</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Object%20Detection%20in%203D%20Scenes%20Using%20CNNs%20in%20Multi-view%20Images/3.png" alt="图片3"></p><p>分母为所有属于该体素的点，分子是该体素所有的点属于C类的概率和</p><h4 id="两步微调statistical-filtering-和-global-adjustment"><a href="#两步微调statistical-filtering-和-global-adjustment" class="headerlink" title="两步微调statistical filtering 和 global adjustment"></a>两步微调statistical filtering 和 global adjustment</h4><p>statistiacl filtering</p><p>对于每一个点，选取和它最近的K个点，计算他和这些点的距离的均值和方差，当这些距离的平均值小于由均值和方差组成的阈值时将这个点当做内部点。（也就是它离周围这些点足够近，可以认为它与周围是同一类）</p><p>global adjustment</p><p>认为当体素足够小的时候，体素内最多只能有一个物体，选取得分最大的那个物体。</p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p>一间办公室的带有深度的四千个帧，对于本次实验随机选取了一千帧</p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>过程不是全自动的，比如微调那些。</p><p>选取帧不一定是随机的，可以用一定机制选取出更有价值的帧。</p><p>对检测不完整没有补救的办法</p><h3 id="个人看法"><a href="#个人看法" class="headerlink" title="个人看法"></a>个人看法</h3><p>这篇论文方法很简单，思路也很简单，可以作为一种辅助机制。但是对数据集要求很严格，在现实所有应用中不是都能对一个物体扫那么多帧。三维检测和二维检测最大差别就是距离了，就人判断距离而言，远小近大，但是训练过程中给的数据就有大有小，人的能力太复杂了。目前应该争取在特定的方向落地，找到能解决所有方法的检测手段固然是好，商业可等不了那么久，也不需要那么强大。</p><h3 id="论文下载"><a href="#论文下载" class="headerlink" title="论文下载"></a>论文下载</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Object%20Detection%20in%203D%20Scenes%20Using%20CNNs%20in%20Multi-view%20Images/Object%20Detection%20in%203D%20Scenes%20Using%20CNNs%20in%20Multi-view%20Images.pdf" target="_blank" rel="noopener">带注解论文下载</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sliding Shapes for 3D Object Detection in Depth Images论文解读</title>
      <link href="/2019/06/11/Sliding%20Shapes%20for%203D%20Object%20Detection%20in%20Depth%20Images/"/>
      <url>/2019/06/11/Sliding%20Shapes%20for%203D%20Object%20Detection%20in%20Depth%20Images/</url>
      
        <content type="html"><![CDATA[<p>现在整理论文的要求按照大师姐给的模板进行，也就是分成以下几点进行总结。</p><h3 id="贡献点（文章的创新点）"><a href="#贡献点（文章的创新点）" class="headerlink" title="贡献点（文章的创新点）"></a>贡献点（文章的创新点）</h3><p>使用cad模型从不同角度渲染得到点云图集，为每一个点云图训练一个svm,将同一个类的所有svm组合起来，当做这一类的分类器。也就是有一个svm认为为正样本就可以。</p><a id="more"></a><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Sliding%20Shapes%20for%203D%20Object%20Detection%20in%20Depth%20Images/1.png" alt="图片1"></p><h3 id="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"><a href="#处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）" class="headerlink" title="处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）"></a>处理数据类型（针对的数据是网格、点云或者是二维多视角图像等），是否数据校正（数据是同一姿态，还是不同姿态）</h3><p>训练：训练的时候用的是CAD模型，对每一个CAD取不一样的方向，大小，位置和摄像机的角度等，再对这些CAD从不同的角度进行渲染得到点云。假设cad是在重力方向，所以绕着z轴旋转。</p><p>测试：局部搜索（假设一个SVM的数据是CAD在某一个位置渲染得到的，测试的时候窗口也只在这个位置附近进行滑动），排除一些空框和用CAD模型的边界来取代实际的边界。</p><h3 id="网络结构（构架，损失函数等）"><a href="#网络结构（构架，损失函数等）" class="headerlink" title="网络结构（构架，损失函数等）"></a>网络结构（构架，损失函数等）</h3><p>传统方法没有什么架构。这里介绍一下它的特征子，由四个特征子组装成了一个特征。将3维空间分成0.1米宽的cell,在每一个cell上提取特征和滑动。</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Sliding%20Shapes%20for%203D%20Object%20Detection%20in%20Depth%20Images/2.png" alt="图片2"></p><p><strong>Point density feature</strong>  点密度特征，考虑到总体的点和cell内各个部分之间密度的关系</p><p><strong>3D shape feature</strong>  cell内再划分成多个体素，考虑体素内的形状特征</p><p><strong>3D normal feature</strong>  法线特征，数数的方式</p><p><strong>Truncated Signed Distance Function (TSDF)</strong>  判断这个cell是在表面之后还是表面之前</p><p>一个点云被分成很多cell,在每一个cell上执行这些特征。</p><p>之后用聚类的方式，将每一种类型的特征聚成成五十类，每一个cell的一个特征可以表示为五十维，分别是到这些聚类中心的距离，也就是每一个cell有两百个维的特征。</p><p>训练时候的特征应该也是这么提的，不过没有看到详细说明</p><h3 id="训练方式（交替迭代方式训练，多步训练，还是整体训练）"><a href="#训练方式（交替迭代方式训练，多步训练，还是整体训练）" class="headerlink" title="训练方式（交替迭代方式训练，多步训练，还是整体训练）"></a>训练方式（交替迭代方式训练，多步训练，还是整体训练）</h3><p>训练SVM:使用某一个CAD的某一个视角的点云作为正样本，负样本为数据集里随便选的，用了hard negative mining。</p><h3 id="数据集（数据集的名称，包含的模型类别和数目），和结果"><a href="#数据集（数据集的名称，包含的模型类别和数目），和结果" class="headerlink" title="数据集（数据集的名称，包含的模型类别和数目），和结果"></a>数据集（数据集的名称，包含的模型类别和数目），和结果</h3><p>RMRC dataset ，五个类，chair, toilet, bed, sofa, and table ，500个当训练，574当测试。</p><p>PASCAL VOC  这是2D的检测数据集</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Sliding%20Shapes%20for%203D%20Object%20Detection%20in%20Depth%20Images/3.png" alt="图片3"></p><h3 id="代码（运行的框架，代码的语言）"><a href="#代码（运行的框架，代码的语言）" class="headerlink" title="代码（运行的框架，代码的语言）"></a>代码（运行的框架，代码的语言）</h3><p>代码语言是用matlab写的</p><h3 id="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"><a href="#改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）" class="headerlink" title="改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）"></a>改进点（文章中自己总结的可能的下一步方向，和自己想可能的方向）</h3><p>现在的主要限制是缺少数据集。下一步方向计划从二维检测中学习，从数据中自动学习特征，和利用三维信息的上下文信息。</p><h3 id="一些相关文件下载"><a href="#一些相关文件下载" class="headerlink" title="一些相关文件下载"></a>一些相关文件下载</h3><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Sliding%20Shapes%20for%203D%20Object%20Detection%20in%20Depth%20Images/Sliding%20Shapes%20for%203D%20Object%20Detection%20in%20Depth%20Images.pdf" target="_blank" rel="noopener">论文下载</a></p><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Sliding%20Shapes%20for%203D%20Object%20Detection%20in%20Depth%20Images/Sliding%20Shapes%20for%203D%20Object%20Detection%20in%20Depth%20Images.pdf" target="_blank" rel="noopener">PPT下载</a></p><p><a href="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Sliding%20Shapes%20for%203D%20Object%20Detection%20in%20Depth%20Images/supp.pdf" target="_blank" rel="noopener">附加说明</a></p>]]></content>
      
      
      <categories>
          
          <category> 三维检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维检测 </tag>
            
            <tag> 三维深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spatial Transformer Networks论文翻译和个人理解</title>
      <link href="/2019/05/14/Spatial%20Transformer%20Networks%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
      <url>/2019/05/14/Spatial%20Transformer%20Networks%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/</url>
      
        <content type="html"><![CDATA[<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>​    卷积神经网络定义了一组十分强大的模型，但是无法在以计算和参数都有效率的方式下实现对输入数据的空间不变性。在这篇论文中，我们介绍一种新的可学习的模块，Spatial Transformer,这个模块显式地实现了在网络架构中对数据的空间变换操作。这个可微分的模块可以被插入已经存在的卷积网络架构，使神经网络主动对特征图进行空间变换，而且是以特征图本身为条件，无需任何额外的培训监督或修改优化过程。我们展示了空间转换器的使用所产生的模型能够学习平移、缩放、旋转和更一般的弯曲的不变性，从而在多个基准测试和许多类转换上获得最先进的表现。</p><a id="more"></a><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>​    在最近几年，由于采用了一个快速的，可量化的，端对端的学习框架—卷积神经网络（CNN)，计算机视觉的发展已经被极大的改变和推动。即使不是最近的发明，我们可以看到基于CNN的众多模型已经在分类、定位、语义分割和动作识别任务等方面达到了最先进的结果。</p><p>​    能够对图像进行推理的系统的一个理想特性是将物体的姿态和部分变形从纹理和形状中分离出来。在卷积神经网络中局部最大池化层的引进是为了满足这个特性，通过网络在一定程度上对特征的位置具有空间不变性。然而，由于对最大池的空间支持通常很小(如2×2像素)，这种空间不变性只在池化和卷积的深层层次结构上实现，而CNN中的中间特征图(卷积层激活)对于输入数据的大的变换并不是真正的不变性。CNN的这种限制是由于只有一个有限的、预先定义的池机制来处理数据空间排列的变化。</p><p>​    在这项工作中，我们介绍了Spatial Transformer 模块，能够被包含进标准的神经网络架构来提供空间转换能力。这个空间转换器的动作是以独立的数据例子为条件，以及在任务训练期间所习得的适当行为(没有额外监督)。与感受野是固定和局部的池化层不同，空间转换器模块是一种动态机制，通过为每个输入样本生成适当的转换，可以主动地对图像(或特征图)进行空间转换。然后这个转换是体现在整个特征图(非局部)，具体操作可以包括缩放、裁剪、旋转以及非刚性变形。这使得包含空间转换器的网络不仅可以选择图像中最相关的区域(注意)，还可以将这些区域转换为规范的、期望的姿态，从而简化后续层的推理。值得注意的是，空间变压器可以用标准的反向传播进行训练，允许对注入的模型进行端到端训练。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/spatial_transformer_network/1.png" alt></p><p>图片1：将一个空间转换器作为一个训练扭曲的MNIST数字分类的全连接网络的第一层的结果。(a)空间转换网络的输入是一个被随机平移、缩放、旋转和杂波扭曲的MNIST数字图片.(b)空间转换器的定位网络预测了一个转换应用于输入的图片(c)在应用了这个转换之后，空间转换器的输出(d)在空间转换器的输出上经过一系列的全连接层的网络产生的分类预测。这个空间转换器网络（一个包含空间转换模块的卷积神经网络）只通过类的标签进行端到端的训练，没有正确转换的答案给这个系统。</p><p>​    空间转换器可以被插入卷积神经网络帮助各种任务，例如（i)图像分类：假设一个CNN被训练来根据图像是否包含一个特定的数字来对图像进行多路分类——这个数字的位置和大小可能会随着每个样本的不同而显著变化(并且与类无关);将适当的区域裁剪出来并进行尺度规格化的空间转换器，可以简化后续的分类任务，提高分类性能，如图1(ii)共定位:给定一组包含相同(但未知)类的不同实例的图像，可以使用空间转换器在每个图像中对它们进行定位;(iii)空间注意:空间转换器可以用于需要注意机制的任务，如[11,29]，但更灵活，可以纯粹通过反向传播进行训练，无需强化学习。使用注意力的一个关键好处是，经过转换(并因此得到关注)后，可以使用分辨率较低的输入来支持分辨率较高的原始输入，从而提高计算效率。</p><p>​    本文的其余部分组织如下:第2节讨论了一些与我们自己相关的工作，我们在第3节中介绍了空间转换器的设计和实现，最后第4节，给了一些实验结果。附加的实验和实现细节在补充材料中给出，或者可以在arXiv版本中找到。</p><h4 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h4><p>​    在本节中,我们讨论了之前的相关工作,包括了神经网络模型转换的核心观点[12,13,27],学习和分析转换不变性表达[3,5,8,17,19,25],以及特征选择的注意力和检测机制[1,6,9,11,23]。</p><p>Hinton[12]的早期工作着眼于为物体的部件分配规范的参考帧，这是一个在[13]中反复出现的主题，其中二维仿射转换被建模成一个由被转换的子部件组成的产生式模型。产生式训练方案的目标(target,也就是模板)是转换后的输入图像，将输入图像与目标之间的转换作为网络的附加输入。这个结果是一个产生式模型，该模型可以通过组合零件来学习生成转换后的物体图像。Tieleman[27]进一步提出了变换部分组成的概念，其中学习的部分被显式仿射变换，并由网络预测变换。这样的生成的压缩模型能够从监督转换中学习重要的特征进行分类。</p><p>在[19]中，通过估计原始图像和转换图像的表示之间的线性关系，研究了CNN的表达方式对输入图像转换的不变性和相等性。Cohen &amp; Welling[5]分析了关于对称组之间的行为，在Gens &amp; Domingos[8]提出的框架中也利用了这种行为，得到了对对称组具有不变性的特征图。设计变换不变表示的其他尝试包括散射网络[3]和构造变换滤波器的滤波器组的CNNs[17,25]。Stollenga等人的[26]使用一种基于网络激活的策略，用来控制对相同图片的一系列前向传播的网络架构过滤器的响应，从而允许注意特定特征。在这项工作中，我们的目标是通过操作数据而不是特性提取器来实现不变的表示，这在[7]中用于集群。</p><p>具有选择性注意的神经网络通过裁剪来对数据进行处理，从而学习平移不变性。像[1,2,3]这样的工作都是经过强化学习训练来避免需要一个可微分的注意力机制。而[11]在产生式模型中利用高斯核来使用可微分注意力机制。Girshick等人的工作[9]使用区域建议算法作为注意的一种形式，[6]表明了CNN可以对显著区域进行回归。我们在本文中提出的框架可以看作是对任何空间变换的可微注意力的一般化。</p><h4 id="空间转换器"><a href="#空间转换器" class="headerlink" title="空间转换器"></a>空间转换器</h4><p>在本节中，我们将描述空间转换器的规划。这是一个可微模块，它在单向前传递过程中对特征图应用空间转换，在此过程中，转换取决于特定的输入，从而生成单个输出特征图。对于多通道输入，对每个通道应用相同的弯曲。为简单起见，在本节中，我们考虑单个转换和每个转换器的单个输出，但是我们可以归一化为多个转换，如实验所示。</p><p>空间转换器机制分为三部分，如图2所示。为了便于计算，首先一个<strong>局部化网络</strong>(3.1节)获取输入的特征图，通过多个隐藏层，输出应该应用于特征图的空间转换参数，给出了一个基于输入的转换。然后，预测的转换参数被用来创建一个<strong>采样网格</strong>，它是一组点，应该在这些点上采样输入图以生成转换后的输出。这是由网格生成器完成的，见第3.2节所示。最后，这个特征图和采样的网格被当做<strong>采样器</strong>的输入，生成输出图，输出图是采样自之前网格点的输入。</p><p>这三个部分的联合组成了一个空间转换器，并且在接下来的章节中会描述更多的细节。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/spatial_transformer_network/2.png" alt></p><p>图2：空间转换器模块的架构。输入的特征图U被传递给一个局部化网络，局部化网络回归得到转换参数θ。在V上的正则空间网格被转换成采样网格$T_θ(G)$,$T_θ（G)$被应用到U上，正如3.3节所示，产生了被扭曲的输出图V。局部化网络架构和采样机制的联合定义了一个空间转换器。</p><h5 id="3-1Localisation-Network（局部化网络）"><a href="#3-1Localisation-Network（局部化网络）" class="headerlink" title="3.1Localisation Network（局部化网络）"></a>3.1Localisation Network（局部化网络）</h5><p>局部化网络将特征图U当做输入，输出θ，θ是被用于特征图的$T_θ（G）$的参数，$θ=f_loc（U)$。θ的大小取决于转换类型的参数化形式，例如，对于一个仿射变换，θ是六维的。</p><p>局部化网络函数$f_loc()$可以采取任何形式,如全连接网络或卷积网络,但应包括一个最终回归层来产生转换参数θ。</p><h5 id="3-2Parameterised-Sampling-Grid"><a href="#3-2Parameterised-Sampling-Grid" class="headerlink" title="3.2Parameterised Sampling Grid"></a>3.2Parameterised Sampling Grid</h5><p>为了对输入特征图进行扭曲，每个输出像素都会通过一个在输入图中以特定位置为中心的采样内核来计算。<em>：</em>通过像素，我们涉及了一般化的特征图的元素，而不一定是图像。出于一般化，输出像素被定义于位于像素$G={(x_i^t,y_i^t)}$的规则网格$G = {G_i}$上，形成输出特征图$V∈R^{H’×W’×C}$，其中H’和w’为网格的高度和宽度，C为和输入相同通道数。</p><p>为了清晰的阐述,假设$T_\theta$是一个二维仿射变换$A_\theta$。我们将在下面讨论其他转换。在这种仿射情况下，基于点的转换如下。其中$(x_i^t,y_i^t)$是输出特征图规范网格的目标坐标，$(x_i^s,y_i^s)$是在原输入图中定义采样点的坐标，并且$A_\theta$是仿射变换矩阵。我们使用高度和宽度的归一化坐标，比如$-1\leqslant x_i^t,y_i^t,x_i^s,y_i^s \leqslant1$。源/目标转换和采样相当于图形中使用的标准纹理映射和坐标。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/spatial_transformer_network/3.png" alt></p><p>[1]中定义的变换允许裁剪,平移、旋转、尺度,和倾斜被应用到输入特征图,只需要局部化网络产生的6个参数($A_{\theta}$的6要素)。它允许裁剪，因为如果转换是一个收缩(即左2×2子矩阵的行列式的大小小于单位矩阵)，那么映射的规则网格将位于一个面积小于$x_i^s,y_i^s$范围的平行四边形中。与恒等变换相比，这种变换对网格的影响如图3所示。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/spatial_transformer_network/4.png" alt></p><p>图三：将参数化采样网格应用于图像U生成输出V的两个例子。(a)采样网格是规范网格，$G=T_I(G)$,其中T是恒等变换参数（b)采样网格是使用一个仿射转换$T_{\theta}(G)$扭曲一个正则网格的结果。</p><p>转换$T_ \theta$的类可能会有更多的限制，比如$A_\theta=[\begin{matrix}  s&amp;0&amp;t_x\\0 &amp;s &amp;t_y \end{matrix}]$,被用于注意力，通过不一样的$s,t_x,t_y$允许裁剪，转移，各向同性的转移。这个变换$T_\theta$也可以更加一般化，比如有八个参数的平面投影，分段仿射，或薄板样条。事实上,转换可以有任何参数化形式,提供,只要它关于参数可微——这是至关重要的是，允许梯度从采样点$T_θ(Gi)$反向传播到局部化网络的输出θ。如果转换是被设置成固定的，低维的，这会减少最终结果传播到局部化网络的复杂度。例如，一个结构化的可微分的一般的转换类，是注意力、仿射、投影和细条插样转换的超集，$T_\theta=M_\theta B$,B是一个目标的网格表达（比如在[1]中，B是齐次坐标下的正则网格G），并且$M_\theta$是一个由θ定义的矩阵。在这种情况下，可能不仅学习到一个例子的θ，同时学到了这个任务的B。</p><h5 id="3-3-Differentiable-Image-Sampling-可微分的图片采样"><a href="#3-3-Differentiable-Image-Sampling-可微分的图片采样" class="headerlink" title="3.3 Differentiable Image Sampling(可微分的图片采样)"></a>3.3 Differentiable Image Sampling(可微分的图片采样)</h5><p>为了在输入图上表现空间转换，一个采样器必须输入采样的点集和输入特征图U,输出被采样的特征图V。</p><p>在$T_\theta(G)$中的每一个$(x_i^s,y_i^s)$坐标定义输入的空间坐标，应用采样核被获取输出V中特定像素的值。可以被写作<img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/spatial_transformer_network/5.png" alt></p><p>其中$\Phi_x 和 \Phi_y$是定义图片插值的采样核的参数.$U_{nm}^c$是输入在定位（n,m)在通道c上的值，$V_i^c$是输出中在$(x_i^t,y_i^t)$在通道c的值。注意，对于输入的每个通道，采样都是相同的，因此每个通道都以相同的方式进行转换(这保持了通道之间的空间一致性)。</p><p>理论来说，任何采样核可以被用，只要关于$x_i^s和y_i^s$的梯度可以被定义。比如使用整数采样核，上式可变为</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/spatial_transformer_network/6.png" alt></p><p>可选择的，一个双线性插值也可以被使用，得到以下</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/spatial_transformer_network/7.png" alt></p><p>为了允许损失通过这种采样机制反向传播，我们可以定义关于U和g的梯度。对于以上双线性采样，偏导数为</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/spatial_transformer_network/8.png" alt></p><p>这给了我们一个可微的采样机制,不仅允许损失梯度回流到输入特征图,而且回流到采样网格坐标,因此可回流到转换参数θ和局部化网络.由于采样函数的不连续性，必须使用子梯度。这种采样机制可以在GPU上非常有效地实现，通过忽略所有输入位置的和，而只查看每个输出像素的内核支持区域。</p><h5 id="3-4-空间转换网络"><a href="#3-4-空间转换网络" class="headerlink" title="3.4 空间转换网络"></a>3.4 空间转换网络</h5><p>局部化网络、网格生成器和采样器的联合组成了空间转换器，这是一个自包含的模块，可以放在CNN架构中的任何位置，任何数量，产生空间转换器网络。该模块的计算速度非常快，不会影响训练速度，在自然地使用时只会造成很少的时间开销，在注意力模型中，由于可以应用于转换器输出的后续下采样，甚至可能会加速。</p><p>在CNN中放置空间转换器可以让网络学习如何积极地转换特征图，以帮助在训练期间将网络的总体成本降到最低。在训练过程中，如何转换每个训练样本的知识被压缩并缓存在局部化网络的权重中(以及空间转换器之前各层的权重)。对于一些任务,局部化网络的输出θ也可能是有用的，将它传播给网络的剩下部分,因为它显式地进行一个物体或者区域的编码转换,以及姿势。</p><p>还可以使用空间转换器去下采样或者过采样一个特征图,因为我们可以定义的输出尺寸$H’$和$W’$和输入尺寸H和W不一样。然而,与一个固定的采样内核,小空间(如双线性内核)的支持,将空间转换器的下采样可能导致混叠效应。</p><p>最后，在CNN中可以有多个空间转换器。将多个空间转换器放置在网络的越来越深的位置，允许对越来越抽象的表示进行转换，同时也为局部化网络提供了潜在的更有信息的表示，以便根据预测的转换参数进行转换。还可以同时使用多个空间转换器——如果在一个特征图中有多个对象或感兴趣的部分需要单独关注，这将非常有用。这种架构在纯前馈网络中的一个限制是，并行空间转换器的数量限制了网络能够建模的对象的数量。</p><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>在这篇论文中，我们提出对于神经网络独立的模块—空间转换器。该模块可以放入网络中，对特征进行显式的空间变换，为神经网络对模型数据的处理开辟了新的途径，并以端到端方式学习，而不需要对损失函数做任何改变。。虽然CNNs提供了一个令人难以置信的强大功能呢，但我们看到在多个任务中使用空间转换器可以提高精确度，从而获得最先进的性能。此外，空间转换器的回归转换参数可以作为输出，并可用于后续任务。虽然我们在这项工作中只研究前馈网络，但早期的实验表明，空间转换器在递归模型中是强大的，并且对于要求处理物体参照框架的任务是有用的。</p><h4 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a>个人理解</h4><p>空间转换器需要学习的是两个东西，一是如何转换，二是取哪些点进行转换。整个结构有三个主要部分，局部化网络，网格产生器和采样。</p><p>局部化网络是通过回归网络来产生变换的参数θ，网格产生器利用参数θ和结果坐标来得到原坐标的位置，这里使用了所谓双线性插值，然后在将得到的原坐标和传进来的原像素生成目标像素。以下贴上两张帮助理解的图</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/spatial_transformer_network/9.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/spatial_transformer_network/10.jpg" alt></p><p><a href="https://arleyzhang.github.io/articles/7c7952f0/" target="_blank" rel="noopener">分析参考链接1</a></p><p><a href="https://zhuanlan.zhihu.com/p/41738716" target="_blank" rel="noopener">分析参考链接2</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pointnet++代码解读</title>
      <link href="/2019/05/10/PointNet++%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/05/10/PointNet++%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>代码链接如下，由于服务器gpu满载，这个代码还没有跑</p><p><a href="https://github.com/erikwijmans/Pointnet2_PyTorch" target="_blank" rel="noopener"><a href="https://github.com/erikwijmans/Pointnet2_PyTorch" target="_blank" rel="noopener">https://github.com/erikwijmans/Pointnet2_PyTorch</a></a></p><a id="more"></a><h3 id="整体网络结构"><a href="#整体网络结构" class="headerlink" title="整体网络结构"></a>整体网络结构</h3><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>Pointnet2MSG是唯一的网络架构</p><h5 id="有多个PointnetSAModuleMSG"><a href="#有多个PointnetSAModuleMSG" class="headerlink" title="有多个PointnetSAModuleMSG"></a>有多个PointnetSAModuleMSG</h5><p>  论文中提到的set abstrction layer ，用以分组和组成一个个局部特征。多个是因为每一层选的centroids数目不一样。相同centeoids又采样多个半径（每一个半径对于不同的采样点数目），每一个采样和组装用的是以下函数。 </p><ul><li><p>QueryAndGroup()    用于对指定半径的采样上限的参数进行采样和分组，返回(B, C + 3, npoint, nsample)，B是batchsize，C+3是额外的特征加上xyz坐标，npoint是centroids个数，nsample是每一个centroids点取的邻居点数。</p><ul><li><p>ball_query    </p><p>获得npoint个中心点和每一个中心点对应的最多nsample个邻居点的下标，得到(B, npoint, nsample)（得到的索引可能是在所有点的数目中）</p></li><li><p>grouping_operation </p><p> 用得到的下标和总的数据去检索数据，得到(B, 3, npoint, nsample)，并在这之后将各自邻居坐标变成对应的中心点的相对坐标。</p></li><li><p>SharedMLP  </p><p>用输入的int列表组建一个相应的二维卷积组</p></li></ul></li></ul><h5 id="一个PointnetSAModule"><a href="#一个PointnetSAModule" class="headerlink" title="一个PointnetSAModule"></a>一个PointnetSAModule</h5><p> 学习层,类似于pointnet,直接调用的以上的PointnetSAModuleMSG（），但是不用分组，简单的训练。</p><h5 id="pt-utils-Seq（）"><a href="#pt-utils-Seq（）" class="headerlink" title="pt_utils.Seq（）"></a>pt_utils.Seq（）</h5><p>一串的全连接层和dropout。</p><h3 id="语法点"><a href="#语法点" class="headerlink" title="语法点"></a>语法点</h3><p>代码用到原作者自己写的接口，自定义一个用以训练的类，聚合了pytorch的诸多方法</p><p><a href="https://github.com/erikwijmans/etw_pytorch_utils" target="_blank" rel="noopener"><a href="https://github.com/erikwijmans/etw_pytorch_utils" target="_blank" rel="noopener">https://github.com/erikwijmans/etw_pytorch_utils</a></a></p><h4 id="modelnet40"><a href="#modelnet40" class="headerlink" title="modelnet40"></a>modelnet40</h4><p>modelnet40训练集有9840个物体，每一个物体有2048个点，每一个点由三维坐标表示。</p><h4 id="subprocess"><a href="#subprocess" class="headerlink" title="subprocess"></a>subprocess</h4><p>用于管理子进程的模块</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">subprocess.call()</span><br><span class="line">父进程等待子进程完成</span><br><span class="line">返回退出信息(returncode，相当于Linux exit code)</span><br><span class="line"></span><br><span class="line">subprocess.check_call()</span><br><span class="line">父进程等待子进程完成</span><br><span class="line">返回0</span><br><span class="line">检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性，可用try…except…来检查</span><br><span class="line"></span><br><span class="line">subprocess.check_output()</span><br><span class="line">父进程等待子进程完成</span><br><span class="line">返回子进程向标准输出的输出结果</span><br></pre></td></tr></table></figure><h4 id="shlex-split"><a href="#shlex-split" class="headerlink" title="shlex.split()"></a>shlex.split()</h4><p>对组装一个shell语句，执行处理这个语句。在这里用以在子进程中下载，解压，删除之类的操作。</p><h4 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h4><p>增强函数的功能，但是又不希望修改函数的定义，这种在代码运行期间动态增加功能的方式，称为装饰器。从另一个角度来说，装饰器是返回函数的高阶函数。并且常与@语法一起合用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(func)</span>:</span></span><br><span class="line"><span class="meta">     @functools.wraps(func)#把原始函数的属性赋值给新的函数 比如_ _name_ _（）之类的</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kw)</span>:</span></span><br><span class="line">        print(<span class="string">'call %s():'</span> % func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kw)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@log</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">now</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'2015-3-25'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>now()  <span class="comment">#相当于now = log(now)</span></span><br><span class="line">call now():</span><br><span class="line"><span class="number">2015</span><span class="number">-3</span><span class="number">-25</span></span><br></pre></td></tr></table></figure><h4 id="ModuleList"><a href="#ModuleList" class="headerlink" title="ModuleList"></a>ModuleList</h4><p>每写一个网络就需要定义一个前向传播略显繁琐，一般有两种简化方式，modulelist和Sequential.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(MyModule, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.module_list = nn.ModuleList([nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), nn.ReLU()])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span><span class="comment">#虽然不需要写具体前向传播，也需要定义相应函数</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">model = MyModule()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 三维深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维深度学习 </tag>
            
            <tag> pytorch </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python的打包和分发</title>
      <link href="/2019/05/08/Python%E7%9A%84%E6%89%93%E5%8C%85%E5%92%8C%E5%88%86%E5%8F%91/"/>
      <url>/2019/05/08/Python%E7%9A%84%E6%89%93%E5%8C%85%E5%92%8C%E5%88%86%E5%8F%91/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么需要包管理"><a href="#为什么需要包管理" class="headerlink" title="为什么需要包管理"></a>为什么需要包管理</h3><p>python的模块或者源文件可以直接复制到目标项目进行使用，但是为了更多项目调用、共享给别人使用或者上传到网络就需要打包</p><a id="more"></a><h3 id="目前主要工具"><a href="#目前主要工具" class="headerlink" title="目前主要工具"></a>目前主要工具</h3><ul><li><p>distutils             官方的库，使用setup.py来构建和安装包。2000年就停止开发了</p></li><li><p>setuptools        替代distutils的增强工具集，包含easy_install工具，支持egg格式的构建和安装。当前是包管理的核心模块。</p></li><li><p>pip                      构建在setuptools上，提供丰富的包管理功能</p></li></ul><h3 id="包的格式"><a href="#包的格式" class="headerlink" title="包的格式"></a>包的格式</h3><p>python库打包的格式包括wheel和egg。egg是setuptools在2004年引入。而Wheel是2012年定义的，它的出现是为了替代egg,现在被认为是python的二进制包的标准格式。他们本质都是zip包。以下是主要区别</p><ul><li>wheel是一种分发格式，即打包格式。Egg即是分发格式，也是一种运行时安装的格式，并且可以被直接import</li><li>Wheel文件不会包含.pyc文件</li><li>wheel使用.dist-info目录，Egg使用.egg-info目录</li></ul><h3 id="具体使用"><a href="#具体使用" class="headerlink" title="具体使用"></a>具体使用</h3><h4 id="setup-py的编写"><a href="#setup-py的编写" class="headerlink" title="setup.py的编写"></a>setup.py的编写</h4><p>python库的打包分发的关键在于编写setup.py文件，编写规则是从setuptools或者distuils模块导入setup函数，并传入各类参数进行调用。比如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line"><span class="comment"># from distutils.core import setup  </span></span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">        name=<span class="string">'demo'</span>,     <span class="comment"># 包名字</span></span><br><span class="line">        version=<span class="string">'1.0'</span>,   <span class="comment"># 包版本</span></span><br><span class="line">        description=<span class="string">'This is a test of the setup'</span>,   <span class="comment"># 简单描述</span></span><br><span class="line">        author=<span class="string">'huoty'</span>,  <span class="comment"># 作者</span></span><br><span class="line">        author_email=<span class="string">'sudohuoty@163.com'</span>,  <span class="comment"># 作者邮箱</span></span><br><span class="line">        url=<span class="string">'https://www.konghy.com'</span>,      <span class="comment"># 包的主页</span></span><br><span class="line">        packages=[<span class="string">'demo'</span>],                 <span class="comment"># 包</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>实际上还有很多其他重要参数，接下来对其中一些进行解释说明。</p><h5 id="packages"><a href="#packages" class="headerlink" title="packages"></a>packages</h5><p>指定需要处理的包目录，但是对于复杂的工程来说，可能需要添加很多的包，setuptools提供了一个find_packages()函数，它默认在与setup.py文件的同一目录下搜索各个含有__init__.py的目录作为要添加的包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">find_packages(where=<span class="string">'.'</span>, exclude=(), include=(<span class="string">'*'</span>,))</span><br><span class="line"><span class="comment">#where指定在哪个目录下搜索 exclude指定排除哪些包 include指定要包含的包</span></span><br></pre></td></tr></table></figure><h5 id="ext-modules"><a href="#ext-modules" class="headerlink" title="ext_modules"></a>ext_modules</h5><p>用于构建C和C++扩展包，是Extension实例的列表，每一个Extension实例描述了一个独立的扩展模块，扩展模块可以设置扩展包名，头文件、源文件、链接库及其路径、宏定义和编辑参数等。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">setup(</span><br><span class="line">    <span class="comment"># other arguments here...</span></span><br><span class="line">    ext_modules=[</span><br><span class="line">        Extension(<span class="string">'foo'</span>,</span><br><span class="line">                  glob(path.join(here, <span class="string">'src'</span>, <span class="string">'*.c'</span>)),</span><br><span class="line">                  libraries = [ <span class="string">'rt'</span> ],</span><br><span class="line">                  include_dirs=[numpy.get_include()])</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h5 id="install-requires"><a href="#install-requires" class="headerlink" title="install_requires"></a>install_requires</h5><p>如果依赖其他包，可以指定包和版本号，在安装包时会自动从pypi仓库中下载指定的依赖包安装</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">install_requires=[</span><br><span class="line">    <span class="string">'requests&gt;=1.0'</span>,</span><br><span class="line">    <span class="string">'flask&gt;=1.0'</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure><h4 id="setup-py使用"><a href="#setup-py使用" class="headerlink" title="setup.py使用"></a>setup.py使用</h4><p>setup.py有很多内置命令可供使用，以下查看所有支持的命令</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python setup.py --help-commands</span><br><span class="line"></span><br><span class="line">python setup.py build  <span class="comment">#构建安装时所需的所有内容 类似于编译 </span></span><br><span class="line"></span><br><span class="line">python setup.py sdist   <span class="comment">#构建源码分发包  之后就会创建dist目录，里面有相应的.tar.gz格式的文件，使用者拿到这个文件之后解压，然后执行 python setup.py insatll 相关文件就会拷贝到相应文件下，以供导入使用</span></span><br><span class="line"></span><br><span class="line"> python setup.py build_ext  <span class="comment">#对ext_modules中的设定进行编译</span></span><br></pre></td></tr></table></figure><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="http://blog.konghy.cn/2018/04/29/setup-dot-py/" target="_blank" rel="noopener">http://blog.konghy.cn/2018/04/29/setup-dot-py/</a></li><li><a href="https://lingxiankong.github.io/2013-12-23-python-setup.html" target="_blank" rel="noopener">https://lingxiankong.github.io/2013-12-23-python-setup.html</a></li><li><a href="https://docs.python.org/3/distutils/setupscript.html" target="_blank" rel="noopener">https://docs.python.org/3/distutils/setupscript.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> python学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PointNet++论文解读</title>
      <link href="/2019/04/26/PointNet++%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/04/26/PointNet++%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>​    这篇论文在PointNet论文的基础上对点云网络进行修改，主要做了两个改进。一是增加对点云局部特征的提取，做法是学习CNN的层级网络；二是增加自适应尺寸特征提取层，增强对多尺寸的特征的鲁棒性。</p><a id="more"></a><h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pointnet%2B%2B/1.png" alt></p><p>该架构一共有三个关键点，以下进行一一说明</p><h4 id="层级网络架构"><a href="#层级网络架构" class="headerlink" title="层级网络架构"></a>层级网络架构</h4><p>为了提取点云的局部特征，在子集点云采集特征，然后向上传播，越后面特征在点云上的感受野越大。该部分叫做set abstraction levels.该部分又可以分为三个子部分，sampling layer,grouping layering和PointNet layer</p><h5 id="sampling-layer"><a href="#sampling-layer" class="headerlink" title="sampling layer"></a>sampling layer</h5><p>采样层主要是在点云中选取若干关键点，本文采用的是FPS(farthest point samping)方法去产生这些关键点，被称为centorid points.</p><h5 id="grouping-layer"><a href="#grouping-layer" class="headerlink" title="grouping layer"></a>grouping layer</h5><p>该层为给的每一个centorid point生成若干相邻点，组成子区域。生成相邻点常用两种方法，一种是kNN也就是找距离最近的K个点；二是ball  query，在一定距离内的点都当成相邻点（但是指定k做为上限）。本文选的是后者，因为点云的采集往往没有均匀。</p><h5 id="pointnet-layer"><a href="#pointnet-layer" class="headerlink" title="pointnet layer"></a>pointnet layer</h5><p>将上层传来的若干子区域提取特征，不过每一个子区域的点要以该区域的centorid point当做原点，将坐标规范化。</p><h4 id="非统一密度学习"><a href="#非统一密度学习" class="headerlink" title="非统一密度学习"></a>非统一密度学习</h4><p>由于点云采集不均匀，对学习的成功率有很大影响，论文提出了自适应密度层，对不一样密度的点云有一定鲁棒性。主要有两种方法。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pointnet%2B%2B/2.png" alt></p><h5 id="MSG"><a href="#MSG" class="headerlink" title="MSG"></a>MSG</h5><p>对不一样密度的区域进行训练，然后将特征级联在一起。具体的实现方法是对点采用dropout，就是每次随机抛弃一些点，对每一次选择进行训练，然后把提取的特征级联在一起。</p><p>不过此方法有个明显的缺点，就是太耗费时间了。</p><h5 id="MRG"><a href="#MRG" class="headerlink" title="MRG"></a>MRG</h5><p>因为该网络是层级网络，把之前层的特征和当前层的特征级联在一起，在进行训练，也一定程度上考虑了不同密度点的特征，大大减少了计算量，但是在后面实验中，准确率还是不如MSG。</p><h4 id="在语义分割中，点的特征的传播"><a href="#在语义分割中，点的特征的传播" class="headerlink" title="在语义分割中，点的特征的传播"></a>在语义分割中，点的特征的传播</h4><p>由于在学习中，点是一直被下采样的。但是在语义分割中，需要对每一个点进行预测。一种方法是将所有的点都当作centorid point 然后分别训练，毫无疑问这样计算量太大了。另一种方法就是将根据当下的点进行插值，有L层特征插值得到L-1层的特征值，直到得到所有原始点的特征。具体公式如下，k取3，p取2.</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pointnet%2B%2B/3.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 三维深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pointnet代码解读</title>
      <link href="/2019/04/23/pointnet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/04/23/pointnet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>代码github链接</p><p>[]: <a href="https://github.com/fxia22/pointnet.pytorch" target="_blank" rel="noopener">https://github.com/fxia22/pointnet.pytorch</a></p><p>加注释的代码链接</p><p>[]: <a href="https://github.com/jianhuaguo/pointnet.pytorch" target="_blank" rel="noopener">https://github.com/jianhuaguo/pointnet.pytorch</a></p><a id="more"></a><h3 id="整体网络结构"><a href="#整体网络结构" class="headerlink" title="整体网络结构"></a>整体网络结构</h3><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pointnet/1.png" alt></p><p>整个分类网络结构中有四个类，类PointNetCls是总的分类器，调用了类PointNetfeat。</p><p>类PointNetfeat计算出了global feature，调用了STN3d和STNkd，STN3d是输入转换，STNkd是特征装换</p><h4 id="STN3d"><a href="#STN3d" class="headerlink" title="STN3d"></a>STN3d</h4><p>对输入的点云进行输入装换（input transform），输入的数据是bathcsize*2500(一个物体取的点的数目）*3（一个点的xyz坐标），经过层层卷积层和紧跟的全连接层，得到一个batchsize<em>9的矩阵，再1对1加上一个[1,0,0,0,1,0,0,0,1]\</em>batchsize，用view，转换成$batchsize \times 3 \times 3$的数组，返回给PointNetfeat</p><p>其中这部分的卷积网络叫做T-Net</p><h4 id="STNkd"><a href="#STNkd" class="headerlink" title="STNkd"></a>STNkd</h4><p>大致和STN3d相同，就是从T-net出来的结果是$32 \times 64 \times 64$的特征，再与一个拉平的对角矩阵相加，返回的结果是$batchsize \times k \times k$大小</p><h4 id="PointNetfeat"><a href="#PointNetfeat" class="headerlink" title="PointNetfeat"></a>PointNetfeat</h4><p>从STN得到矩阵组合[32,3,3]和原来的数据[32,2500,3]相乘，经过一个卷积层之后得到[32,64,2500],再送入STNkd，也就是特征转换（feature transform)，返回$batchsize \times k \times k$大小。再将经过卷积的[32,64,2500]与特征转换之后的结果相乘，得到[32,64,2500]。如果是分类的话将结果送回PointNetCls即可，分割将全局特征分给每一个点，还有局部特征等都返回。</p><h3 id="语法点"><a href="#语法点" class="headerlink" title="语法点"></a>语法点</h3><h4 id="Pip-install-e"><a href="#Pip-install-e" class="headerlink" title="Pip install -e ."></a>Pip install -e .</h4><p>在当前路径中的setup.py中的说明为这个工程安装依赖，称为editable installs.在本地工程路径上创造egg目录，实际上和“python setup.py develop”相差不大。</p><p>由于我已经安装了这些包，我就把setup中装包的部分注释点，要是不执行install -e . 那么整个工程的路径都很混乱。利用了python中egg的机制，这个有机会再学习。</p><h4 id="卷积和反卷积（transposed-convolution-deconvolution"><a href="#卷积和反卷积（transposed-convolution-deconvolution" class="headerlink" title="卷积和反卷积（transposed convolution/deconvolution)"></a>卷积和反卷积（transposed convolution/deconvolution)</h4><p>参考链接</p><p>[]: <a href="https://blog.csdn.net/lanadeus/article/details/82534425" target="_blank" rel="noopener">https://blog.csdn.net/lanadeus/article/details/82534425</a></p><p>卷积可以看成一个卷积矩阵和图片向量相乘得到一个值的向量，反过来将卷积矩阵的装置乘所得值的向量就能得到这个原图向量，从本质上来说卷积建立了多对一的关系，是建立input和output值的位置性关系。</p><p>卷积实际上是不可逆的，反卷积只是计算出了位置性关系的矩阵。和普通卷积相比,intput和output的关系被反向处理(转置卷积是1对多,而不是普通的多对1),才是转置卷积的本质。反卷积矩阵是被学习的。</p><h4 id="np-random-choice（）"><a href="#np-random-choice（）" class="headerlink" title="np.random.choice（）"></a>np.random.choice（）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">numpy.random.choice(a, size=<span class="literal">None</span>, replace=<span class="literal">True</span>, p=<span class="literal">None</span>)</span><br><span class="line">从一维数组中随机采样</span><br><span class="line">如果a是一个数组，就从a中采样，如果a是整数，就当做arange(a)，再从中取数</span><br><span class="line">size就是要取的数的数量</span><br></pre></td></tr></table></figure><h4 id="torch-nn-Conv1d"><a href="#torch-nn-Conv1d" class="headerlink" title="torch.nn.Conv1d"></a>torch.nn.Conv1d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">首先一维卷积即卷积时只看在最后一维上操作</span><br><span class="line">in_channels是输入的通道，也就是要等于要进行操作数据的第二维 第一维是batchsize</span><br><span class="line">out_channels是输出的通道，实际上也就是卷积核的数目</span><br><span class="line">kernel_size是卷积核的大小 一维卷积中 卷积核的宽一定等于in_channel；kernel_size也就是决定了在第三维上滑动卷积核的长</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">对于数据，输入的是（N,C，L） 输出的是（N,C，L） 其中N是是bathcsize c是数据的通道 L是数据的长度</span><br><span class="line">在这个实验中 N就是<span class="number">60</span> C是<span class="number">3</span>（x,y,z） L是<span class="number">2500</span>（一个物体采集的点的数目）</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Conv1d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>,stride=<span class="number">2</span>)</span><br><span class="line">input = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>)</span><br><span class="line">output = m(input)</span><br><span class="line">print(output.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#torch.Size([20, 33, 24])</span></span><br><span class="line">输出的<span class="number">20</span>是batchsize大小，<span class="number">33</span>是指定的输出也就是卷积核个数</span><br><span class="line"><span class="number">24</span>=(<span class="number">50</span><span class="number">-3</span>)/<span class="number">2</span>+<span class="number">1</span> 也就是一个卷积核在第三维上卷积的结果，该结果是一个一维数组</span><br></pre></td></tr></table></figure><h4 id="torch-nn-Conv2d"><a href="#torch-nn-Conv2d" class="headerlink" title="torch.nn.Conv2d"></a>torch.nn.Conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">参数和一维卷积基本一样 但是核的大小和步长可以是一个二维元组 也就是在两个维度卷积的参数可以设置不同</span><br><span class="line">输入和输出不大一样 因为这是在两个维度上卷积</span><br><span class="line">输入为（N,C_in,H,W）输出为（N,C_out,H_out，W_out） H和W是数据的长和宽  c是通道 比如图像的RGB</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Conv1d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>,<span class="number">3</span>),stride=<span class="number">2</span>)</span><br><span class="line">input = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">100</span>)</span><br><span class="line">output = m(input)</span><br><span class="line">print(output.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出torch.Size([20, 33, 24, 49])</span></span><br><span class="line"><span class="number">20</span>是batchsize自然不会变 <span class="number">33</span>是卷积核 <span class="number">24</span>和<span class="number">49</span>分别是大小为<span class="number">3</span>的卷积核在图上卷积所得结果。</span><br></pre></td></tr></table></figure><h4 id="torch-max"><a href="#torch-max" class="headerlink" title="torch.max"></a>torch.max</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)</span><br><span class="line"></span><br><span class="line">根据指定的维度返回每一维度的最大值，返回的第二个是最大值所在的下标</span><br><span class="line">在二维中 dim=<span class="number">0</span> 每一行最大 dim=<span class="number">1</span> 每一列最大 </span><br><span class="line">或者可以理解选dim所在一维的最大 比如（<span class="number">4</span>,<span class="number">3</span>） dim=<span class="number">0</span> 就是在四个中找最大 dim=<span class="number">1</span> 就是在<span class="number">3</span>个找最大</span><br><span class="line">如果keepdim=<span class="literal">True</span>的话 数据形状保持不变 但是那些不是最大的元素都去掉了</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">-1.2360</span>, <span class="number">-0.2942</span>, <span class="number">-0.1222</span>,  <span class="number">0.8475</span>],</span><br><span class="line">        [ <span class="number">1.1949</span>, <span class="number">-1.1127</span>, <span class="number">-2.2379</span>, <span class="number">-0.6702</span>],</span><br><span class="line">        [ <span class="number">1.5717</span>, <span class="number">-0.9207</span>,  <span class="number">0.1297</span>, <span class="number">-1.8768</span>],</span><br><span class="line">        [<span class="number">-0.6172</span>,  <span class="number">1.0036</span>, <span class="number">-0.6060</span>, <span class="number">-0.2432</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.max(a, <span class="number">1</span>)</span><br><span class="line">(tensor([ <span class="number">0.8475</span>,  <span class="number">1.1949</span>,  <span class="number">1.5717</span>,  <span class="number">1.0036</span>]), tensor([ <span class="number">3</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>]))</span><br></pre></td></tr></table></figure><h4 id="torch-bmm"><a href="#torch-bmm" class="headerlink" title="torch.bmm()"></a>torch.bmm()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">矩阵乘法</span><br><span class="line">torch.bmm(batch1, batch2, out=<span class="literal">None</span>) → Tensor</span><br><span class="line">batch1 和 batch2 都是三维向量</span><br><span class="line">他们的第一维要相等 相当于固定数目的二维矩阵相乘</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>batch1 = torch.randn(<span class="number">10</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>batch2 = torch.randn(<span class="number">10</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res = torch.bmm(batch1, batch2)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.size()</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure><h4 id="np-eye-k"><a href="#np-eye-k" class="headerlink" title="np.eye(k)"></a>np.eye(k)</h4><p>输出k行k列的单位对角矩阵</p><h4 id="torch-repeat"><a href="#torch-repeat" class="headerlink" title="torch.repeat()"></a>torch.repeat()</h4><p>在特定的维度重复tensor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.repeat(<span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将x重复四行两列</span></span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 三维深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维深度学习 </tag>
            
            <tag> pytorch </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PointNet论文解读</title>
      <link href="/2019/04/20/PointNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/04/20/PointNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>点云是现实生活中的一种关键数据集，但是之前的处理方法大多是用体素网格的方法或者多视角的方法处理，引入了多余的数据又丢失了诸多关键信息。本文提出一种网络架构PointNet，直接对点云进行分类和语义分割。</p><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>对点云数据处理需要解决点的排列和刚性变换的不变性。</p><p>点和相邻一些点之间组成了有含义的子集，需要学习到这种局部特征</p><h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pointnet/1.png" alt></p><p>如上图示，分类和分割共有网络的一部分，整个网络架构有三个重要组成部分。</p><ul><li><p>最大池化层作为一个对称函数从所有点收集信息</p></li><li><p>为了实现分割，将局部特征和全局特征相连接</p></li><li><p>变换输入和变换特征的两个网络</p></li></ul><h4 id="设计的细节"><a href="#设计的细节" class="headerlink" title="设计的细节"></a>设计的细节</h4><h5 id="T-Net在网络结构中起的本质作用是什么？需要预训练吗？"><a href="#T-Net在网络结构中起的本质作用是什么？需要预训练吗？" class="headerlink" title="T-Net在网络结构中起的本质作用是什么？需要预训练吗？"></a>T-Net在网络结构中起的本质作用是什么？需要预训练吗？</h5><p>T-Net 是一个预测特征空间变换矩阵的子网络，它从输入数据中学习出与特征空间维度一致的变换矩阵，然后用这个变换矩阵与原始数据向乘，实现对输入特征空间的变换操作，使得后续的每一个点都与输入数据中的每一个点都有关系。通过这样的数据融合，实现对原始点云数据包含特征的逐级抽象。</p><h5 id="对称函数的设计"><a href="#对称函数的设计" class="headerlink" title="对称函数的设计"></a>对称函数的设计</h5><p>由于输入的点是无序的，已有三种比较常见的方法，一是按照某种规则，强制将所有点排成有序；二是用不同顺序的输入来训练RNN,希望能学到这种不变性；第三是本文采用的，利用对称函数，使输出与输入的顺序无关。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pointnet/2.png" alt></p><p>其中h()是多层感知层，g()是简单的集合函数和一个池化层</p><h5 id="将局部特征和全级特征结合"><a href="#将局部特征和全级特征结合" class="headerlink" title="将局部特征和全级特征结合"></a>将局部特征和全级特征结合</h5><p>为了做语义分割要将学习局部的特征（对语义分割，输入n个点，输出n*m个数据，其中m是类的数目），将全局特征和每一个点的特征级联输入新的分类器，产生相应的$n \times m$个得分</p><h5 id="联合对齐网络（Joint-Alignment-Network"><a href="#联合对齐网络（Joint-Alignment-Network" class="headerlink" title="联合对齐网络（Joint Alignment Network)"></a>联合对齐网络（Joint Alignment Network)</h5><p>由于物体有多种形式，如转移，旋转，缩放等，需要对这些变化也学习不变性，利用类似于仿射矩阵的（TNet)学习这种不变性,对点云和提取的特征都先经过T-Net，再进行下一步的处理。</p><h4 id="论文中涉及的两个定理"><a href="#论文中涉及的两个定理" class="headerlink" title="论文中涉及的两个定理"></a>论文中涉及的两个定理</h4><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pointnet/3.jpg" alt></p><p>定理1证明了PointNet的网络结构能够拟合<strong>任意的连续集合函数</strong>。其作用类似证明神经网络能够拟合任意连续函数一样。同时，作者发现PointNet模型的表征能力和maxpooling操作输出的数据维度(K)相关，K值越大，模型的表征能力越强。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pointnet/4.jpg" alt></p><p>定理2(a)说明对于任何输入数据集S，都存在一个最小集Cs和一个最大集Ns，使得对Cs和Ns之间的任何集合T，其网络输出都和S一样。这也就是说，模型对输入数据在有噪声(引入额外的数据点，趋于Ns)和有数据损坏(缺少数据点，趋于Cs)的情况都是<strong>鲁棒</strong>的。定理2(b)说明了最小集Cs的数据多少由maxpooling操作输出数据的维度K给出上界。</p>]]></content>
      
      
      <categories>
          
          <category> 三维深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Faster-RCNN论文解读</title>
      <link href="/2019/04/15/Faster-RCNN%20%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/04/15/Faster-RCNN%20%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>本文关键是对目标检测的候选区域算法的改进，提出了RPN(Region Proposal Networks)，让RPN和检测网络共享图像的卷积特征，使候选区域可以被自动学习。其中使用了锚点的思想，是RPN的核心思想。</p><a id="more"></a><h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/faster-rcnn/1.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/faster-rcnn/4.png" alt></p><p>如上图所示，Faster-RCNN由两个模块组成，一个是生成提议区域的全卷积层（RPN),另一个是用于检测的Fast-RCNN检测器。</p><h4 id="区域提议网络（RPN"><a href="#区域提议网络（RPN" class="headerlink" title="区域提议网络（RPN)"></a>区域提议网络（RPN)</h4><p>RPN将任意大小的图像作为输入，输出一组矩形的目标提议，每一个提议都有一个目标得分。在最终的feature map层滑动窗口，最终得到一个特征(这个特征长度就是最后特征的层数，也就是最后一个卷积层的卷积核个数，zf为256，vgg为512），再将特征输入到之后的两个子全连接层，边界框回归层和边界框分类层。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/faster-rcnn/2.png" alt></p><h5 id="锚点"><a href="#锚点" class="headerlink" title="锚点"></a>锚点</h5><p>在每一个滑动窗口位置预测k个区域提议(k是尺度个数和长宽比个数乘积，本文用的是3*3）。</p><p>之前处理多尺度图片有两种经典方法，一个是图像金字塔；第一就是在特征上使用多尺度滑动窗口，类似多尺度滤波器。现在锚点的多尺度也能解决多尺度图片</p><h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><p>该损失函数是RPN的损失函数，与Fast R-CNN中大致相同。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/faster-rcnn/3.png" alt></p><p>其中的回归的损失只在正锚点中计算，分类用的是对数损失，回归损失用的是L1损失。其中的λ在一定程度内对准确率影响不大。</p><h3 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h3><p>因为RPN和检测框架都需要图像的特征层，本文采用交替训练实现在两种网络之中共享特征，先训练RPN,并用这些提议来训练Fast R-CNN,用fasr R-CNN微调好的网络再来初始化RPN,重复这个过程。</p><p>具体四步如下</p><ul><li>按照之前的描述训练RPN。该网络使用ImageNet的预训练模型进行初始化，并针对区域提议任务进行了端到端的微调。</li><li>接下来使用由第一步RPN生成的提议，由Fast R-CNN训练单独的检测网络。该检测网络也由ImageNet的预训练模型进行初始化。此时两个网络不共享卷积层。</li><li>接着，我们使用检测器网络来初始化RPN训练，但是我们修正共享的卷积层，并且只对RPN特有的层进行微调。现在这两个网络共享卷积层。</li><li>最后，保持共享卷积层的固定，我们对Fast R-CNN的独有层进行微调。因此，两个网络共享相同的卷积层并形成统一的网络。</li></ul><p>在实际训练中会有大量跨界的锚点，在训练中会忽视他们，但在测试时候会考虑他们。并且RPN的提议之间会有高度重叠，减少冗余，采用非极大值抑制。</p><h3 id="一些对照试验"><a href="#一些对照试验" class="headerlink" title="一些对照试验"></a>一些对照试验</h3><ul><li>移除RPN的分类层时，如果取样够多，对Map几乎没有影响，少了map也会下降；如果去除回归层，mAP将明显下降，说明锚的多尺度和多长宽比不足以准确检测。</li><li>使用一阶段的fast R-CNN模拟OverFeat,用来比对检测的两阶段和单阶段之间的差异，结果两阶段的map明显更好，一定程度表明级联架构的更准确。</li><li>把VGG和ZFNet替换成ResNet，效果明显提升</li><li>用更大的数据量，效果也更好</li></ul>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>了解三维深度学习</title>
      <link href="/2019/04/14/%E4%BA%86%E8%A7%A3%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/04/14/%E4%BA%86%E8%A7%A3%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>二维数据有着其局限性，比如如何通过一张照片判断别人和你的距离呢？在相机将三维世界压缩成二维图像时会丢失很多重要信息，用三维数据直接来表示世界或者增强二维视图成了一种选择。</p><h3 id="如何获取和表示三维数据"><a href="#如何获取和表示三维数据" class="headerlink" title="如何获取和表示三维数据"></a>如何获取和表示三维数据</h3><p>要想在三维空间中操作计算机视觉方法，需要感知、表示和理解三维数据。<br><a id="more"></a></p><h4 id="感知"><a href="#感知" class="headerlink" title="感知"></a>感知</h4><h5 id="立体视觉系统Stero"><a href="#立体视觉系统Stero" class="headerlink" title="立体视觉系统Stero"></a>立体视觉系统Stero</h5><p>需要在待测物体的一些特定位置放置摄像头，利用不同结构获取的图像，匹配相应的像素点，计算像素点在不同图像间的差异，从而计算出像素点在三维空间中的位置，他的硬件设施十分简单。</p><p>不过在准确率和运行速度方面表现不好，这是因为使用视觉细节来匹配不同的摄像头得到的图像之间对应的像素点不仅具有很高的计算复杂度，而且在缺乏纹理特征或视觉重复的环境中也很容易出错。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BA%86%E8%A7%A3%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.png" alt></p><h5 id="RGB-D"><a href="#RGB-D" class="headerlink" title="RGB-D"></a>RGB-D</h5><p>RGB-D是一种特殊的摄像头，除了颜色信息，还是D(深度)，它能告诉你之中的像素点离你有多远。RGB-D一般有两种工作原理，结构光或者飞行时间法。结构光是将红外线图案投射到一个场景上，感知红外线在几何表面上变形；飞行时间法观察其红外线返回摄像头所需要的时间，举例来说，微软的 Kinect 以及 Iphone X 的 FaceID 传感器都是 RGB-D 摄像头。</p><p>这种摄像头不像上述方法会出现视觉匹配，但是也有缺点。前景图像会遮挡后方物体的投影，感知范围等问题，如离摄像头远的地方，投影和感知就会变得很困难。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BA%86%E8%A7%A3%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2.png" alt></p><h5 id="激光雷达"><a href="#激光雷达" class="headerlink" title="激光雷达"></a>激光雷达</h5><p>激光雷达向物体发出快速激光脉冲，测量返回传感器的时间，类似上面的飞行时间法，但是激光雷达感知范围更广，有着更强的鲁棒性，目前自动驾驶大多使用这个，不过成本太高。</p><h4 id="三维表示"><a href="#三维表示" class="headerlink" title="三维表示"></a>三维表示</h4><p>获取了三维数据，需要用一定的组织结构来表示。有四种常用方式，点云，体素网格，三角网格和多视角表示。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BA%86%E8%A7%A3%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3.png" alt></p><h5 id="点云"><a href="#点云" class="headerlink" title="点云"></a>点云</h5><p>点云即三维空间中点的集合；每一点都是由某个（xyz）位置决定的，我们同时可以为其指定其它的属性（如 RGB 颜色）。它们是激光雷达数据被获取时的原始形式，立体视觉系统和 RGB-D 数据（包含标有每个像素点深度值的图像）通常在进行进一步处理之前会被转换成点云。</p><h5 id="体素网格"><a href="#体素网格" class="headerlink" title="体素网格"></a>体素网格</h5><p>体素网格是从点云发展而来的。「体素（Voxel）」就好比三维空间中的像素点，我们可以把体素网格看作量化的、大小固定的点云。然而，点云在空间中的任何地方能够以浮点像素坐标的形式涵盖无数个点；体素网格则是一种三维网格，其中的每个单元（或称「体素」）都有固定的大小和离散的坐标。</p><h5 id="多边形网格"><a href="#多边形网格" class="headerlink" title="多边形网格"></a>多边形网格</h5><p>多边形网格由一组带公共顶点的凸多边形表面组成，可近似一个几何表面。我们可以将点云看作是从基础的连续集合表面采样得到的三维点集；多边形网格则希望通过一种易于渲染的方式来表示这些基础表面。尽管多边形网格最初是为计算机图形学设计的，但它对于三维视觉也十分有用。我们可以通过几种不同的方法从点云中得到多边形网格，其中包括 Kazhdan 等人于 2006 年提出的「泊松表面重建法」。</p><h5 id="多视图表示"><a href="#多视图表示" class="headerlink" title="多视图表示"></a>多视图表示</h5><p>多视图表示是从不同的模拟视角（「虚拟摄像头」）获取到的渲染后的多边形网格二维图像集合，从而通过一种简单的方式表现三维几何结构。简单地从多个摄像头（如立体视觉系统 stereo）捕捉图像和构建多视图表示之间的区别在于，多视图实际上需要构建一个完整的 3D 模型，并从多个任意视点渲染它，以充分表达底层几何结构。与上面的其他三种表示不同，多视图表示通常只用于将 3D 数据转换为易于处理或可视化的格式。</p><h4 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h4><h5 id="通过多视图输入学习"><a href="#通过多视图输入学习" class="headerlink" title="通过多视图输入学习"></a>通过多视图输入学习</h5><p>将三维图像的多视角二维图像组用常用的二维深度学习技术来学习，代表作就是multi-view CNN,从三维物体的多个二维视图学到特征描述符，再将这些特征描述符组合在一起，再通过卷积层进行进一步的特征学习得以实现。</p><p>局限性：固定数量的二维视图是对三维结构的不完美近似，在复杂的物体和场景中会丢失很多信息，而且由于计算开销很大，使这种方法在自动驾驶或者虚拟现实等对实时性要求高的任务是不被接受的。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BA%86%E8%A7%A3%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.png" alt></p><h5 id="通过体素网格进行学习"><a href="#通过体素网格进行学习" class="headerlink" title="通过体素网格进行学习"></a>通过体素网格进行学习</h5><p>体素网格缩小了二维和三维之间的差距，是最接近图像的三维表示形式。VoxNet是最早基于体素网格输入的取得优异表现的深度学习方法。VoxNet 使用的是概率占用网格，其中的每个体素都包含了该体素在空间中被占用的概率。这样做的一个好处就是，它允许网络区分已知是自由的体素（例如，激光雷达光束经过的体素）和占用情况未知的体素（例如，激光雷达击中位置后方的体素）。</p><p>但是体素网格仍然具有一些缺点。首先，与点云相比，它们丢失了分辨率。因为如果代表复杂结构的不同点距离很近，它们会被被绑定在同一个体素中。与此同时，与稀疏环境中的点云相比，体素网格可能导致不必要的高内存使用率。这是因为它们主动消耗内存来表示自由和未知的空间，而点云只包含已知点。而且体素网格不具有旋转不变性，只是通过一些随机的视角学习旋转不变性。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BA%86%E8%A7%A3%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/5.png" alt></p><h5 id="通过点云学习"><a href="#通过点云学习" class="headerlink" title="通过点云学习"></a>通过点云学习</h5><p>由于各种方法不能完全模拟点云，基于原始点云数据上进行操作的架构自然而然产生了。</p><h6 id="PointNet"><a href="#PointNet" class="headerlink" title="PointNet"></a>PointNet</h6><p>点云只是一组表示位置的点，给定点云中的N个点，我们需要学习到这些点全排列之后不变的特征，同时网络也应该对点云的旋转和平移等转换有很强的鲁棒性。</p><p>PointNet的解决方法就是使用一个简单的对称函数，函数为任意顺序排列的输入生成一致的输出。基本架构可以定义为以下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/c24484fd-b9de-4438-831e-71a6e3433377/1536734242661.png" alt="img"></p><p>其中 f 是将输入点转换为 k 维向量的转换函数（用于物体分类）。该函数 f 可以近似表示为另一个存在的对称函数 g。在方程中，h 是一个多层感知机（MLP），它将单个输入点（以及它们相应的特征，如 xyz 位置、颜色、表面法线等）映射到更高维度的潜在空间。最大池化操作则会作为对称函数 g 起作用，它将学到的特征聚合为点云的全局描述符。这个单一特征向量会被传递给 另一个输出物体预测结果的多层感知机 γ。</p><p>为了应对学习对于点云的几何变换具有不变性的表示方式的挑战，PointNet 使用了一个称为 T-Net 的小型网络，T-Net 由可学习的参数组成，这些参数使 PointNet 能够将输入点云变换为一个固定的、规范的空间，从而确保整个网络对于即使是最细微的变化都具有很强的鲁棒性。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BA%86%E8%A7%A3%E4%B8%89%E7%BB%B4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/6.png" alt></p><h6 id><a href="#" class="headerlink" title=" "></a> </h6><h6 id="PointNet-1"><a href="#PointNet-1" class="headerlink" title="PointNet++"></a>PointNet++</h6><p>获取局部结构作为输入</p><h6 id="Graph-CNNs"><a href="#Graph-CNNs" class="headerlink" title="Graph CNNs"></a>Graph CNNs</h6><p>通过点学习其他点的独特的特征(?)</p><h6 id="SPLATNet"><a href="#SPLATNet" class="headerlink" title="SPLATNet"></a>SPLATNet</h6><h6 id="基于截椎体的-PointNet"><a href="#基于截椎体的-PointNet" class="headerlink" title="基于截椎体的 PointNet"></a>基于截椎体的 PointNet</h6>]]></content>
      
      
      <categories>
          
          <category> 三维深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三维深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fast R-CNN论文解读</title>
      <link href="/2019/04/13/Fast-RCNN%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/04/13/Fast-RCNN%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>Fast R-CNN相较R-CNN和SPPNet准确度和时间都提高不少，提出一串的创新方法。如合并损失，微调所有层参数，将分类和定位同时处理，取消SVM再分类；还做不少对照试验，探讨在目标检测中一些做法的必要性。</p><a id="more"></a><h3 id="Fast-R-CNN架构"><a href="#Fast-R-CNN架构" class="headerlink" title="Fast R-CNN架构"></a>Fast R-CNN架构</h3><p>fast R-CNN将输入图像和ROI(regions of interest)都送入神经网络，在feature map上将roi对应的feature提取成固定大小（学习spp）送入全连接层，输出有两个，一个是分类得分，一个是回归数据，对于某ROI,对每一个类都有一个预测的回归框。具体如下图</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/fast%20r-cnn_1.png" alt></p><p>其中ROI pooling层实际上就是一层的金字塔池化，假设图像有$h \times w$，将它划分成$H \times W$子窗口，每个窗口大小即$h /H 和 w/W$。然后执行最大池化。</p><h3 id="网络的训练"><a href="#网络的训练" class="headerlink" title="网络的训练"></a>网络的训练</h3><p>得到预训练的网络需要做以下三步改变</p><ul><li><p>将最后的最大池化层用ROI层代替</p></li><li><p>将网络最后的全连接层和Softmax层替换成之前的两个输出，即分类用的Softmax和同级的用以定位回归器</p></li><li><p>将网络改为两个输入，图像和各个ROI</p></li></ul><h4 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h4><p>SPPNet的微调只能更新全连接层，因为ROI的视野很大，反向训练基本是训练一张图像，效率很低（这就相当于从头训练了），本文提出在分层采样，对每个minipatch，先对图像采样再对图像中的ROI取样，同一张图片的ROI共享权重，这样就能减少计算量。（ROI是从图像中得到的feature map,ROI来自同一张图片的话就节省去重复得到这张图片的卷积和储存空间），并且这样没有出现收敛慢的问题。</p><h4 id="通过SVD分解全连接层"><a href="#通过SVD分解全连接层" class="headerlink" title="通过SVD分解全连接层"></a>通过SVD分解全连接层</h4><p>分解全连接层，并且不会减少很多准确率，这个很有技术含量，先记住有这种方法，以后有用到再去研究</p><p>可参照以下链接</p><p>[]: <a href="https://blog.csdn.net/WoPawn/article/details/52463853" target="_blank" rel="noopener">https://blog.csdn.net/WoPawn/article/details/52463853</a></p><h4 id="多任务损失"><a href="#多任务损失" class="headerlink" title="多任务损失"></a>多任务损失</h4><p>有两个同级输出，一个是关于K+1个类的得分，第二个是关于K个类的检测框（共有k组四元数组）。u是真实的值，v是检测框真值。那么损失函数就是</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/fast%20r-cnn_2.png" alt></p><p>其中Loc定义如下，λ是权衡两种损失的系数，论文中使用1</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/fast%20r-cnn_3.png" alt></p><h3 id="一些对照试验"><a href="#一些对照试验" class="headerlink" title="一些对照试验"></a>一些对照试验</h3><h4 id="微调哪些层"><a href="#微调哪些层" class="headerlink" title="微调哪些层"></a>微调哪些层</h4><p>对于只微调全连接层，把卷积层一起微调的有着更好的效果。不过对于第一层卷积层没有提高准确度，论文对于较大的网络微调第三层及之后的所有层，对较小的网络微调第二层及之后的所有层。</p><h4 id="多任务训练"><a href="#多任务训练" class="headerlink" title="多任务训练"></a>多任务训练</h4><p>多任务不仅使训练避免多阶段，而且通过对照试验多任务训练比逐级训练有着更好的表现。</p><h4 id="多尺度训练"><a href="#多尺度训练" class="headerlink" title="多尺度训练"></a>多尺度训练</h4><p>通过对照试验，SPPNet采用的多尺度训练比起消耗的时间和空间，提升的准确度有限。考虑是卷积层有学习尺度不变性的能力。</p><h4 id="需要更多的数据吗"><a href="#需要更多的数据吗" class="headerlink" title="需要更多的数据吗"></a>需要更多的数据吗</h4><p>通过更多的数据，准确度上升，说明数据量还没饱和</p><h4 id="SVM和Softmax"><a href="#SVM和Softmax" class="headerlink" title="SVM和Softmax"></a>SVM和Softmax</h4><p>在实验中Softmax反而效果比SVM更好一些，R-Cnn使用SVM是因为数据增强导致数据定位不精准，需要使用SVM用准确的正样本和负样本进行训练。</p><h4 id="更多的候选区域"><a href="#更多的候选区域" class="headerlink" title="更多的候选区域"></a>更多的候选区域</h4><p>候选区域的增加对准确度的帮助有限，而且超出一定范围之后反而会导致表现变差。</p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SPP论文解读</title>
      <link href="/2019/04/13/SPP%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/04/13/SPP%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p> SPPNet考虑到图像中大小不一，而之前的网络每次输入固定大小，引进空间金字塔池化使网络可以对各个尺寸的图片进行处理。用SPP替代最后一个池化层。（实际上在处理之前也需要知道图像的大小，才能在SPP部分做相应变化）</p><a id="more"></a><h3 id="什么是SPP（Spatial-pyramid-pooling"><a href="#什么是SPP（Spatial-pyramid-pooling" class="headerlink" title="什么是SPP（Spatial pyramid pooling)"></a>什么是SPP（Spatial pyramid pooling)</h3><p>空间金字塔池化，是传统图像处理经典方法，将特征图划分成不同的层次，在每一个层次上选取固定的大小的池化特征（本文用的是最大池化），这样就自然形成固定长度的特征，然后送入到全连接层中。</p><p>作者认为引进SPP的主要优点有：</p><ul><li>可以接受任意尺寸的图片，不用裁剪或者压缩来改变图片原本传达的信息</li><li>SPP用到多个尺寸的池化箱子，对图像的大小形变具有鲁班性</li></ul><h3 id="具体的训练细节"><a href="#具体的训练细节" class="headerlink" title="具体的训练细节"></a>具体的训练细节</h3><ul><li>为了充分利用已用的库，将金字塔池化模仿成之前的滑动窗口，步长等于滑动窗口的大小，在不一样的金字塔层上实行以上步骤。</li><li>对于不同尺寸照片的训练实际就是spp层的变化，不一样的图像为了获得一样的金字塔特征，需要对步长和窗口大小做不一样的变化，其他参数都是共享的。为了减少不同尺寸之间变化的代价，论文对不一样的尺寸的图像跑了一个epoch之后再交换，交替进行。</li></ul><h3 id="一些对照试验"><a href="#一些对照试验" class="headerlink" title="一些对照试验"></a>一些对照试验</h3><ul><li><p>在单尺寸图像的训练中，仅因为多层次池化就能提升准确度。即训练的时候和之前网络一样只使用单尺寸图像，把最后的池化层转换为SPP池化层</p></li><li><p>多尺度训练也提升准确度，但是要是更多尺度反而准确率下降，作者认为是因为在测试的时候用的是单一尺寸，更多尺度训练对测试结果反而没有帮助。</p></li><li><p>全图像表达的重要性，把全图片的视角加入测试会提高准确度</p></li></ul><h3 id="用于检测"><a href="#用于检测" class="headerlink" title="用于检测"></a>用于检测</h3><p>较RCNN的最大改进就是耗时，只在图像提取一次全特征，然后在特征上进去提取和训练。</p><p>用于检测的训练也是采用多尺寸，在测试的时候看候选区域最接近哪一个尺寸，就用该尺寸的SPP窗口大小进行测试</p><h3 id="如何通过候选区域得到他的feature-map上的位置"><a href="#如何通过候选区域得到他的feature-map上的位置" class="headerlink" title="如何通过候选区域得到他的feature map上的位置"></a>如何通过候选区域得到他的feature map上的位置</h3><p>SPPNet通过角点尽量将图像像素映射到feature map感受野的中央，假设每一层的padding都是p/2，p为卷积核大小。对于feature map的一个像素（x’,y’），其实际感受野为：（Sx‘，Sy’），其中S为之前所有层步伐的乘积。然后对于region proposal的位置，我们获取左上右下两个点对应的feature map的位置，然后取特征就好了。左上角映射为：</p><p><img src="https://pic2.zhimg.com/80/v2-8c5eddc9f856822aad5ae8d030ce1779_hd.png" alt="img"></p><p>右下角映射为：</p><p><img src="https://pic3.zhimg.com/80/v2-7a4ce0c60b8fcac5eb7ffe365f99572e_hd.png" alt="img"></p><p>当然，如果padding大小不一致，那么就需要计算相应的偏移值啦。</p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git和Github</title>
      <link href="/2019/04/03/Git%E5%92%8CGithub/"/>
      <url>/2019/04/03/Git%E5%92%8CGithub/</url>
      
        <content type="html"><![CDATA[<p>本篇学习笔记主要基于廖雪峰的git教程</p><a id="more"></a><h3 id="什么是GIT"><a href="#什么是GIT" class="headerlink" title="什么是GIT"></a>什么是GIT</h3><p>GIT是当今应用最广的分布式版本控制系统。</p><p>相对于分布式，另一个词就是集中式，集中式是指版本库集中存放在中央服务器，干活的时候从中央服务器取得最新的版本，更改完之后在推送给中央服务器。最大毛病就是需要联网。</p><p>分布式版本控制系统，每个人的电脑上都是一个完整的版本库，没有中央服务器。为了方便有时候也有一台充当“中央服务器”的电脑</p><h3 id="安装和初步使用"><a href="#安装和初步使用" class="headerlink" title="安装和初步使用"></a>安装和初步使用</h3><p>Windows的去<a href="https://git-scm.com/downloads" target="_blank" rel="noopener">官网网址</a>下载即可，按默认选项安装，安装之后的设置自己的github名称和邮箱。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git config --<span class="keyword">global</span> user.name <span class="string">"Your Name"</span></span><br><span class="line">$ git config --<span class="keyword">global</span> user.email <span class="string">"email@example.com"</span></span><br></pre></td></tr></table></figure><p>指定一个文件夹为git仓库 在当前文件夹中打开git bash</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><p>将当前文件夹中的文件加入仓库中 注意这个文件一定要在仓库所在的文件夹 不然git可没有办法整个电脑去找他</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如 git add readme.txt</span><br></pre></td></tr></table></figure><p>然后再确认一下 把该文件加到仓库中 commit后面的参数是对当前改变的描述</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;我加了一个文件&quot;</span><br></pre></td></tr></table></figure><p>为什么需要add 和commit呢 因为commit一次可以提交很多文件 你可以add很多文件之后 一次commit</p><h3 id="时光机"><a href="#时光机" class="headerlink" title="时光机"></a>时光机</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">git status 查看当前的状态</span><br><span class="line">git diff readme.txt 查看某个文件的修改记录</span><br><span class="line">git log 查看commit的历史记录</span><br><span class="line">git log --pretty=oneline 查看commit的简单记录 执行完会出现每个commit的head 这个head是由SHA1计算出来的</span><br><span class="line"></span><br><span class="line">#返回上一个版本</span><br><span class="line">git reset -hard HEAD^  返回上一个版本 上上个是HEAD^^ 往上一百个是HEAD~100</span><br><span class="line"></span><br><span class="line">#也可以回到指定版本号 也就是head</span><br><span class="line">git reset -hard ****</span><br><span class="line"></span><br><span class="line">#查看命令记录 和上面的commit的历史记录不一样</span><br><span class="line">git reflog</span><br></pre></td></tr></table></figure><h3 id="工作区和暂存区"><a href="#工作区和暂存区" class="headerlink" title="工作区和暂存区"></a>工作区和暂存区</h3><p>add的时候是将改变提交到缓存区，commit是将缓冲区中的改变一次性送到分支master。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">git diff 比较的是工作区文件与暂存区文件的区别（上次git add 的内容）</span><br><span class="line">git diff --cached 比较的是暂存区的文件与仓库分支里（上次git commit 后的内容）的区别</span><br><span class="line"></span><br><span class="line">#撤销刚才的修改  不用去手动修改文件 这是在没add之前</span><br><span class="line">git checkout -- readme.txt  </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">如果已经提交了到缓存库（也就是已经add了） 并且对工作区文件再修改 执行这条指令意思是把文件回退到和缓存库一样</span><br><span class="line">git checkout -- readme.txt  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">假如已经存入了缓存区 将缓存区的修改撤回掉  </span><br><span class="line">git reset HEAD readme.txt  </span><br><span class="line">再执行 git checkout -- readme.txt</span><br><span class="line">就好了</span><br><span class="line"></span><br><span class="line">删除文件</span><br><span class="line">先在文件夹系统里删删除文件 然后执行以下两句命令</span><br><span class="line">git rm readme.txt</span><br><span class="line">git commit</span><br><span class="line"></span><br><span class="line">但是如果是删错了</span><br><span class="line">git checkout -- readme.txt 就能撤销删除了</span><br></pre></td></tr></table></figure><h3 id="GitHub使用指南"><a href="#GitHub使用指南" class="headerlink" title="GitHub使用指南"></a>GitHub使用指南</h3><p>首先先需要创建SSH KEY</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C &quot;youremail@example.com&quot;</span><br></pre></td></tr></table></figure><p>把在用户目录的生成的id_rsa.pub添加到github的账户设置里 这样你每次操作的时候 github才知道你对这个仓库的所有权</p><h4 id="将自己的仓库推送到github上"><a href="#将自己的仓库推送到github上" class="headerlink" title="将自己的仓库推送到github上"></a>将自己的仓库推送到github上</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git remote add origin git@github.com:账户名/仓库名.git</span><br><span class="line">这样以后 在github上的库在本地就显示为origin 当然也可以改个名方便记忆</span><br><span class="line"></span><br><span class="line">$ git push -u origin master</span><br><span class="line">将当前的库的master分支都推送到github上  </span><br><span class="line">第一次推送的需要加参数 将两个仓库联系在一起 在以后的推送就可以简单命令</span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure><h4 id="从远程库克隆到本地"><a href="#从远程库克隆到本地" class="headerlink" title="从远程库克隆到本地"></a>从远程库克隆到本地</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone git@github.com:用户名/仓库名.git</span><br></pre></td></tr></table></figure><h3 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b dev 创建分支并切换到该dev分支 分支名叫dev</span><br><span class="line"></span><br><span class="line">相当于</span><br><span class="line">git branch dev</span><br><span class="line">git checkout dev      #git check 分支名 意味着切换到该分支</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">然后查看所有分支 当前分支前会有一个星号 </span><br><span class="line">git branch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">切换到master分支 并将dev分支合并</span><br><span class="line">git checkout master </span><br><span class="line">git merge dev </span><br><span class="line"></span><br><span class="line">然后可以删除分支</span><br><span class="line">git branch -d dev</span><br></pre></td></tr></table></figure><h4 id="合并分支的错误"><a href="#合并分支的错误" class="headerlink" title="合并分支的错误"></a>合并分支的错误</h4><p>当主分支和一般分支各自修改之后，合并不了，只能把冲突文件手动修改为我们希望的内容（这时git也会在改冲突文件中加上注释），然后在主分支中提交修改。</p><h4 id="多种分支管理"><a href="#多种分支管理" class="headerlink" title="多种分支管理"></a>多种分支管理</h4><p>一般合并分支的时候，git会使用Fast Forward,但在这种模式下删除分支会丢失掉分支信息。如果要强制禁用<code>Fast forward</code>模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。通常使用的是<code>--no-ff</code>方式的<code>git merge</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用 --no-ff 的合并时会生成一个新的commit就可以看到修改历史</span></span><br><span class="line">git merge --no-ff -m <span class="string">"merge with no-ff"</span> dev</span><br></pre></td></tr></table></figure><p>关于分支的使用，一般来说主分支是稳定的，所以不建议在主分支上做修改，在各个分支做修改再合并到主分支上。</p><h3 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h3><p>标签在指向某一个时刻的版本，实际上就是一个版本库的快照，和分支有点类似，不过分支可以移动，标签是固定的。他与commit绑定在一起，一般是通过标签来查找commit</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#切换到需要打标签的分支上 然后打标签</span></span><br><span class="line">git checkout master</span><br><span class="line">git tag V1.0<span class="comment">#标签名</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看所有标签 </span></span><br><span class="line">git tag</span><br><span class="line"></span><br><span class="line"><span class="comment">#可以对之前的commit加标签 而不是当前最新的commit</span></span><br><span class="line"><span class="comment">#获取之前的commit编号</span></span><br><span class="line">git <span class="built_in">log</span> --pretty=oneline --abbrev-commit</span><br><span class="line"><span class="comment">#对特定的commit打标签</span></span><br><span class="line">$ git tag v0.9 f52c633</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看git的详细信息 创立时间等</span></span><br><span class="line">git show &lt;tagname&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建带有说明的标签，用-a指定标签名，-m指定说明文字：</span></span><br><span class="line">git tag -a v0.1 -m <span class="string">"version 0.1 released"</span> 1094adb</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#删除标签</span></span><br><span class="line">git tag -d v0.1</span><br><span class="line"></span><br><span class="line"><span class="comment">#推送标签到远程</span></span><br><span class="line">git push origin v1.0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记_李宏毅（22-23）</title>
      <link href="/2019/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%8822-23%EF%BC%89/"/>
      <url>/2019/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%8822-23%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-22-Ensemble"><a href="#Lecture-22-Ensemble" class="headerlink" title="Lecture 22 Ensemble"></a>Lecture 22 Ensemble</h2><p>综合一些不同的分类器，提升perform</p><a id="more"></a><h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>bias很小，variance较大时（详见lec2),主要是为了减少过拟合.</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8522_1.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8522_2.png" alt></p><h4 id="Decision-tree"><a href="#Decision-tree" class="headerlink" title="Decision tree"></a>Decision tree</h4><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8522_3.png" alt></p><h4 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h4><p>决策树很容易过拟合，决策树加baggging就是随机森林的方法。</p><p>在每次决策树做分支的时候，随机选择一些问题或者条件不能被使用，将这些树组合起来就是随机森林。</p><h4 id="out-of-bag"><a href="#out-of-bag" class="headerlink" title="out-of-bag"></a>out-of-bag</h4><p>因为做bagging一般没有用到所有的数据，所以验证的时候不需要将划分测试集验证集，只需要将当前模型没有用来训练的数据做验证即可。</p><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>用在弱的模型上，将一堆弱分类器组合成一个强分类器，训练是有顺序的</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8522_4.png" alt></p><h4 id="如何得到多个分类器"><a href="#如何得到多个分类器" class="headerlink" title="如何得到多个分类器"></a>如何得到多个分类器</h4><p>在不一样的数据上训练出多个模型，如何选取不一样的数据，一就是随机抽样数据，二是对不同的数据赋上不同的权值（在实际操作的时候也就是对不同数据的损失函数赋上一个不一样的权重）</p><h4 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h4><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8522_5.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8522_6.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8522_7.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8522_8.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8522_9.png" alt></p><h3 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8522_10.png" alt></p><h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8522_11.png" alt></p><h2 id="Lecture-23-Deep-Reinforcement-Learning"><a href="#Lecture-23-Deep-Reinforcement-Learning" class="headerlink" title="Lecture 23 Deep Reinforcement Learning"></a>Lecture 23 Deep Reinforcement Learning</h2><p>根据之前的激励决定接下来要做什么，难点是很多行动之后没有reward,比如下棋只有最后才知道输赢。</p><p>在人也不知道正确答案是什么的时候，强化学习比监督学习更有用。抽象点说，监督学习是从一个老师那里学习，强化学习是从过去经验学习。alpha go就是先用监督学习再用强化学习。</p><h3 id="Reinforcement-Learning的难点"><a href="#Reinforcement-Learning的难点" class="headerlink" title="Reinforcement Learning的难点"></a>Reinforcement Learning的难点</h3><ul><li>激励延迟，有些action没有实时的激励但是会对接下来的目标完成有很大帮助</li><li>机器的活动会影响接下来的观测的结果，所以需要机器去做一些之前没做过的东西，多去explore</li></ul><h3 id="Reinforcement-Learning的两大方法"><a href="#Reinforcement-Learning的两大方法" class="headerlink" title="Reinforcement Learning的两大方法"></a>Reinforcement Learning的两大方法</h3><ul><li><p>policy-based   学习到一个做事的机器人  actor</p></li><li><p>value-based    学习到一个不做事的评论者    critic</p></li></ul><p>以上两种方法加起来叫做Actor-Critic的方法</p><h3 id="Policy-based-Approach-Learning-an-Actor"><a href="#Policy-based-Approach-Learning-an-Actor" class="headerlink" title="Policy-based Approach -Learning an Actor"></a>Policy-based Approach -Learning an Actor</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8523_1.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8523_2.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8523_3.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8523_4.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8523_5.png" alt></p><p>强化学习还有一部分 ，这方面太硬核了，而且暂时用不到，就先留个坑。有时间把强化学习这三节再好好看看。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记_李宏毅（19-21）</title>
      <link href="/2019/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%8819-21%EF%BC%89/"/>
      <url>/2019/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%8819-21%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-19-Transfer-Learning"><a href="#Lecture-19-Transfer-Learning" class="headerlink" title="Lecture 19 Transfer Learning"></a>Lecture 19 Transfer Learning</h2><p>用和任务无关的图片进行预训练</p><a id="more"></a><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8519_10.png" alt></p><h3 id="Model-Fine-tuning"><a href="#Model-Fine-tuning" class="headerlink" title="Model Fine-tuning"></a>Model Fine-tuning</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8519_1.png" alt></p><h4 id="Conservative-Training"><a href="#Conservative-Training" class="headerlink" title="Conservative Training"></a>Conservative Training</h4><p>但是由于训练数据太少，要加点限制，使新生成的模型和本来的模型相差不多，避免过拟合</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8519_2.png" alt></p><h4 id="Layer-Transfer"><a href="#Layer-Transfer" class="headerlink" title="Layer Transfer"></a>Layer Transfer</h4><p>为了减少过拟合，也可以直接复制大部分层，只训练一部分层，这样也可以有效避免过拟合。那么复制哪几层呢？在语音中，通常copy后几层；在图像中，一般复制前面几层</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8519_3.png" alt></p><h3 id="Multitask-Learning"><a href="#Multitask-Learning" class="headerlink" title="Multitask Learning"></a>Multitask Learning</h3><p>和fine-tuning不一样的是，它可能共用几层，然后完成不一样的任务。应用：翻译多种语言</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8519_4.png" alt></p><h3 id="Domain-adversarial-training"><a href="#Domain-adversarial-training" class="headerlink" title="Domain-adversarial training"></a>Domain-adversarial training</h3><p>训练数据没有标签，资源数据有标签，我们需要使两类数据分布大致相同.把source data当train data,把train data 当test data.</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8519_5.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8519_6.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8519_7.png" alt></p><h3 id="Zero-shot-Learning"><a href="#Zero-shot-Learning" class="headerlink" title="Zero-shot Learning"></a>Zero-shot Learning</h3><p> 训练数据在资源数据中没有出现过。这样表征图像的时候提取特征而不是直接说他有什么，根据训练图片有哪些特征搭配上词典。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8519_8.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8519_9.png" alt></p><h2 id="Lecture-20-Support-Vector-Machine-SVM"><a href="#Lecture-20-Support-Vector-Machine-SVM" class="headerlink" title="Lecture 20 Support Vector Machine(SVM)"></a>Lecture 20 Support Vector Machine(SVM)</h2><p>两大重点：Hinge Loss 和 Kernel Method</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>各个损失函数的比较 hinge loss使正确分类的数据大于1之后就不在去调整这个数据 但是cross entropy原则是越准确越好。实际上两者差异不显著，hinge loss可能会略胜一筹。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_1.png" alt></p><h3 id="线性SVM"><a href="#线性SVM" class="headerlink" title="线性SVM"></a>线性SVM</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_2.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_3.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_4.png" alt></p><h3 id="Dual-Representation"><a href="#Dual-Representation" class="headerlink" title="Dual Representation"></a>Dual Representation</h3><p>求出来的W实际上是一些数据的线性组合，里面大多数参数都为0,不为0的数据称为支持向量。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_5.png" alt></p><p>我们其实不需要知道x的具体的值，只需要只是x和$x’$之间的关系</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_6.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_7.png" alt></p><h3 id="Kernel-Trick"><a href="#Kernel-Trick" class="headerlink" title="Kernel Trick"></a>Kernel Trick</h3><p>已知两个向量，要计算他们的内积，但是由于在现在空间中不能解决，需要把他们映射到高维空间再做内积，但是这样的计算量很大。我们就会想在低维空间做一些操作，使这些操作的结果等于在高维空间内积结果，也就是$K(x,y)=&lt;φ(x),φ(y)&gt;$,这里的φ就是投影到高维空间，K称为核函数，这种方法就叫做核技巧。</p><p>我们就不需要考虑x和y本身，以及φ的值，只要K能表示x和y在某个高维空间之间的相似程度就可以了。有个Mercer’s theory可以检查这个K能否符合条件，但是是哪个高维空间，我们就不需要去知道了。</p><p>数据先转换特征空间再做内积，实际上等于在原空间做内积，再做特定的变形。而且后者的计算量比较小。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_8.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_9.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_10.png" alt><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_11.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_12.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8520_13.png" alt></p><h2 id="Lecture-21-Recurrent-Neural-Network"><a href="#Lecture-21-Recurrent-Neural-Network" class="headerlink" title="Lecture 21 Recurrent Neural Network"></a>Lecture 21 Recurrent Neural Network</h2><p>不单单从当下的输入获得输出，还会考虑之前的输入。所以输入的顺序会对整个网络产生影响，</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8521_1.png" alt></p><h3 id="Slot-Filling"><a href="#Slot-Filling" class="headerlink" title="Slot Filling"></a>Slot Filling</h3><p>从一句话中找到想要的关键词，比如旅客要订票，你要从他的一句话中提取时间，出发地，到达地等等。</p><h3 id="Elman-Network和Jordan-Network"><a href="#Elman-Network和Jordan-Network" class="headerlink" title="Elman Network和Jordan Network"></a>Elman Network和Jordan Network</h3><p>Elman Network的memory存的是上一次训练的隐藏层,而Jordan Network的memory存的是上次训练的输出</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8521_2.png" alt></p><h3 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h3><p>双向的RNN,优点是看的范围比较多</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8521_3.png" alt></p><h3 id="LSTM-Long-Short-term-Memory"><a href="#LSTM-Long-Short-term-Memory" class="headerlink" title="LSTM(Long Short-term Memory)"></a>LSTM(Long Short-term Memory)</h3><p>对memory加了三道门，输入阈值输出阈值和遗忘阈值。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8521_4.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8521_5.png" alt></p><p>LSTM需要更多的参数量，当和一般网络有一样节点的时候，参数量大概是四倍</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8521_6.png" alt></p><p>LSTM实际上还考虑到前一个的隐藏层和输出层，LSTM是现在最常用的RNN</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8521_7.png" alt></p><h3 id="RNN的训练"><a href="#RNN的训练" class="headerlink" title="RNN的训练"></a>RNN的训练</h3><p>将slot分类，然后用1-N来表达特征，损失函数为两个特征的交叉熵。训练方法也是梯度下降，但是RNN的梯度下降不是BP，叫做BPTT，考虑了时间的BP。</p><p>RNN很难训练，它的损失函数是凹凸剧烈的，有个调整的办法叫做<strong>clipping</strong>就是当一些梯度很大的时候就限制在一个阈值里。</p><h4 id="RNN为什么难以训练？"><a href="#RNN为什么难以训练？" class="headerlink" title="RNN为什么难以训练？"></a>RNN为什么难以训练？</h4><p>因为他考虑时间和以前的状态，一个小小的变化会导致最后结果的爆炸。而且有些地方梯度有些地方梯度小，也不能只通过调小学习率来避免梯度爆炸。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8521_8.png" alt></p><p>LSTM可以处理梯度消失问题，虽然不能处理梯度爆炸，但是就可以统一采用小学习率来学习。因为他的memory不会像一般RNN那样简单洗掉，所以能保留一定值，也就是处理梯度消失</p><h3 id="Attention-based-Model"><a href="#Attention-based-Model" class="headerlink" title="Attention-based Model"></a>Attention-based Model</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8521_9.png" alt></p><h3 id="RNN和Structured-Learning"><a href="#RNN和Structured-Learning" class="headerlink" title="RNN和Structured Learning"></a>RNN和Structured Learning</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8521_10.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>R-CNN论文解读</title>
      <link href="/2019/03/28/R_CNN%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/03/28/R_CNN%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>这篇论文是R-CNN系列的第一篇，将传统的区域候选框方法和深度卷积网络相结合，用于目标检测。还有一个很重要的贡献点是对小样本数据采用了迁移学习。</p><a id="more"></a><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>1.生成两千个区域候选</p><p>2.用CNN提取特征</p><p>3.使用SVM进行分类</p><p>4.利用回归框，对预测的框进行微调</p><h3 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h3><p>先用只带有标签的大数据库对当前架构的CNN部分进行预训练，然后用自己的数据集（带有检验框的）进行微调，如果候选区域和ground-truth的IOU大于0.5就视作正样本，否则就视为负样本。在每次随机梯度下降的时候，选择32个正样本，96个负样本，因为数据中正样本太少了，所以采用这种方法平衡训练。</p><p>在使训练SVM二分类器用以分类的时候，将IOU&gt;0.3的就置为正样本，论文从0试到0.5得到的结论。</p><p>对产生候选区域的方法不是很在意，本篇论文采用了selective search的方法去产生候选区域。用alexnet的架构从每个候选区域上提取4096维的特征，对于图片的处理不管纵横比，直接扭曲到想要的大小。</p><h3 id="可视化和消融"><a href="#可视化和消融" class="headerlink" title="可视化和消融"></a>可视化和消融</h3><p>论文还研究探讨了CNN的可能，可视化并没有采用复杂的方法，选定一个单元，将使它激活最大的图片拿出来进行分析，每个单元都学习到了一定的特征，比如都有斑点，或者都有三角形之类的。</p><p>消融就是去掉卷积层的若干层，看这些层本来发挥的作用，在没有微调之前，去掉全连接层对整个网络的精度影响不大，微调之后有全连接层的对照试验进步比较大，表明卷积层学习的是较为一般的特征，全连接层则是将这些特征进行组合。</p><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>通过将预测框的平移加尺度放缩，接近ground truth。利用损失函数最小找到合适的变化。这个检测框回归器是针对于每个类别，也就是每一组的检测框回归器是共同学习，共同使用的。以下是一个关于边框回归的详细说明。</p><p>[]: <a href="https://blog.csdn.net/zijin0802034/article/details/77685438" target="_blank" rel="noopener">https://blog.csdn.net/zijin0802034/article/details/77685438</a></p><h3 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a>测试方法</h3><p>对一张测试图片，利用cnn提取特征之后送到SVM,得到当前区域关于不同类的得分。并对每一个类做非极大值抑制，也就是两个候选区域在同一类得分上较多，并且他们有较大重叠面积就把得分较小的区域归为背景。也就是避免了一个物体同时被两个候选框框出来。</p><h3 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h3><h4 id="为什么在微调CNN和训练目标检测SVM时定义的正负样本不同？"><a href="#为什么在微调CNN和训练目标检测SVM时定义的正负样本不同？" class="headerlink" title="为什么在微调CNN和训练目标检测SVM时定义的正负样本不同？"></a>为什么在微调CNN和训练目标检测SVM时定义的正负样本不同？</h4><p>在微调的时候实际上正样本要多出很多，因为他是把IOU大于0.5的所有框都视为正样本，其他就视为背景，也就是负样本。</p><p>但是SVM进行训练的时候，只有当前类的IOU大于一定值才能视为负样本，对于同一类别的IOU小于一定值的视为负样本，对于其他类的候选的IOU大于0.3则会被忽视。简单来说这个SVM的阈值是被试出来的。</p><h4 id="为什么微调之后不直接使用全连接层的softmax，而要使用SVM"><a href="#为什么微调之后不直接使用全连接层的softmax，而要使用SVM" class="headerlink" title="为什么微调之后不直接使用全连接层的softmax，而要使用SVM"></a>为什么微调之后不直接使用全连接层的softmax，而要使用SVM</h4><p>因为微调时候的数据不够多，作者对正样本进行“抖动”，增加了三十倍的正样本，虽然增加了数据量避免了过拟合，但是这反而不利于精确定位。所以采用了SVM用于分类。</p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记_李宏毅（16-18）</title>
      <link href="/2019/03/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%8816-18%EF%BC%89/"/>
      <url>/2019/03/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%8816-18%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-16-Unsupervised-Learning-Auto-encoder"><a href="#Lecture-16-Unsupervised-Learning-Auto-encoder" class="headerlink" title="Lecture 16 Unsupervised Learning-Auto-encoder"></a>Lecture 16 Unsupervised Learning-Auto-encoder</h2><p>用在图像上，和PCA差不多，也是降维</p><a id="more"></a><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8516_1.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8516_2.png" alt></p><p>可以用在文字处理上，或者相似图片的搜索，也可以用在预训练上（因为中间层比两端数目大，要加上大权重的正则化，避免学习完只是简单的复制）</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8516_3.png" alt></p><h4 id="增加对噪声的鲁棒性"><a href="#增加对噪声的鲁棒性" class="headerlink" title="增加对噪声的鲁棒性"></a>增加对噪声的鲁棒性</h4><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8516_4.png" alt></p><h3 id="Unpooing"><a href="#Unpooing" class="headerlink" title="Unpooing"></a>Unpooing</h3><p>记住之前池化时的最大位置，反池化就是把扩大feature map，并且将相应位置的最大值赋回去。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8516_5.png" alt></p><h3 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h3><p>实际上deconvolution就是convolution，下图灰色图表示为零输入</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8516_6.png" alt></p><h3 id="扩充数据集"><a href="#扩充数据集" class="headerlink" title="扩充数据集"></a>扩充数据集</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8516_7.png" alt></p><h2 id="Lecture-17-Unsupervised-Learning-Deep-Generative-Model-上"><a href="#Lecture-17-Unsupervised-Learning-Deep-Generative-Model-上" class="headerlink" title="Lecture 17 Unsupervised Learning-Deep Generative Model(上)"></a>Lecture 17 Unsupervised Learning-Deep Generative Model(上)</h2><p>generative models大概能分成三个方法</p><ul><li><p>PixelRNN</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8517_1.png" alt></p></li><li><p>Variational Autoencoder(VAE)</p></li><li><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8517_2.png" alt></p></li><li><p>Generative Adversarial Network(GAN)</p></li></ul><h2 id="Lecture-18-Unsupervised-Learning-Deep-Generative-Model（下）"><a href="#Lecture-18-Unsupervised-Learning-Deep-Generative-Model（下）" class="headerlink" title="Lecture 18 Unsupervised Learning -Deep Generative Model（下）"></a>Lecture 18 Unsupervised Learning -Deep Generative Model（下）</h2><h3 id="Why-VAE"><a href="#Why-VAE" class="headerlink" title="Why VAE?"></a>Why VAE?</h3><p>增加了噪声，使得不在输入图像中的点也可以稳定输出一些图片。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8518_1.png" alt></p><p>还有很多数学推导，大概是假设图像的出现符合混合高斯分布，产生一个和训练数据相近的图片。具体需要可以跟着视频推导一边</p><h3 id="Generative-Adversarial-Network"><a href="#Generative-Adversarial-Network" class="headerlink" title="Generative Adversarial Network"></a>Generative Adversarial Network</h3><p>有点类似于仿生学的进化</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8518_2.png" alt></p><p>训练Discriminator和Generator</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8518_3.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8518_4.png" alt></p><p>最大难点是不知道现在产生的图片的评判标准，只能用可视化然后人眼检查</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记_李宏毅（13-15）</title>
      <link href="/2019/03/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%8813-15%EF%BC%89/"/>
      <url>/2019/03/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%8813-15%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-13-Unsupervised-Learning-Linear-Methods"><a href="#Lecture-13-Unsupervised-Learning-Linear-Methods" class="headerlink" title="Lecture 13 Unsupervised Learning-Linear Methods"></a>Lecture 13 Unsupervised Learning-Linear Methods</h2><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>可以分成两类Reduction（化繁为简）和Generation（无中生有）</p><p>其中Reduction可以再分为两类，clustering（聚类）和Dimension(降维)</p><a id="more"></a><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8513_1.png" alt></p><h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><h4 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h4><p>有要几个类，只能凭经验来</p><h4 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h4><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8513_2.png" alt></p><h4 id="HAC-Hierarchical-Agglomerative-Clustering"><a href="#HAC-Hierarchical-Agglomerative-Clustering" class="headerlink" title="HAC(Hierarchical Agglomerative Clustering)"></a>HAC(Hierarchical Agglomerative Clustering)</h4><p>按相似度依次建树，然后根据不一样的聚类需求切刀 自然就分成了几类</p><p>和K-means的差别是不用决定K等于多少</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8513_3.png" alt></p><h3 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed Representation"></a>Distributed Representation</h3><p>聚类太过绝对，会遗漏一些信息，以偏概全。</p><p>这个做法是将高维信息装换为低维信息。比如一张图片可能不需要用那么多像素点就可以描述完。</p><h4 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h4><h5 id="Feature-selection"><a href="#Feature-selection" class="headerlink" title="Feature selection"></a>Feature selection</h5><p>当数据在某一维上变化不大时，可以把这个特征拿掉</p><h5 id="Principle-component-analysis-PCA-主成分分析"><a href="#Principle-component-analysis-PCA-主成分分析" class="headerlink" title="Principle component analysis(PCA 主成分分析)"></a>Principle component analysis(PCA 主成分分析)</h5><p>假设输入和输出符合线性转换。Z=Wx,目标是找到W.</p><p>同时我们希望投影之后得到的点之间要有明显的区别，不能把点和点之间的差别度拿掉了。</p><p>投影到多维时候注意各个维度之间要垂直，找到的W就是正交矩阵了。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8513_4.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8513_5.png" alt></p><p>PCA经典解法，拉格朗日乘子法</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8513_6.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8513_7.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8513_8.png" alt></p><p>PCA变形的结果，各个特征之间是相互独立</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8513_9.png" alt></p><h5 id="PCA的另一种理解"><a href="#PCA的另一种理解" class="headerlink" title="PCA的另一种理解"></a>PCA的另一种理解</h5><h5 id="PCA的缺点"><a href="#PCA的缺点" class="headerlink" title="PCA的缺点"></a>PCA的缺点</h5><ul><li><p>因为PCA是非监督学习，所以降维的时候没有考虑到不同维之间的差异</p></li><li><p>PCA是线性的</p></li></ul><h3 id="Matrix-Factorization"><a href="#Matrix-Factorization" class="headerlink" title="Matrix Factorization"></a>Matrix Factorization</h3><p>用svd</p><p>当有数据缺失的时候，考虑用随机梯度下降，训练的时候不去采那些数据缺失的地方，</p><p>加入一些其他参数</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8513_10.png" alt></p><h2 id="Lecture-14-Unsupervised-Learning-Word-Embedding"><a href="#Lecture-14-Unsupervised-Learning-Word-Embedding" class="headerlink" title="Lecture 14 Unsupervised Learning Word Embedding"></a>Lecture 14 Unsupervised Learning Word Embedding</h2><h3 id="表示单词的方法"><a href="#表示单词的方法" class="headerlink" title="表示单词的方法"></a>表示单词的方法</h3><ul><li><p>1-of-N Encoding</p><p>有多少个单词就有多少维，哪个单词所在维为1，其他维为0.这样就忽略了相近单词之间的关系。</p></li><li><p>Word Class</p><p>把相近单词放到一类里，然后用这个类来表示这个单词，类似于聚类。这样也损失了很多的信息</p></li><li><p>Word embudding</p><p>  将单词在高维空间用点表示出来，但是这个高维肯定比1-of-N encoding的维度要低。构建word embedding的过程也是非监督学习。训练的时候是用上下文来的。</p></li></ul><h3 id="利用上下文做word-embeding的方法"><a href="#利用上下文做word-embeding的方法" class="headerlink" title="利用上下文做word embeding的方法"></a>利用上下文做word embeding的方法</h3><ul><li>count based</li></ul><p>如果两个词汇经常一起出现，就默认他们比较接近</p><p> 代表性方法：Grove vector</p><ul><li>prediction based（实际上这个网络不需要深度，经典的只用一个隐藏层）</li></ul><p>训练一个网络，对于给定一个单词，预测下一个出现的单词是什么。</p><p>然后把这个网络的第一个隐藏层当做这个单词的表征向量。</p><p>代表性方法：CBOW: 拿前后两个单词去预测中间那个单词（continue bag of words)</p><p>​            Skip-gram：拿中间的单词去预测之前和之后的单词</p><p>embedding还能用于翻译，根据同一种语言不同单词之间的关系 和一些翻译</p><h2 id="Lecture-15-Unsupervised-Learning-Neighbor-Embedding"><a href="#Lecture-15-Unsupervised-Learning-Neighbor-Embedding" class="headerlink" title="Lecture 15 Unsupervised Learning -Neighbor Embedding"></a>Lecture 15 Unsupervised Learning -Neighbor Embedding</h2><p>manifold:把低维塞到高维</p><p>在做聚类或者学习的时候需要先把高维里的东西摊平。以下介绍几种经典方法</p><h3 id="Locally-Linear-Embedding（LLE"><a href="#Locally-Linear-Embedding（LLE" class="headerlink" title="Locally Linear Embedding（LLE)"></a>Locally Linear Embedding（LLE)</h3><p>主要思想：相对距离在不同维度里不会变化</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8515_1.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8515_2.png" alt></p><h3 id="Laplacian-Eigenmaps"><a href="#Laplacian-Eigenmaps" class="headerlink" title="Laplacian Eigenmaps"></a>Laplacian Eigenmaps</h3><h3 id="t-SNE"><a href="#t-SNE" class="headerlink" title="t-SNE"></a>t-SNE</h3><p>t-sne一般是用来做可视化的，因为对于新的点没有办法处理</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8515_3.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch入门系列（3）_Data loading and processing</title>
      <link href="/2019/03/22/pytorch%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89_Data%20loading%20and%20processing/"/>
      <url>/2019/03/22/pytorch%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89_Data%20loading%20and%20processing/</url>
      
        <content type="html"><![CDATA[<h3 id="pandas-read-csv"><a href="#pandas-read-csv" class="headerlink" title="pandas.read_csv"></a>pandas.read_csv</h3><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pandas.read_csv(filepath_or_buffer, sep=&apos;, &apos;, delimiter=None, header=&apos;infer&apos;, names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression=&apos;infer&apos;, thousands=None, decimal=b&apos;.&apos;, lineterminator=None, quotechar=&apos;&quot;&apos;, quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=None, error_bad_lines=True, warn_bad_lines=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None)</span><br><span class="line"></span><br><span class="line">读取csv文件的 介绍一些重要的参数</span><br><span class="line"></span><br><span class="line">filepath_or_buffer  文件路径</span><br><span class="line">sep   分隔符 默认为，</span><br></pre></td></tr></table></figure><h3 id="pandas-DataFrame-iloc"><a href="#pandas-DataFrame-iloc" class="headerlink" title="pandas.DataFrame.iloc"></a>pandas.DataFrame.iloc</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">按行和列索引 切片等</span><br><span class="line">如 DataFrame.iloc（3,5） 表示取第三行第五个</span><br></pre></td></tr></table></figure><h3 id="Dataset-class"><a href="#Dataset-class" class="headerlink" title="Dataset class"></a>Dataset class</h3><p>torch.utils.data.Dataset是用来表达数据库的抽象类</p><p>一般来说，需要继承这个类，然后重载以下这些方法</p><p>__len__() 求数据库的长度</p><p>__getitem__() 用来索引这个数据库的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FaceLandmarksDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="string">"""Face Landmarks dataset."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, csv_file, root_dir, transform=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            csv_file (string): Path to the csv file with annotations.</span></span><br><span class="line"><span class="string">            root_dir (string): Directory with all the images.</span></span><br><span class="line"><span class="string">            transform (callable, optional): Optional transform to be applied</span></span><br><span class="line"><span class="string">                on a sample.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.landmarks_frame = pd.read_csv(csv_file)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.landmarks_frame)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span><span class="comment">#idx就是读取哪个数字</span></span><br><span class="line">        img_name = os.path.join(self.root_dir,</span><br><span class="line">                                self.landmarks_frame.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = io.imread(img_name)</span><br><span class="line">        landmarks = self.landmarks_frame.iloc[idx, <span class="number">1</span>:].as_matrix()</span><br><span class="line">        landmarks = landmarks.astype(<span class="string">'float'</span>).reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">        sample = &#123;<span class="string">'image'</span>: image, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"><span class="comment">#转换成二维数组</span></span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            sample = self.transform(sample)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure><h4 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h4><p>transforms是初始化data类时的可选参数，用来对例子进行归一化处理之类的</p><h5 id="定义transform具体的类"><a href="#定义transform具体的类" class="headerlink" title="定义transform具体的类"></a>定义transform具体的类</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rescale</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Rescale the image in a sample to a given size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        output_size (tuple or int): Desired output size. If tuple, output is</span></span><br><span class="line"><span class="string">            matched to output_size. If int, smaller of image edges is matched</span></span><br><span class="line"><span class="string">            to output_size keeping aspect ratio the same.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_size)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(output_size, (int, tuple))</span><br><span class="line">        self.output_size = output_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"></span><br><span class="line">        h, w = image.shape[:<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> isinstance(self.output_size, int):</span><br><span class="line">            <span class="keyword">if</span> h &gt; w:</span><br><span class="line">                new_h, new_w = self.output_size * h / w, self.output_size</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_h, new_w = self.output_size, self.output_size * w / h</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_h, new_w = self.output_size</span><br><span class="line"></span><br><span class="line">        new_h, new_w = int(new_h), int(new_w)</span><br><span class="line"></span><br><span class="line">        img = transform.resize(image, (new_h, new_w))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># h and w are swapped for landmarks because for images,</span></span><br><span class="line">        <span class="comment"># x and y axes are axis 1 and 0 respectively</span></span><br><span class="line">        landmarks = landmarks * [new_w / w, new_h / h]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: img, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomCrop</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Crop randomly the image in a sample.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        output_size (tuple or int): Desired output size. If int, square crop</span></span><br><span class="line"><span class="string">            is made.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_size)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(output_size, (int, tuple))</span><br><span class="line">        <span class="keyword">if</span> isinstance(output_size, int):</span><br><span class="line">            self.output_size = (output_size, output_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> len(output_size) == <span class="number">2</span></span><br><span class="line">            self.output_size = output_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"></span><br><span class="line">        h, w = image.shape[:<span class="number">2</span>]</span><br><span class="line">        new_h, new_w = self.output_size</span><br><span class="line"></span><br><span class="line">        top = np.random.randint(<span class="number">0</span>, h - new_h)</span><br><span class="line">        left = np.random.randint(<span class="number">0</span>, w - new_w)</span><br><span class="line"></span><br><span class="line">        image = image[top: top + new_h,</span><br><span class="line">                      left: left + new_w]</span><br><span class="line"></span><br><span class="line">        landmarks = landmarks - [left, top]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: image, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToTensor</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Convert ndarrays in sample to Tensors."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># swap color axis because</span></span><br><span class="line">        <span class="comment"># numpy image: H x W x C</span></span><br><span class="line">        <span class="comment"># torch image: C X H X W</span></span><br><span class="line">        image = image.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: torch.from_numpy(image),</span><br><span class="line">                <span class="string">'landmarks'</span>: torch.from_numpy(landmarks)&#125;</span><br></pre></td></tr></table></figure><h5 id="组装transform"><a href="#组装transform" class="headerlink" title="组装transform"></a>组装transform</h5><p>有Compose（)函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scale = Rescale(<span class="number">256</span>)</span><br><span class="line">crop = RandomCrop(<span class="number">128</span>)</span><br><span class="line">composed = transforms.Compose([Rescale(<span class="number">256</span>),</span><br><span class="line">                               RandomCrop(<span class="number">224</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply each of the above transforms on sample.</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">sample = face_dataset[<span class="number">65</span>]</span><br><span class="line"><span class="keyword">for</span> i, tsfrm <span class="keyword">in</span> enumerate([scale, crop, composed]):</span><br><span class="line">    transformed_sample = tsfrm(sample)</span><br><span class="line"></span><br><span class="line">    ax = plt.subplot(<span class="number">1</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    ax.set_title(type(tsfrm).__name__)</span><br><span class="line">    show_landmarks(**transformed_sample)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="DataLoader-数据加载器"><a href="#DataLoader-数据加载器" class="headerlink" title="DataLoader 数据加载器"></a>DataLoader 数据加载器</h3><p>加入batchsize 和打乱数据等工作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">dataloader = DataLoader(transformed_dataset, batch_size=<span class="number">4</span>,</span><br><span class="line">                        shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#worker是用来读取数据用几个线程?</span></span><br><span class="line"><span class="comment"># Helper function to show a batch</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_landmarks_batch</span><span class="params">(sample_batched)</span>:</span></span><br><span class="line">    <span class="string">"""Show image with landmarks for a batch of samples."""</span></span><br><span class="line">    images_batch, landmarks_batch = \</span><br><span class="line">            sample_batched[<span class="string">'image'</span>], sample_batched[<span class="string">'landmarks'</span>]</span><br><span class="line">    batch_size = len(images_batch)</span><br><span class="line">    im_size = images_batch.size(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    grid = utils.make_grid(images_batch)</span><br><span class="line">    <span class="comment">#将几张图拼成一张图</span></span><br><span class="line">    plt.imshow(grid.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line"><span class="comment">#表示交换维 前两维变成图片 第三维是这个batchsize</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        plt.scatter(landmarks_batch[i, :, <span class="number">0</span>].numpy() + i * im_size,</span><br><span class="line">                    <span class="comment">#因为四个都拼接在一起 所以x轴上的位置要相对偏移</span></span><br><span class="line">                    landmarks_batch[i, :, <span class="number">1</span>].numpy(),</span><br><span class="line">                    s=<span class="number">10</span>, marker=<span class="string">'.'</span>, c=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">        plt.title(<span class="string">'Batch from dataloader'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_batch, sample_batched <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">    print(i_batch, sample_batched[<span class="string">'image'</span>].size(),</span><br><span class="line">          sample_batched[<span class="string">'landmarks'</span>].size())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># observe 4th batch and stop.</span></span><br><span class="line">    <span class="keyword">if</span> i_batch == <span class="number">3</span>:</span><br><span class="line">        plt.figure()</span><br><span class="line">        show_landmarks_batch(sample_batched)</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        plt.ioff()</span><br><span class="line">        plt.show()</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> pytorch学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch入门系列（2)_莫烦教程</title>
      <link href="/2019/03/21/pytorch%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%EF%BC%882)_%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B/"/>
      <url>/2019/03/21/pytorch%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%EF%BC%882)_%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<ul><li>pytorch建计算图是动态图，TensorFlow是静态图</li></ul><h3 id="pytorch和numpy的对比"><a href="#pytorch和numpy的对比" class="headerlink" title="pytorch和numpy的对比"></a>pytorch和numpy的对比</h3><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment">#将numpy转化为torch</span></span><br><span class="line">torch_data = torch.from_numpy(np_data) </span><br><span class="line"><span class="comment">#将torch再表示为numpy的</span></span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\nnumpy array:'</span>, np_data,          <span class="comment"># [[0 1 2], [3 4 5]]</span></span><br><span class="line">    <span class="string">'\ntorch tensor:'</span>, torch_data,      <span class="comment">#  0  1  2 \n 3  4  5    [torch.LongTensor of size 2x3]</span></span><br><span class="line">    <span class="string">'\ntensor to array:'</span>, tensor2array, <span class="comment"># [[0 1 2], [3 4 5]]</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h3><ul><li><p>variable和Tensor的差别就体现在反向求导时，variable本身就是一个数据格式</p><p>variable设计是为了用来求导的，variable中grad(梯度)，data(数据，是tensor的形式)</p></li></ul><h3 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h3><p>解决不能线性方程求解的问题</p><p>卷积神经网络中常用的是Relu</p><p>在循环神经网络中常用的tanh或者Relu</p><h3 id="建造一个回归网络"><a href="#建造一个回归网络" class="headerlink" title="建造一个回归网络"></a>建造一个回归网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)  <span class="comment"># x data (tensor), shape=(100, 1) 升维 后面的dim表示将升的放在哪维去</span></span><br><span class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.2</span>*torch.rand(x.size())                 <span class="comment"># noisy y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line">x,y=Variable(x),Variable(y)</span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span>  <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature, n_hidden, n_output)</span>:</span></span><br><span class="line">        super(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        <span class="comment"># 定义每层用什么样的形式</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)   <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span>   <span class="comment"># 这同时也是 Module 中的 forward 功能</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.predict(x)             <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">1</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(net)  <span class="comment"># net 的结构</span></span><br><span class="line">plt.ion()   <span class="comment"># 画图 实时输出</span></span><br><span class="line">plt.show()</span><br><span class="line">optimizer=torch.optim.SGD(net.parameters(),lr=<span class="number">0.5</span>)<span class="comment">#定义优化器</span></span><br><span class="line">loss_func=torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):<span class="comment">#开始训练</span></span><br><span class="line">    prediction = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出预测值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(prediction, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值 对于损失函数使用.backward()</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># plot and show learning process</span></span><br><span class="line">        plt.cla()</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">'Loss=%.4f'</span> % loss.data.numpy(), fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h3 id="一个分类网络"><a href="#一个分类网络" class="headerlink" title="一个分类网络"></a>一个分类网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假数据</span></span><br><span class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)         <span class="comment"># 数据的基本形态</span></span><br><span class="line">x0 = torch.normal(<span class="number">2</span>*n_data, <span class="number">1</span>)      <span class="comment"># 类型0 x data (tensor), shape=(100, 2)</span></span><br><span class="line"></span><br><span class="line">y0 = torch.zeros(<span class="number">100</span>)               <span class="comment"># 类型0 y data (tensor), shape=(100, 1)</span></span><br><span class="line">x1 = torch.normal(<span class="number">-2</span>*n_data, <span class="number">1</span>)     <span class="comment"># 类型1 x data (tensor), shape=(100, 1)</span></span><br><span class="line">y1 = torch.ones(<span class="number">100</span>)                <span class="comment"># 类型1 y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据,第二个参数表示要合并第几维)</span></span><br><span class="line">x = torch.cat((x0, x1),<span class="number">0</span>).type(torch.FloatTensor)  <span class="comment"># FloatTensor = 32-bit floating</span></span><br><span class="line">y = torch.cat((y0, y1), ).type(torch.LongTensor)    <span class="comment"># LongTensor = 64-bit integer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#[:]表示复制这个列表</span></span><br><span class="line"><span class="comment">#plt.scatter(x.data.numpy()[:, 1], x.data.numpy()[:, 0], c=y.data.numpy(), s=100, lw=0, cmap='RdYlGn')</span></span><br><span class="line"><span class="comment">#plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span>     <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature, n_hidden, n_output)</span>:</span></span><br><span class="line">        super(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.out = torch.nn.Linear(n_hidden, n_output)       <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.out(x)                 <span class="comment"># 输出值, 但是这个不是预测值, 预测值还需要再另外计算</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">2</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">2</span>) <span class="comment"># 几个类别就几个 output</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.02</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line"><span class="comment"># 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)</span></span><br><span class="line"><span class="comment"># 但是预测值是2D tensor (batch, n_classes) 不同维度的放在一起算交叉熵</span></span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line">plt.ion()   <span class="comment"># 画图</span></span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    out = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出分析值</span></span><br><span class="line">    print(out.size())</span><br><span class="line">    print(y.size())</span><br><span class="line">    loss = loss_func(out, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        plt.cla()</span><br><span class="line">        <span class="comment"># 过了一道 softmax 的激励函数后的最大概率才是预测值</span></span><br><span class="line">        prediction = torch.max(F.softmax(out), <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        pred_y = prediction.data.numpy().squeeze()</span><br><span class="line">        target_y = y.data.numpy()</span><br><span class="line">        plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c=pred_y, s=<span class="number">100</span>, lw=<span class="number">0</span>, cmap=<span class="string">'RdYlGn'</span>)</span><br><span class="line">        accuracy = sum(pred_y == target_y) / <span class="number">200.</span>  <span class="comment"># 预测中有多少和真实值一样</span></span><br><span class="line">        plt.text(<span class="number">1.5</span>, <span class="number">-4</span>, <span class="string">'Accuracy=%.2f'</span> % accuracy, fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>: <span class="string">'red'</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    plt.ioff()  <span class="comment"># 停止画图</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="快速搭建法"><a href="#快速搭建法" class="headerlink" title="快速搭建法"></a>快速搭建法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#搭建网络可以不用创建类</span></span><br><span class="line">net2 = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="保存提取"><a href="#保存提取" class="headerlink" title="保存提取"></a>保存提取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假数据</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)  <span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.2</span>*torch.rand(x.size())  <span class="comment"># noisy y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 建网络</span></span><br><span class="line">    net1 = torch.nn.Sequential(</span><br><span class="line">        torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    optimizer = torch.optim.SGD(net1.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">    loss_func = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        prediction = net1(x)</span><br><span class="line">        loss = loss_func(prediction, y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">torch.save(net1, <span class="string">'net.pkl'</span>)  <span class="comment"># 保存整个网络</span></span><br><span class="line">torch.save(net1.state_dict(), <span class="string">'net_params.pkl'</span>)   <span class="comment"># 只保存网络中的参数 (速度快, 占内存少) </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#提取网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_net</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># restore entire net1 to net2</span></span><br><span class="line">    net2 = torch.load(<span class="string">'net.pkl'</span>)</span><br><span class="line">    prediction = net2(x)</span><br><span class="line">    </span><br><span class="line"> <span class="comment">#提取之前训练的所有参数 这个需要先构建一样的网络</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 新建 net3</span></span><br><span class="line">    net3 = torch.nn.Sequential(</span><br><span class="line">        torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将保存的参数复制到 net3</span></span><br><span class="line">    net3.load_state_dict(torch.load(<span class="string">'net_params.pkl'</span>))</span><br><span class="line">    prediction = net3(x)</span><br></pre></td></tr></table></figure><h3 id="批数据训练"><a href="#批数据训练" class="headerlink" title="批数据训练"></a>批数据训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">5</span>      <span class="comment"># 批训练的数据个数</span></span><br><span class="line"></span><br><span class="line">x = torch.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>)       <span class="comment"># x data (torch tensor)</span></span><br><span class="line">y = torch.linspace(<span class="number">10</span>, <span class="number">1</span>, <span class="number">10</span>)       <span class="comment"># y data (torch tensor)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先转换成 torch 能识别的 Dataset</span></span><br><span class="line">torch_dataset = Data.TensorDataset(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 dataset 放入 DataLoader 每次加载数据库中的batch_size个数据</span></span><br><span class="line">loader = Data.DataLoader(</span><br><span class="line">    dataset=torch_dataset,      <span class="comment"># torch TensorDataset format</span></span><br><span class="line">    batch_size=BATCH_SIZE,      <span class="comment"># mini batch size</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,               <span class="comment"># 要不要打乱数据 (打乱比较好)</span></span><br><span class="line">    num_workers=<span class="number">2</span>,              <span class="comment"># 多线程来读数据</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">3</span>):   <span class="comment"># 训练所有!整套!数据 3 次</span></span><br><span class="line">    <span class="keyword">for</span> step, (batch_x, batch_y) <span class="keyword">in</span> enumerate(loader):  <span class="comment"># 每一步 loader 释放一小批数据用来学习</span></span><br><span class="line">        <span class="comment"># 假设这里就是你训练的地方...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打出来一些数据</span></span><br><span class="line">        print(<span class="string">'Epoch: '</span>, epoch, <span class="string">'| Step: '</span>, step, <span class="string">'| batch x: '</span>,</span><br><span class="line">              batch_x.numpy(), <span class="string">'| batch y: '</span>, batch_y.numpy())</span><br></pre></td></tr></table></figure><h3 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">EPOCH = <span class="number">12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fake dataset</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">1000</span>), dim=<span class="number">1</span>)</span><br><span class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.1</span>*torch.normal(torch.zeros(*x.size()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot dataset</span></span><br><span class="line">plt.scatter(x.numpy(), y.numpy())</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用上节内容提到的 data loader</span></span><br><span class="line">torch_dataset = Data.TensorDataset(x, y)</span><br><span class="line">loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认的 network 形式</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(<span class="number">1</span>, <span class="number">20</span>)   <span class="comment"># hidden layer</span></span><br><span class="line">        self.predict = torch.nn.Linear(<span class="number">20</span>, <span class="number">1</span>)   <span class="comment"># output layer</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># activation function for hidden layer</span></span><br><span class="line">        x = self.predict(x)             <span class="comment"># linear output</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每个优化器创建一个 net</span></span><br><span class="line">net_SGD         = Net()</span><br><span class="line">net_Momentum    = Net()</span><br><span class="line">net_RMSprop     = Net()</span><br><span class="line">net_Adam        = Net()</span><br><span class="line">nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]</span><br><span class="line"></span><br><span class="line"><span class="comment"># different optimizers</span></span><br><span class="line">opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)</span><br><span class="line">opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.8</span>)</span><br><span class="line">opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=<span class="number">0.9</span>)</span><br><span class="line">opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</span><br><span class="line">optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</span><br><span class="line"></span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line">losses_his = [[], [], [], []]   <span class="comment"># 记录 training 时不同神经网络的 loss</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">    print(<span class="string">'Epoch: '</span>, epoch)</span><br><span class="line">    <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> enumerate(loader):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对每个优化器, 优化属于他的神经网络</span></span><br><span class="line">        <span class="keyword">for</span> net, opt, l_his <span class="keyword">in</span> zip(nets, optimizers, losses_his):</span><br><span class="line">            output = net(b_x)              <span class="comment"># get output for every net</span></span><br><span class="line">            loss = loss_func(output, b_y)  <span class="comment"># compute loss for every net</span></span><br><span class="line">            opt.zero_grad()                <span class="comment"># clear gradients for next train</span></span><br><span class="line">            loss.backward()                <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">            opt.step()                     <span class="comment"># apply gradients</span></span><br><span class="line">            l_his.append(loss.data.numpy())     <span class="comment"># loss recoder</span></span><br><span class="line">  labels = [<span class="string">'SGD'</span>, <span class="string">'Momentum'</span>, <span class="string">'RMSprop'</span>, <span class="string">'Adam'</span>]</span><br><span class="line"><span class="keyword">for</span> i, l_his <span class="keyword">in</span> enumerate(losses_his):</span><br><span class="line">    plt.plot(l_his, label=labels[i])</span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Steps'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">    plt.ylim((<span class="number">0</span>, <span class="number">0.2</span>))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> pytorch学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 3_10-3_16周报</title>
      <link href="/2019/03/17/2019%203_10-3_16%E5%91%A8%E6%8A%A5/"/>
      <url>/2019/03/17/2019%203_10-3_16%E5%91%A8%E6%8A%A5/</url>
      
        <content type="html"><![CDATA[<h3 id="这周完成的事"><a href="#这周完成的事" class="headerlink" title="这周完成的事"></a>这周完成的事</h3><a id="more"></a><ul><li>看了Inception系列的最后两篇论文，没有去跑一跑他们也只能是知道个大概</li><li>看了十节 李宏毅的机器学习</li><li>大概了解了机器学习中的贝叶斯学派</li></ul><h3 id="计划这周完成却没有完成的事"><a href="#计划这周完成却没有完成的事" class="headerlink" title="计划这周完成却没有完成的事"></a>计划这周完成却没有完成的事</h3><ul><li><p>莫烦的pytorch教程没看完</p></li><li><p>代码又没跑 一是这两周gpu满负载运行根本跑不了 二是也没有时间</p></li><li>没跑步</li><li>李宏毅的课计划看到19 现在只看到13</li></ul><h3 id="下周计划"><a href="#下周计划" class="headerlink" title="下周计划"></a>下周计划</h3><ul><li><p>莫烦 pytorch教程看完（10小时）</p></li><li><p>李宏毅的课 再看十节（20小时）</p></li><li><p>论文先缓一缓 多运行运行代码 重点在pytorch（30小时）</p></li><li><p>写一篇总结支持向量机（10小时）</p></li></ul><h3 id="一些感想"><a href="#一些感想" class="headerlink" title="一些感想"></a>一些感想</h3><p>多实践多实践，不能沉浸在走马观花的学习里。提高效率，要明确做每一件事都有什么用。</p>]]></content>
      
      
      <categories>
          
          <category> 周报 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记_李宏毅（10-12）</title>
      <link href="/2019/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%8810-12%EF%BC%89/"/>
      <url>/2019/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%8810-12%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-10-Convolutional-Neural-Network"><a href="#Lecture-10-Convolutional-Neural-Network" class="headerlink" title="Lecture 10 Convolutional Neural Network"></a>Lecture 10 Convolutional Neural Network</h2><h3 id="为什么要有卷积来处理图像"><a href="#为什么要有卷积来处理图像" class="headerlink" title="为什么要有卷积来处理图像"></a>为什么要有卷积来处理图像</h3><ul><li><p>图像拉成一个向量的话 ，有很多维，如果用一般的全连接神经网络来处理，参数太多了。</p></li><li><p>特征往往只在图像的一小部分，只需要读一部分图像就能找到我们想要用以区分的特征</p></li><li><p>在不同图像或者整张图像的不同尺寸的不同部分可能有相同的特征，要是把整张图片喂给网络的话，可能就比较难找到他们的相似。</p><a id="more"></a></li></ul><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8510_1.png" alt></p><h3 id="卷积的各个部件"><a href="#卷积的各个部件" class="headerlink" title="卷积的各个部件"></a>卷积的各个部件</h3><p>卷积网络实际上就是去掉一些权重的神经网络，并且共享一些权重，所以训练也是BP,共享权重就把要共享的权重求出来再平均。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8510_2.png" alt></p><h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>池化层有最大池化核平均池化。</p><h4 id="flatten和全连接"><a href="#flatten和全连接" class="headerlink" title="flatten和全连接"></a>flatten和全连接</h4><h3 id="CNN学习了什么"><a href="#CNN学习了什么" class="headerlink" title="CNN学习了什么"></a>CNN学习了什么</h3><p>可以把输入图片做参数，找到使某一个卷积核激励最大的图片，观察这个卷积核和图片的关系。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8510_3.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8510_4.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8510_5.png" alt></p><h4 id="Deep-Dream"><a href="#Deep-Dream" class="headerlink" title="Deep Dream"></a>Deep Dream</h4><p>机器增加他所看到的东西</p><h4 id="Deep-Style"><a href="#Deep-Style" class="headerlink" title="Deep Style"></a>Deep Style</h4><p>让一些照片相似于一些名画的风格</p><h3 id="CNN的应用"><a href="#CNN的应用" class="headerlink" title="CNN的应用"></a>CNN的应用</h3><ul><li>围棋</li></ul><p>围棋用全连接就能工作了，但是用CNN有着更好的表现。Alpho go 没有用池化层，要针对事物的本质来设计网络架构。</p><ul><li><p>语音识别</p><p>spectrogram.一张时间和频率的二维图。滤波器一般只在频率轴上移动，不在时间轴上移动。 </p></li><li><p>文字处理</p><p>word embeding,把文字序列当成向量。滤波器只在时间轴上移动。</p></li></ul><h2 id="Lecture-11-Why-Deep"><a href="#Lecture-11-Why-Deep" class="headerlink" title="Lecture 11 Why Deep"></a>Lecture 11 Why Deep</h2><p>实验证明，差不多的参数，瘦长的网络结构要比矮胖的表现好。</p><h3 id="逻辑上解释"><a href="#逻辑上解释" class="headerlink" title="逻辑上解释"></a>逻辑上解释</h3><p>类似于模组化。更深的网络，使更多的模块相对独立，从而可以重复使用。好比写程序不能什么都写在主函数里，多写几个子函数方便调用。</p><p>虽然说理论上一个隐藏层就可以模仿所有函数，但是这个效率明显不如深层网络</p><p>实现端到端的任务，少了很多人去提取特征的环节，不管是语音还是图像。</p><h3 id="类比（analogy"><a href="#类比（analogy" class="headerlink" title="类比（analogy)"></a>类比（analogy)</h3><p>多层电路可以用更少的元器件实现我们想要实现的功能</p><h3 id="语音上用到deep-learning"><a href="#语音上用到deep-learning" class="headerlink" title="语音上用到deep learning"></a>语音上用到deep learning</h3><p>语言是由phoneme（类似音标）组成的，每个phoneme的发音（不同顺序组成了tri-phone)又都有好几种。</p><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ul><li><p>把acoustic转成state</p><p>acoustic:声音信号是一段波浪信号，在信号上截取一个窗口，用特征来描述这个窗口就叫做acoustic feature。一串声音信号转换之后就叫做acoustic feature sequence.</p></li><li><p>然后把state转成phoneme，再将phoneme转成文字</p></li><li><p>接下来还要考虑同音异字的问题</p></li></ul><h4 id="传统方法和CNN做比较"><a href="#传统方法和CNN做比较" class="headerlink" title="传统方法和CNN做比较"></a>传统方法和CNN做比较</h4><ul><li><p>HMM-GMM</p><p>传统方法，假设每一个state的acoustic feature是固定分布的。但是搭配之后的模型太多了，参数也就不多了。</p></li><li><p>实际上发音是有联系的，和深度学习的模块化有点相似，实际上用到的参数还不如GMM。</p></li></ul><h2 id="Lecture-12-Semi-supervised"><a href="#Lecture-12-Semi-supervised" class="headerlink" title="Lecture 12 Semi-supervised"></a>Lecture 12 Semi-supervised</h2><h3 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h3><p>有两部分数据，一部分是有标签，一部分没有标签。一般来说，没有标签的数据远远大于有标签的数据。</p><p>半监督学习有两种，一种叫做Transductive learning,另一种叫做inductive learning。在训练过程中，已知testing data（unlabelled data）是transductive learing，在训练过程中，并不知道testing data ，训练好模型后去解决未知的testing data 是inductive learing。</p><h3 id="为什么要半监督学习"><a href="#为什么要半监督学习" class="headerlink" title="为什么要半监督学习"></a>为什么要半监督学习</h3><p>虽然可以获取很多数据，但是有标签的数据很少。而且人在成长过程中也不会什么事都有lable，需要根据之前有标签的数据去学习这些没有标签的数据。</p><h3 id="为什么半监督学习有用"><a href="#为什么半监督学习有用" class="headerlink" title="为什么半监督学习有用"></a>为什么半监督学习有用</h3><h4 id="在产生模型的使用使用半监督学习"><a href="#在产生模型的使用使用半监督学习" class="headerlink" title="在产生模型的使用使用半监督学习"></a>在产生模型的使用使用半监督学习</h4><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8512_1.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8512_2.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8512_3.png" alt></p><h4 id="Low-density-Separation"><a href="#Low-density-Separation" class="headerlink" title="Low-density Separation"></a>Low-density Separation</h4><p>意思是几个类别之间有明显的分界线，未标记数据都是非黑即白的。</p><h5 id="Self-training："><a href="#Self-training：" class="headerlink" title="Self-training："></a>Self-training：</h5><p>（不能用在回归问题）和之前的用高斯分布生成模型很像，只是这个无标签数据加的标签是固定的，而之前那个是带可能性的。对于神经网络结构，self-training是有用的，但是soft-lable是不起工作。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8512_4.png" alt></p><h5 id="Entropy-based-Regularization："><a href="#Entropy-based-Regularization：" class="headerlink" title="Entropy-based Regularization："></a>Entropy-based Regularization：</h5><p>假设数据的标签是有分布，需要找出一个参数使分布集中于某一类的分布。所以用交叉熵来表示这部分并加入到最后损失函数的计算。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8512_5.png" alt></p><h5 id="Semi-supervised-SVM"><a href="#Semi-supervised-SVM" class="headerlink" title="Semi-supervised SVM"></a>Semi-supervised SVM</h5><p>把损失分成两部分，一个有标签数据的分割线，第二个所有可能数据而产生的分割线。找一条分割线使有标签的数据分得尽可能开，也使未标记的数据的错误率尽可能低。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8512_6.png" alt></p><h5 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h5><p>思想：近朱者赤近墨者黑</p><p>如果未标记数据和标记数据之间有足够多的相似的过渡，可以假设这个未标记数据的标签可能和标记数据一样。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8512_7.png" alt></p><p>具体操作的方法：</p><ul><li>先聚类，再分类</li></ul><p>或者</p><ul><li><p>Graph-based Approach</p><p>A点能不能通过graph上的边走到B点，建graph的方法是很重要的。一个常用方法是</p><ul><li>先描述各个点之间的相似程度（比如RBM)</li><li>然后根据这些相似度在点之间加边。这些边也可以加上权重</li></ul></li></ul><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%8512_8.png" alt></p><p>最后的损失函数就是加上分类之后的所有数据的光滑度（也就是所有标签相同的数据之间的距离和）</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inception-v4和Inception_ResNet</title>
      <link href="/2019/03/16/Inception-v4%E5%92%8CInception_ResNet/"/>
      <url>/2019/03/16/Inception-v4%E5%92%8CInception_ResNet/</url>
      
        <content type="html"><![CDATA[<p>这篇论文主要提出了两种架构，inception的又一个inception V4和inception 与residual架构相结合的Inception-Resnet架构。</p><p>具体这篇论文没有什么理论，就是把两种热门架构融合在一起，然后改善了结果。</p><h3 id="Inception-V4"><a href="#Inception-V4" class="headerlink" title="Inception-V4"></a>Inception-V4</h3><a id="more"></a><p>inceptionv4在v3的版本上，增多了一些层次，使结构更复杂。为了减少计算量，引进了两个reduction模块，利用v3讲过的并行，不对称卷积和$1 \times 1$卷积来减少计算量。以下三图分别是总设计图、Inception部分、reduction部分。</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/Inception_V4.png" alt></p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/Inceptionv4_1.png" alt></p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/inceptionv4_2.png" alt></p><h3 id="Inception-ResNet"><a href="#Inception-ResNet" class="headerlink" title="Inception_ResNet"></a>Inception_ResNet</h3><p>论文设计了两种Inception_ResNet,Inception_Resnet_v1跟Inception_v3计算量大致相同。在Inception_Resnet中，为了节约计算量并且多塞几个Inception模块，没有对所有层使用BN。Inception_Resnet_V2和Inception_V4大致计算量相同。以下两张图分别是总设计图、Inception_Resnet部分.</p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/Inception_ResNet.png" alt></p><p><img src="https://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/Inceptionv4_3.png" alt></p><h3 id="残参模块的不稳定性"><a href="#残参模块的不稳定性" class="headerlink" title="残参模块的不稳定性"></a>残参模块的不稳定性</h3><p>当滤波器很多时（超过1000），残差网络会不稳定。之前的做法是先用小的学习率再用大的学习率，本篇论文觉得以上不管用，提出了放缩的做法，将残差模块送到激活层之前乘一个放缩因子（0.1到0.3）</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>InceptionV4和Inception_resnet的准确度大似相同，但是有残差模块训练会快很多。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inception-V3</title>
      <link href="/2019/03/14/Inception-V3/"/>
      <url>/2019/03/14/Inception-V3/</url>
      
        <content type="html"><![CDATA[<p>通过分解卷积核和加入正则化，来增大网络。并分享了一些增大网络而不会使性能下降的原则。</p><h3 id="大规模网络的设计原则"><a href="#大规模网络的设计原则" class="headerlink" title="大规模网络的设计原则"></a>大规模网络的设计原则</h3><h4 id="避免特征瓶颈"><a href="#避免特征瓶颈" class="headerlink" title="避免特征瓶颈"></a>避免特征瓶颈</h4><p>在深度网络中，数据从输入端流向输出端，在训练的时候不能极端压缩表征（大概不能为了减少计算量，采用步长大的卷积核或池化）</p><h4 id="更高维度的表示在网络中更容易局部处理。在卷积网络中增加每个图块的激活允许更多解耦的特征。所产生的网络将训练更快。"><a href="#更高维度的表示在网络中更容易局部处理。在卷积网络中增加每个图块的激活允许更多解耦的特征。所产生的网络将训练更快。" class="headerlink" title="更高维度的表示在网络中更容易局部处理。在卷积网络中增加每个图块的激活允许更多解耦的特征。所产生的网络将训练更快。"></a>更高维度的表示在网络中更容易局部处理。在卷积网络中增加每个图块的激活允许更多解耦的特征。所产生的网络将训练更快。</h4><a id="more"></a><p>解释：高维度特征是指网络中经过几层处理后的特征，可以把这些特征可做图片的高级表示，例如不同feature map表示不同特征；处理这样的特征比处理原始图片更加容易。 非线性变化可以解耦和图像的特征，例如把图片内容分开到不同的feature map；增加非线性能更好抽取不同特征，训练更快</p><h4 id="在低维是可以进行空间聚合，而且不会造成太大损失"><a href="#在低维是可以进行空间聚合，而且不会造成太大损失" class="headerlink" title="在低维是可以进行空间聚合，而且不会造成太大损失"></a>在低维是可以进行空间聚合，而且不会造成太大损失</h4><p>猜测是低维数据相邻单元相关性比较强，聚合一下损失的信息不多，而且还能加速网络训练</p><h4 id="平衡网络的宽度和深度"><a href="#平衡网络的宽度和深度" class="headerlink" title="平衡网络的宽度和深度"></a>平衡网络的宽度和深度</h4><p>两者并行增加，则可以达到恒定计算量的最佳改进。因此，计算预算应该在网络的深度和宽度之间以平衡方式进行分配。（照搬论文，没有解释）</p><h3 id="对大滤波器进行因式分解"><a href="#对大滤波器进行因式分解" class="headerlink" title="对大滤波器进行因式分解"></a>对大滤波器进行因式分解</h3><p>这部分是这篇论文的重点和贡献点。</p><h4 id="大卷积变小卷积（这个好像在之前网络就有提出来）"><a href="#大卷积变小卷积（这个好像在之前网络就有提出来）" class="headerlink" title="大卷积变小卷积（这个好像在之前网络就有提出来）"></a>大卷积变小卷积（这个好像在之前网络就有提出来）</h4><p>大卷积的计算量比小卷积大很多，但是大卷积有着更大的感受野，所以考虑用多层小卷积来取代大卷积。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/inceptionv3_1.png" alt></p><p>而且经过实验，修正线性单元比线性激活性能好</p><h4 id="因式分解成不对称卷积"><a href="#因式分解成不对称卷积" class="headerlink" title="因式分解成不对称卷积"></a>因式分解成不对称卷积</h4><p>将n<em>n卷积核分解成1\</em>n,后接一个n*1的卷积核，计算量大大减少。但是这么分解在前面的层次不能很好的工作，在中间的特征图上（论文给出12到20之间）表现良好。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/inceptionv3_2.png" alt></p><h3 id="辅助分类器"><a href="#辅助分类器" class="headerlink" title="辅助分类器"></a>辅助分类器</h3><p>辅助分类器就是V1上那两个在中间层经过softmax以一定权重直接送到最后的技巧。</p><p>之前的认为这个是解决梯度消失问题，把下层的梯度送到最后，有助于演变底层的特征很有可能是不对的。因为把最下面的移除，结果没有变化。</p><p>论文认为这个方式就是正则化的变形。</p><p>（结构太复杂，没有理论研究，论文是通过实验来猜想，没有确凿证据）</p><h4 id="网格尺寸减少"><a href="#网格尺寸减少" class="headerlink" title="网格尺寸减少"></a>网格尺寸减少</h4><p>为了避免之前说的陷入特征瓶颈（也就是要保证有足够的特征往后传播），在池化前会增加滤波器的数量来保持特征，但是这样又会增加计算量。如下第一图右边左式，然后提出了一种并行结构，下图第二图所示，左边卷积右边池化。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/inceptionv3_3.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/inceptionv3_4.png" alt></p><h3 id="光滑标签来正则化分类层"><a href="#光滑标签来正则化分类层" class="headerlink" title="光滑标签来正则化分类层"></a>光滑标签来正则化分类层</h3><p>损失函数定义为交叉熵函数，论文觉得这会发生过拟合。所以对训练样本的标签加入一项均匀分布，下式第一项就是数据中的分布，第二项是固定分布。（称为LSR,或者标签平滑正则化，论文中有较为详细的公式推导）</p><p>​     q′(k|x)=(1−ϵ)δk,y+ϵu(k)</p><h3 id="处理低分辨率输入"><a href="#处理低分辨率输入" class="headerlink" title="处理低分辨率输入"></a>处理低分辨率输入</h3><p>通常方法是使用高分辨率感受野的模型，但是作者认为这是高分辨率需要的高计算量带来的效果。在一样的计算量限制中，低分辨率虽然训练比较慢，但是达到的效果还是差不多。</p><p>作者给了一个推荐的方法，在较低分辨率输入的情况下，减少前两层的步长，或者简单地移除网络的第一个池化层。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记_李宏毅（7-9）</title>
      <link href="/2019/03/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%887-9%EF%BC%89/"/>
      <url>/2019/03/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%887-9%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-7-Back-propagation"><a href="#Lecture-7-Back-propagation" class="headerlink" title="Lecture 7  Back propagation"></a>Lecture 7  Back propagation</h2><p>应用Gradient Descent,只是数据多很多，需要应用链式法则。</p><p>当损失函数对一个W求偏导的时候，可以分成两部分，forwardpass和backpass</p><a id="more"></a><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%857_1.png" alt></p><h3 id="其中forwardpass"><a href="#其中forwardpass" class="headerlink" title="其中forwardpass"></a>其中forwardpass</h3><p>只需要看当前w的输入是什么，也就是上一层的output</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/forepass.png" alt></p><h3 id="backwardpass"><a href="#backwardpass" class="headerlink" title="backwardpass"></a>backwardpass</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%857_2.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%857_3.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%857_4.png" alt></p><p>算back的时候，实际相当建一个网络，然后还是算向前传播</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%857_5.png" alt></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%857_6.png" alt></p><h2 id="Lecture-8-Keras"><a href="#Lecture-8-Keras" class="headerlink" title="Lecture 8 Keras"></a>Lecture 8 Keras</h2><p>Keras可以说是tensorflow的interface，也就是集合一些TensorFlow的步骤，帮助更好地集中注意在搭建网络上。</p><p>具体代码就大概看一下，有需要再去看官方文档。</p><p>当batch size=1的时候，mini bathc训练就是stochastic gradient descent。在实际操作的时候,两者训练同样的数据同样的次数需要的时间差不多，而且mini batch比较稳定。那为什么不全都扔进去运算呢？一是因为GPU可能不能支持这么多数据平行运算。二是因为batch size设很大的话，可能一下子就陷入saddle point，或者局部最优。mini batch加入了一些随机性，对于跳出局部最优。</p><h2 id="Lecture-9-Tips-For-Training-DNN"><a href="#Lecture-9-Tips-For-Training-DNN" class="headerlink" title="Lecture 9 Tips For Training DNN"></a>Lecture 9 Tips For Training DNN</h2><h3 id="卷积网络训练一般步骤"><a href="#卷积网络训练一般步骤" class="headerlink" title="卷积网络训练一般步骤"></a>卷积网络训练一般步骤</h3><p>·<img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%859_1.png" alt="9_1"></p><h3 id="训练两大问题"><a href="#训练两大问题" class="headerlink" title="训练两大问题"></a>训练两大问题</h3><h4 id="在训练数据就表现不好"><a href="#在训练数据就表现不好" class="headerlink" title="在训练数据就表现不好"></a>在训练数据就表现不好</h4><p>可能的处理方法 更换学习率或者激活函数</p><h4 id="在测试数据上表现不好"><a href="#在测试数据上表现不好" class="headerlink" title="在测试数据上表现不好"></a>在测试数据上表现不好</h4><p>可能的处理方法   dropout 正则化 earlystoppping</p><h4 id="另外，overfitting是指训练数据的准确率不错，测试数据表现不好。不能什么都让overfitting背锅。"><a href="#另外，overfitting是指训练数据的准确率不错，测试数据表现不好。不能什么都让overfitting背锅。" class="headerlink" title="另外，overfitting是指训练数据的准确率不错，测试数据表现不好。不能什么都让overfitting背锅。"></a>另外，overfitting是指训练数据的准确率不错，测试数据表现不好。不能什么都让overfitting背锅。</h4><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p>随着网络层数的增加会造成梯度消失。</p><h4 id="激活函数Relu"><a href="#激活函数Relu" class="headerlink" title="激活函数Relu"></a>激活函数Relu</h4><p>max(0,x)，当变量小于零的时候，参数就不会改变，我们可以稍稍给他点斜度，如0.01z,这种方法叫做Leaky Relu. 或者不用固定参数0.01,用网络中的参数，αz，这种方法叫做Parametric Relu。</p><p>也有自学的激励网络，叫Maxout.类似于在层上做Max pooling。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%859_2.png" alt="9_2"></p><p>maxout是可以模仿Relu</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%859_3.png" alt="9_3"></p><h4 id="maxout的训练"><a href="#maxout的训练" class="headerlink" title="maxout的训练"></a>maxout的训练</h4><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%859_4.png" alt></p><h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><h4 id="adagrad"><a href="#adagrad" class="headerlink" title="adagrad"></a>adagrad</h4><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%859_5.png" alt></p><h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p>因为损失函数可能是不规则的，要求在同一个方向上学习率也要快速的变化，adagrad就不能满足了</p><p>（这个方法没有论文 是在线上课程提出来的）</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%859_6.png" alt></p><p>RMSProp对不同时候算出来的梯度采取了不一样的信任度，而不是像adagrad直接把之前的梯度求和。</p><h3 id="处理局部最优或者plateau"><a href="#处理局部最优或者plateau" class="headerlink" title="处理局部最优或者plateau"></a>处理局部最优或者plateau</h3><p>Yann Lecun提出由于特征很多，要所有特征在某一个点都局部最低，才会导致结果陷在局部最低里，所以局部最优并不会经常遇见。</p><h4 id="momentum"><a href="#momentum" class="headerlink" title="momentum"></a>momentum</h4><p>不同于一般的梯度下降，momentum在训练的时候还会考虑之前时间的梯度，类似于现实世界的惯性。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%859_7.png" alt></p><h4 id="adam"><a href="#adam" class="headerlink" title="adam"></a>adam</h4><p>就是RMSProp+Momentum</p><h3 id="处理过拟合"><a href="#处理过拟合" class="headerlink" title="处理过拟合"></a>处理过拟合</h3><h4 id="early-stopping"><a href="#early-stopping" class="headerlink" title="early stopping"></a>early stopping</h4><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%859_8.png" alt="9_8"></p><h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>做正则项是为了曲线更平滑，所以一般只考虑权重，不会考虑bias.在实际中，我们会使权重越来越接近0.和我们训练的目的使权重远离0有些冲突。而且early stop也有点阻止我们的权重零太远，正则化不是不可替代的。所以，在神经网络中，正则化的帮助并不明显。</p><p>正则化不一定要用参数的L2,也有用L1.不过L2使权重接近0的方法是乘一个权重，而L1是通过一直减一个值。所以当权重很大的时候，L2会下降比较快。另一方面，权重比较小的话，权重下降就比较快。L2算出来的所有权重比较平均，L1算出来的权重分布比较稀疏。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%859_9.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%859_10.png" alt></p><h4 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h4><p>训练的时候以一定几率随机丢弃一些神经元</p><p>在测试的时候不要加dropout，但是要注意到输出的结果乘以一定的权重（1-P%) p是dropout的概率。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%859_11.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%859_12.png" alt></p><p>dropout为什么有用</p><p>​    可能的解释是类似于ensemble的做法</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习概率部分（1）</title>
      <link href="/2019/03/12/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%8C%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%E5%92%8C%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/03/12/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%8C%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%E5%92%8C%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h3 id="概率和统计"><a href="#概率和统计" class="headerlink" title="概率和统计"></a>概率和统计</h3><p>首先，我们需要先分清楚概率和统计的关系</p><p>概率研究的问题是，已知一个模型和参数，怎么去预测这个模型产生的结果的特性（例如均值，方差，协方差等等）。 </p><p>统计研究的问题则相反。统计是，有一堆数据，要利用这堆数据去预测模型和参数。</p><p>一句话总结：<strong>概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。</strong>机器学习中常要解决的问题是统计问题。</p><a id="more"></a><h3 id="频率学派和贝叶斯学派"><a href="#频率学派和贝叶斯学派" class="headerlink" title="频率学派和贝叶斯学派"></a>频率学派和贝叶斯学派</h3><p>（备注：本部分照搬自 <a href="https://www.zhihu.com/question/20587681/answer/17435552" target="_blank" rel="noopener">https://www.zhihu.com/question/20587681/answer/17435552</a>  认真看一遍豁然开朗）</p><p>简单地说，频率学派与贝叶斯学派探讨「不确定性」这件事时的出发点与立足点不同。频率学派从「自然」角度出发，试图直接为「事件」本身建模，即事件A在独立重复试验中发生的频率趋于极限p，那么这个极限就是该事件的概率。举例而言，想要计算抛掷一枚硬币时正面朝上的概率，我们需要不断地抛掷硬币，当抛掷次数趋向无穷时正面朝上的频率即为正面朝上的概率。</p><p>然而，贝叶斯学派并不从试图刻画「事件」本身，而从「观察者」角度出发。贝叶斯学派并不试图说「事件本身是随机的」，或者「世界的本体带有某种随机性」，这套理论根本不言说关于「世界本体」的东西，而只是从「观察者知识不完备」这一出发点开始，构造一套在贝叶斯概率论的框架下可以对不确定知识做出推断的方法。频率学派下说的「随机事件」在贝叶斯学派看来，并不是「事件本身具有某种客观的随机性」，而是「观察者不知道事件的结果」而已，只是「观察者」知识状态中尚未包含这一事件的结果。但是在这种情况下，观察者又试图通过已经观察到的「证据」来推断这一事件的结果，因此只能靠猜。贝叶斯概率论就想构建一套比较完备的框架用来描述最能服务于理性推断这一目的的「猜的过程」。因此，在贝叶斯框架下，同一件事情对于知情者而言就是「确定事件」，对于不知情者而言就是「随机事件」，随机性并不源于事件本身是否发生，而只是描述观察者对该事件的知识状态。</p><p>总的来说，贝叶斯概率论为人的知识（knowledge）建模来定义「概率」这个概念。频率学派试图描述的是「事物本体」，而贝叶斯学派试图描述的是观察者知识状态在新的观测发生后如何更新。为了描述这种更新过程，贝叶斯概率论假设观察者对某事件处于某个知识状态中（例如：小明先验地相信一枚硬币是均匀的，可能是出于认为均匀硬币最常见这种信念），之后观察者开始新的观测或实验（小明开始不断地抛硬币，发现抛了100次后，居然只有20次是正面朝上）。经过中间的独立重复试验，观察者获得了一些新的观测结果，这些新的观测将以含有不确定性的逻辑推断的方式影响观察者原有的信念（小明开始怀疑这枚硬币究竟是不是均匀的，甚至开始断定硬币并不均匀）。在这一过程中，观察者无法用简单的逻辑来推断，因为观察者并没有完全的信息作为证据，因此只能采用似真推断（plausible reasoning），对于各种各样可能的结果赋予一个「合理性」（plausibility）。例子中，小明原先认为硬币的分布是均匀的，于是根据小明原有的信念，这个论断合理性非常高；在观察到100次抛掷中只有20次正面朝上后，小明开始怀疑硬币的均匀性，此时小明很可能认为「硬币不均匀」这一推断的合理性很高，支持的证据就是他刚刚实验的观测结果。</p><p>上面的例子用贝叶斯概率论的语言来描述，就是观察者持有某个前置信念（prior<br>belief），通过观测获得统计证据（evidence），通过满足一定条件的逻辑一致推断得出的关于该陈述的「合理性」，从而得出后置信念（posterior belief）来最好的表征观测后的知识状态（state of knowledge）。这里，贝叶斯概率推断所试图解决的核心问题就是如何构建一个满足一定条件的逻辑体系赋予特定论断一个实数所表征的论断合理性的度量（measure of plausibility），从而可以允许观测者在不完全信息的状态下进行推断。这里，观察者对某变量的信念或知识状态就是频率学派所说的「概率分布」，也就是说，观察者的知识状态就是对被观察变量取各种值所赋予的「合理性」的分布。</p><p>从这个意义上来讲，贝叶斯概率论试图构建的是知识状态的表征，而不是客观世界的表征。因此，在机器学习、统计推断中，许多情况下贝叶斯概率推断更能解决观察者推断的问题，而绕开了关于事件本体的讨论，因为没有讨论本体的必要性。</p><p>频率学派的代表是最大似然估计；贝叶斯学派的代表是最大后验概率估计。</p><h3 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h3><p>贝叶斯定理，也就是计算条件概率的公式。根据常识，$P(A |B)$=$\frac{P(A \cap B)}{P(B)}$,</p><p>可得$P(A \cap B)=P(A |B) \times P(B)$ </p><p>同理可得$P(A \cap B)=P(B |A) \times P(A)$</p><p>所以 $P(A |B)=\frac{P(B |A) \times P(A)}{P(B)}​$。</p><p>将上式变形得$P(A |B)=P(A) \times \frac{P(B |A) }{P(B)}$,我们将$P(A)$称为先验概率，也就是事件B发生前，事件A发生的概率。$P(A|B)$称为后验概率。$\frac{P(B |A) }{P(B)}$是事件B发生后对事件A概率的重新评估，称为可能性函数，这是一个调整因子。条件概率可以理解为$后验概率 = 先验概率  \times 调整因子$。简单来说，调整因子就是事件B发生之后，会不会佐证了事件A发生的可能性。$P(B|A)$比$P(B)$大的话，就是事件A发生的情况下事件B发生的情况比单独事件B发生的情况大的话，而且事件B已经发生了，就有足够理由增大事件A已经发生的可能性，也就是调整因子大于1。</p><h3 id="朴素贝叶斯法"><a href="#朴素贝叶斯法" class="headerlink" title="朴素贝叶斯法"></a>朴素贝叶斯法</h3><p>朴素贝叶斯就是基于上述定理，并且假定各个特征之间是相互独立的。通过训练数据学习先验概率和$P(B|A)$,利用全概率公式也可得到了$P(B)​$。测试的时候把特征代入，找到满足最大的后验概率的事件A即可。在测试的时候也有不同的需求，比如最小错误率，最小风险之类的决策偏好。</p><p>在学习$P(B|A)和P(A)$时，需要用到概率密度函数，可以应用极大似然估计，或者贝叶斯估计。</p><h3 id="最大似然估计（Maximum-likelihood-estimation-MLE）"><a href="#最大似然估计（Maximum-likelihood-estimation-MLE）" class="headerlink" title="最大似然估计（Maximum likelihood estimation, MLE）"></a>最大似然估计（Maximum likelihood estimation, MLE）</h3><p>对于P(x|θ)，x表示一个具体的数据，θ表示模型的参数。当θ是确定时，x是变量，这个函数叫做概率函数；当x是确定的，θ是变量，这个函数就叫做似然函数了。</p><p>最大似然估计就是最大化这个似然函数，通常方法是先求对数，再求导找极大值。</p><p>总结来说，最大似然估计是已知模型，是利用已知的样本结果信息，反推出最具有可能导致这些样本结果出现的模型参数值。</p><h3 id="最大后验概率估计（Maximum-a-posteriori-estimation-MAP）"><a href="#最大后验概率估计（Maximum-a-posteriori-estimation-MAP）" class="headerlink" title="最大后验概率估计（Maximum a posteriori estimation, MAP）"></a>最大后验概率估计（Maximum a posteriori estimation, MAP）</h3><p>最大似然估计是求参数θ，使似然函数最大，而最大后验概率估计是求参数θ，使$P(x_0|θ)P(θ)$最大,就是引进了先验概率，类似于最大化P(θ|x),可以被当成正则化的最大似然函数。</p><p>最大后验概率估计认为变量θ具有某种概率分布，计算时要将这个先验概率考虑进来。</p><h3 id="贝叶斯估计（Bayes-estimation-BS"><a href="#贝叶斯估计（Bayes-estimation-BS" class="headerlink" title="贝叶斯估计（Bayes estimation,BS)"></a>贝叶斯估计（Bayes estimation,BS)</h3><p>贝叶斯估计是最大后验估计的进一步拓展，同样假定θ是一个随机变量，但是不是像最大后验概率估计直接估计出θ的某个特征值，而是估计θ的分布。在贝叶斯估计中除了类条件概率密度符合一定的先验分布，参数θ也符合正态分布，我们需要通过贝叶斯规则将参数的先验分布转换成后验分布进行求解。</p><p>贝叶斯估计和最大后验概率估计的最大差别就是贝叶斯估计考虑了所有可能的参数值并且将他们加权平均出一个对参数的估计，而后者直接选择后验概率最大的参数。</p><p>将参数θ视为随机变量，求它在最小误差意义下的估计。在所有的样本和θ参数空间中，求出平均损失。我们想要的θ就是使这个平均损失最小。经推导，等价于求条件风险最小 </p><p><img src="https://img-blog.csdn.net/20170606223909132" alt="img"></p><h3 id="贝叶斯学习"><a href="#贝叶斯学习" class="headerlink" title="贝叶斯学习"></a>贝叶斯学习</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0.png" alt="在贝叶斯估计中，θ的估计量为$\int \limits_\Theta θ P(θ|x)dθ​$,其中P(θ|x)是由贝叶斯定理得到的"></p><h3 id="备注：本文仅做记录学习心得之用，没有完善公式具体推导等。"><a href="#备注：本文仅做记录学习心得之用，没有完善公式具体推导等。" class="headerlink" title="备注：本文仅做记录学习心得之用，没有完善公式具体推导等。"></a>备注：本文仅做记录学习心得之用，没有完善公式具体推导等。</h3>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记_李宏毅（4-6）</title>
      <link href="/2019/03/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%884-6%EF%BC%89/"/>
      <url>/2019/03/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%884-6%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-4-Classification"><a href="#Lecture-4-Classification" class="headerlink" title="Lecture 4 Classification"></a>Lecture 4 Classification</h2><ul><li>回归问题和分类问题对模型的好坏判断标准不一样，所以一般不能使用回归问题来解决分类问题</li></ul><h3 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h3><ul><li><p>信用评级</p></li><li><p>医疗诊断</p></li><li><p>手写</p></li><li><p>人脸识别</p><a id="more"></a></li></ul><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li><p>损失函数</p><p>用未准确分类的个数来衡量</p></li><li><p>常用方法</p><p>感知机、SVM,Generate Model</p></li></ul><h3 id="Genetate-Model-贝叶斯公式"><a href="#Genetate-Model-贝叶斯公式" class="headerlink" title="Genetate Model(贝叶斯公式)"></a>Genetate Model(贝叶斯公式)</h3><p>先验概率一般容易得到，默认正态分布</p><p>用极大似然估计计算出概率密度函数，也就是假设样本是独立的，用求导或者公式找到可能性最大的均值和方差</p><ul><li>当数据不多的时候，可以考虑给不同特征的协方差矩阵一样的值，减少overfit的可能性。损失函数有三个变量，$μ1，μ2，σ$。$μ1和μ2$的求法上上面一样，也就是求平均数。$σ=\frac{n_1}{n_1+n_2} \times σ1 + \frac{n_2}{n_1+n_2} \times σ2 ​$</li></ul><p>​      当$σ_1和σ_2​$共用的时候，分割线是一条直线</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sigma1.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sigma2.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sigma3.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sigma4.png" alt></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sigma5.png" alt></p><h2 id="Lecture-5-Logistic-Regression"><a href="#Lecture-5-Logistic-Regression" class="headerlink" title="Lecture 5 Logistic Regression"></a>Lecture 5 Logistic Regression</h2><h3 id="logistic-regression和linear-regression差别"><a href="#logistic-regression和linear-regression差别" class="headerlink" title="logistic regression和linear regression差别"></a>logistic regression和linear regression差别</h3><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic.png" alt></p><h3 id="为什么logistic-regression的损失不用平方和"><a href="#为什么logistic-regression的损失不用平方和" class="headerlink" title="为什么logistic regression的损失不用平方和"></a>为什么logistic regression的损失不用平方和</h3><p>在距离目标远的时候，交叉熵梯度很大，更新比较快</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/square_error.png" alt></p><h3 id="Discrimination和Generative-直接求参数和预设模型通过概率极大似然化"><a href="#Discrimination和Generative-直接求参数和预设模型通过概率极大似然化" class="headerlink" title="Discrimination和Generative(直接求参数和预设模型通过概率极大似然化)"></a>Discrimination和Generative(直接求参数和预设模型通过概率极大似然化)</h3><p>一样的数据一样的模型，找出来的W和B不会是同一个。因为我们在Generative假设是正态分布或者伯努利分布什么的。</p><p>一把来说，discrimination的表现要比较好。假设可能会猜对一些联系，也自然会遗漏一些信息。</p><p>但是generative也有适用处，当训练数据较少时或者训练数据有较多的噪声。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Discriminative.png" alt></p><h3 id="Limitation-of-Logistic-Regression"><a href="#Limitation-of-Logistic-Regression" class="headerlink" title="Limitation of Logistic Regression"></a>Limitation of Logistic Regression</h3><p>无法处理线性不可分问题，  部分问题可以通过变换特征将问题转换为线性可分问题</p><p>更好的方法是将多个线性集合起来，也就是深度网络了。可以把底层的神经节点当做机器在做变换特征。（李宏毅老师的各个环节之间的过渡都很经典）</p><h2 id="Lecture-6-Brief-Introduction-of-Deep-Learning"><a href="#Lecture-6-Brief-Introduction-of-Deep-Learning" class="headerlink" title="Lecture 6 Brief Introduction of Deep Learning"></a>Lecture 6 Brief Introduction of Deep Learning</h2><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/dphistory.png" alt></p><h3 id="传统方法和深度学习"><a href="#传统方法和深度学习" class="headerlink" title="传统方法和深度学习"></a>传统方法和深度学习</h3><p>深度学习是把特征选择的问题转换为网络结构设计的问题，并没有绝对的好坏</p><p>对于图像和语音而言，设计网络结构设计是要比去图像里找关键特征简单</p><p>对于自然语音处理，深度学习的贡献没有像如图像和语音方面那样大，因为对人来说，人对语言非常熟悉，特征选择的问题也相对比较简单</p><h3 id="fat-neural-network-和deep-neural-network"><a href="#fat-neural-network-和deep-neural-network" class="headerlink" title="fat neural network 和deep neural network"></a>fat neural network 和deep neural network</h3><p>理论上一层的隐藏层就可以模拟任何一个函数，那么为什么不用一层的宽网络来算我们需要的函数，而是要通过deeplearning呢？</p><p>请听下回分解</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记_李宏毅（0-3）</title>
      <link href="/2019/03/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%880-3%EF%BC%89/"/>
      <url>/2019/03/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%880-3%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-0-Introduction-of-Machine-Learning"><a href="#Lecture-0-Introduction-of-Machine-Learning" class="headerlink" title="Lecture 0 Introduction of Machine Learning"></a>Lecture 0 Introduction of Machine Learning</h2><h3 id="机器学习三大任务"><a href="#机器学习三大任务" class="headerlink" title="机器学习三大任务"></a>机器学习三大任务</h3><ul><li><p>regression（回归）</p></li><li><p>classification（分类）</p></li><li><p>structured learning (结构化学习) </p><p>​    输入和输出不是向量 可能是结构化 树或者列表</p></li></ul><a id="more"></a><h3 id="Reinforcement-Learning-强化学习"><a href="#Reinforcement-Learning-强化学习" class="headerlink" title="Reinforcement Learning(强化学习)"></a>Reinforcement Learning(强化学习)</h3><pre><code> 不同于监督学习，会给输出评分，不会明确给机器正确的标签。从评价中学习，知道做的好坏，但是不知道哪里好，哪里坏。</code></pre><p>​    Alpha GO的训练是先利用监督学习，再通过强化学习</p><h2 id="Lecture-1-Regression"><a href="#Lecture-1-Regression" class="headerlink" title="Lecture 1 Regression"></a>Lecture 1 Regression</h2><h3 id="基本应用"><a href="#基本应用" class="headerlink" title="基本应用"></a>基本应用</h3><ul><li>股市预测</li><li>自动驾驶</li><li>推荐系统 </li></ul><h3 id="回归常用步骤"><a href="#回归常用步骤" class="headerlink" title="回归常用步骤"></a>回归常用步骤</h3><p>备注：表示数据时，下标表示该数据的一部分，上标表示完整的该个体，并给之编号</p><h4 id="先给定一个模型-比如线性模型-y-w-times-x-b"><a href="#先给定一个模型-比如线性模型-y-w-times-x-b" class="headerlink" title="先给定一个模型 比如线性模型$y=w \times x+b$"></a>先给定一个模型 比如线性模型$y=w \times x+b$</h4><h4 id="根据训练数据，求出模型的参数"><a href="#根据训练数据，求出模型的参数" class="headerlink" title="根据训练数据，求出模型的参数"></a>根据训练数据，求出模型的参数</h4><p>用损失函数和梯度下降方法求出损失较小的参数，其中损失函数加上正则化</p><h4 id="注意点：大类中有不同的小类，用不一样的模型或参数去分别拟合，再线性组合。-w-和-b-也可以用不一样学习率，adagrad"><a href="#注意点：大类中有不同的小类，用不一样的模型或参数去分别拟合，再线性组合。-w-和-b-也可以用不一样学习率，adagrad" class="headerlink" title="注意点：大类中有不同的小类，用不一样的模型或参数去分别拟合，再线性组合。$w$和$b$也可以用不一样学习率，adagrad"></a>注意点：大类中有不同的小类，用不一样的模型或参数去分别拟合，再线性组合。$w$和$b$也可以用不一样学习率，adagrad</h4><h2 id="Lecture-2-Where-does-the-error-come-from"><a href="#Lecture-2-Where-does-the-error-come-from" class="headerlink" title="Lecture 2 Where does the error come from"></a>Lecture 2 Where does the error come from</h2><p>错误来自两个方向，一个是bias,另一个是来自于variance</p><h3 id="bias："><a href="#bias：" class="headerlink" title="bias："></a>bias：</h3><p>目标函数和估计的函数之间的距离（类似于模型的选择）</p><p>简单的模型一般有比较大的bias 复杂的模型虽然看起来比较乱 平均之后反而有比较小的bias</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%9D%8E%E5%AE%8F%E6%AF%85_lec2_1.png" alt="bias"></p><h3 id="variance"><a href="#variance" class="headerlink" title="variance:"></a>variance:</h3><p>实际的值算出的函数和估计函数之间的距离（噪声数据的存在）</p><h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><ul><li><p>目标就是要找到一个权衡点，当bias大的时候，模型都没有办法拟合训练数据，叫作underfitting；当variance比较大的时候，就会出现过拟合。</p><p>当bias大的时候，需要重新设计模型，再加点特征或者增加变量的多项式的次数。</p><p>当variance大的时候，要么增加训练数据；或者加入正则化，但是这时候有可能会增加bias。</p></li><li><p>用验证集来选择模型，直接用测试集在选择模型的话，测试集和实际真正数据分布也不一样。测试集的功能只是大致检验模型，而不是用来选择模型。这样在实际中得到的错误率才有可能和测试集差不多。</p></li><li>在划分训练集和验证集的时候，可以采取N-fold Cross Validation,交叉验证</li></ul><h2 id="Lecture-3-Gradient-Descent"><a href="#Lecture-3-Gradient-Descent" class="headerlink" title="Lecture 3  Gradient Descent"></a>Lecture 3  Gradient Descent</h2><h3 id="TIP-1-学习率"><a href="#TIP-1-学习率" class="headerlink" title="TIP 1 学习率"></a>TIP 1 学习率</h3><p>由于学习率不好提前设定的原因，在训练用梯度下降的方法时，可以把损失函数的图像画出来，要是损失越来越大就很有可能是因为学习率太高了。</p><h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><ul><li><p>不同的参数有不一样的学习率</p></li><li><p>它的学习率是实时变化，并且是η（随着时间变化的参数）除以之前所有求得微分的平方和的平均数的根号。具体如下图</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/adagrad.png" alt="adagrad"></p></li></ul><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/adagrad1.png" alt="adagrad1"></p><ul><li>adagrad的原理，不仅考虑一次微分还考虑了二次微分，虽然没有直接算出二次微分，但是大概估计了二次微分的相对大小</li></ul><h3 id="TIP-2-Stochastic-Gradient-Descent"><a href="#TIP-2-Stochastic-Gradient-Descent" class="headerlink" title="TIP 2 Stochastic Gradient Descent"></a>TIP 2 Stochastic Gradient Descent</h3><p>不用所有的loss求导，用部分数据进行计算</p><h3 id="TIP-3-Feature-Scaling"><a href="#TIP-3-Feature-Scaling" class="headerlink" title="TIP 3 Feature Scaling"></a>TIP 3 Feature Scaling</h3><p>多个特征的话，建议把所有特征的取值范围变化到大致相同的范围，比如$x_1$从-1到1之间波动，$x_2$从-100到500之间变化，这样他们的学习率就肯定不能一样了。没有scaling的话，显然更新比较复杂。</p><p>做scaling的方法，有多种。要注意考虑方差和均值</p><h3 id="Gradient-Descent-Theory"><a href="#Gradient-Descent-Theory" class="headerlink" title="Gradient Descent Theory"></a>Gradient Descent Theory</h3><p>就损失函数而言，在当前点的位置，用泰勒展开。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/bp%E6%95%B0%E5%AD%A61.png" alt="bp数学1"></p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/bp%E6%95%B0%E5%AD%A62.png" alt="bp数学2"></p><p>前提是这个红色的圆足够小，泰勒展开才能近似。η是使$△θ_1和△θ_2$的值等于$u和V$,其中$U和V$是梯度大小。也就是要η足够小。也就是learnrate要足够小。</p><p>当然可以计算二次微分，更加拟合损失函数，比如牛顿法，只是运算成本太大</p><h3 id="Gradient-Descent的缺点"><a href="#Gradient-Descent的缺点" class="headerlink" title="Gradient Descent的缺点"></a>Gradient Descent的缺点</h3><ul><li><p>可能卡在局部最小值</p></li><li><p>也也可能卡在微分值为0的地方，saddle point</p></li><li><p>在梯度平缓的地方非常慢，plateau</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/bp%E7%BC%BA%E7%82%B9.png" alt="bp缺点"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019_2_24-3_09周报</title>
      <link href="/2019/03/09/2019_2_24-3_09%E5%91%A8%E6%8A%A5/"/>
      <url>/2019/03/09/2019_2_24-3_09%E5%91%A8%E6%8A%A5/</url>
      
        <content type="html"><![CDATA[<p>这两周先是落枕，落枕好了环岛骑车又摔了。。好像基本没做成什么事</p><h3 id="这两周完成的事"><a href="#这两周完成的事" class="headerlink" title="这两周完成的事"></a>这两周完成的事</h3><a id="more"></a><ul><li>跑了一次五千米</li><li>又环岛一次</li><li>看了BN的论文</li><li>复习了一些python基本知识，主要根据莫烦的教程</li><li>开始接触pytorch，把官网的基础教程看了一遍</li><li>重新学习一下机器学习，看了两节李宏毅的课，讲的挺好</li><li>看了几部电影，大腕，手机，神奇动物2，头号玩家</li><li>看了一些浮生六记，沈复真是个有趣的人</li></ul><h3 id="下周计划"><a href="#下周计划" class="headerlink" title="下周计划"></a>下周计划</h3><ul><li><p>看完莫烦的pytorch教程</p></li><li><p>把inception v3和v4 论文看了</p></li><li><p>跑一跑alexnet和vgg的代码</p></li><li><p>李宏毅的课看到第十九，也就是SVM前</p></li><li><p>要是腿好了天气允许就跑一跑步，不过我感觉得再两周才能彻底好</p></li><li><p>把浮生六记看完，这个不强求</p></li><li><p>那两门课的作业就不用多说了吧</p></li></ul><h3 id="若干感想"><a href="#若干感想" class="headerlink" title="若干感想"></a>若干感想</h3><p>马上就研二了，再不针对性学点东西马上就毕业了。最多再两周，也就是清明回来要开始接触三维分类了。现在学java也太蠢了，找工作的事先不急。深度学习的还是蛮有趣的，先一心专研自己的方向。等九十月份再开始考虑面试和具体技术的问题。</p><p>所以最近两三个月的大方向要达成的目标</p><ul><li><p>熟悉基本的机器学习方法，起码到能自己推导，看懂别人代码的地步</p></li><li><p>能运行并且修改深度学习模型</p></li><li><p>关于三维分类方向至少要看二三十篇论文</p></li><li><p>然后好好锻炼，五千米跑进21就参加明年校运会</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 周报 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>pytorch入门系列（1)_A 60 Minute BLITZ</title>
      <link href="/2019/03/08/pytorch%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%EF%BC%881)_A%2060%20Minute%20BLITZ/"/>
      <url>/2019/03/08/pytorch%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%EF%BC%881)_A%2060%20Minute%20BLITZ/</url>
      
        <content type="html"><![CDATA[<h2 id="A-60-MINUTE-BLITZ"><a href="#A-60-MINUTE-BLITZ" class="headerlink" title="A 60 MINUTE BLITZ"></a>A 60 MINUTE BLITZ</h2><h3 id="What-is-pytorch"><a href="#What-is-pytorch" class="headerlink" title="What is pytorch"></a>What is pytorch</h3><p>基于python的计算包，两个主要目的</p><p>​    一、类似于numpy，但是能调用gpu进行运算</p><p>​    二、为深度学习研究提供便利性</p><a id="more"></a><ul><li><p>tensor</p><p>tensor,张量，可以理解为多维数组。</p></li></ul>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创造一个不初始化的tensor</span></span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创造一个随机初始化的Tensor</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创造一个初始化为0 类型为long的tensor</span></span><br><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"><span class="comment">#可以直接由数据初始化tensor</span></span><br><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#用一个tensor创造另一个tensor 若新的张量没有指定一些属性 将继承旧的tensor</span></span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)   </span><br><span class="line">x = torch.randn_like(x, dtype=torch.float)  </span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor.size()</span></span><br></pre></td></tr></table></figure><ul><li><p>关于张量a+b 以下几个效果都一样 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a+b</span><br><span class="line"></span><br><span class="line">torch.add(a,b)</span><br><span class="line"></span><br><span class="line">result=torch.randn_like(a)</span><br><span class="line">torch(a,b,out=result)</span><br><span class="line"></span><br><span class="line">a.add_(b)</span><br></pre></td></tr></table></figure></li><li><p><strong>pytorch中带_的函数一般都是要改变原来值的功能</strong></p></li><li><p>使用view可以重新定义tensor的大小，当你不知道某一维的大小，可以用-1替代，pytorch会帮你算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>)  <span class="comment"># 系统将用16除以8 得知有2行</span></span><br><span class="line">print(x.size(), y.size(), z.size()) </span><br><span class="line"><span class="comment">#将输出  torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</span></span><br></pre></td></tr></table></figure></li><li><p>Torch tensor和Numpy array的转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tensor转换为 array  并且转换完改变一个的值 另一个也会变</span></span><br><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">b = a.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment">#array转换为tensor 同样他们的值相互影响</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br></pre></td></tr></table></figure><p>不过chartensor不能转变为numpy</p></li><li><p>将tensor的运算指定到gpu</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用.to方法可以把张量移动到任何设备 本例最后把z移动到cpu上 所以结果不一样</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)          <span class="comment"># a CUDA device object</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># directly create a tensor on GPU</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># or just use strings ``.to("cuda")``</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))  </span><br><span class="line">    </span><br><span class="line">  <span class="comment">#运行结果：</span></span><br><span class="line">tensor([<span class="number">1.9806</span>], device=<span class="string">'cuda:0'</span>)</span><br><span class="line">tensor([<span class="number">1.9806</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure></li></ul><h3 id="AUTOGRAD-自动求导机制"><a href="#AUTOGRAD-自动求导机制" class="headerlink" title="AUTOGRAD  自动求导机制"></a>AUTOGRAD  自动求导机制</h3><ul><li><p>pytorch是Define-by-run，数据引导结构生成;define-and-run 先有结构，再运行，比如tensorflow</p></li><li><p>在autograd中tensor的一些属性</p><p><strong>.requires_grad</strong> 如果当前tensor需要通过求导运算，应该设置为true 并且我们只能设置graph leaves(创建变量)，不能设置leaf variables（结果变量） 默认是False</p><p><strong>.backward()</strong>,当使用这个方法时，它会计算当前张量的梯度值，并且把结果赋值给<strong>.grad</strong>。当结果是标量的时候不需要指定结果的形状，如果多于一个元素，就需要明确他的参数</p><p><strong>.detach()</strong>,该方法停止当前张量进入计算</p><p><strong>.with torch.no_grad():</strong>让作用域内的tensor不参与计算</p><p><strong>grad_fn</strong> 储存产生这个张量的函数的信息，如果这个张量是用户创建的，它的grad_fn值为none</p></li></ul><ul><li>计算的原理？</li></ul><h3 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h3><ul><li><p>构造网络时用的包都是torch.nn里的</p></li><li><p>nn.Conv2d（）卷积    nn.fuctional.max_pool2d()池化 </p></li><li><p>nn.Linear() 线性化 例子如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Linear(<span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">128</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(output.size())</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">30</span>])</span><br></pre></td></tr></table></figure></li></ul><ul><li>一个网络的可学习参数可通过.parameters()返回</li></ul><h3 id="Training-A-Classifier"><a href="#Training-A-Classifier" class="headerlink" title="Training A Classifier"></a>Training A Classifier</h3><ul><li><p>torchvision是用以下载图像数据库的包</p></li><li><p>加载并转换数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                        download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line"><span class="comment">#获取图片和标签</span></span><br><span class="line">images, labels = dataiter.next()</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>定义优化器，启动优化器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure></li><li><p>测试的时候 不再训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images: %d %%'</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure></li><li><p>在GPU上训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义设备</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment">#把网络送到GPU</span></span><br><span class="line">net.to(device)</span><br><span class="line"><span class="comment">#把输入和标签送到GPU</span></span><br><span class="line">inputs, labels = inputs.to(device), labels.to(device)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python入门系列（1）_基于莫烦教程</title>
      <link href="/2019/03/08/python%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89_%E8%8E%AB%E7%83%A6/"/>
      <url>/2019/03/08/python%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89_%E8%8E%AB%E7%83%A6/</url>
      
        <content type="html"><![CDATA[<h3 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h3><ul><li><p>python的没有多次方符号^,二次方$<strong>$，三次方$</strong>3$，以此类推</p></li><li><p>如果0作为while的条件，将被视为false；其他数字都是true</p></li><li><p>如果集合，如list,tuple,dict等作为while的条件，如果集合中的元素数量为0，那么将被视为false;否则被视为true;</p></li><li><p>range()可以产生一个序列</p><p>​    如果是range(start,stop),则左边是闭区间，左边是开区间，如range(1,10)会产生1到9</p><p>​    如果省略了start,则将从0开始</p><p>​    如果是range(start,stop,step),表示间隔取数，直到大于或者等于stop</p><a id="more"></a></li><li><p>python内置了list,tuple,dict,set四种基本集合，dic有key,set集合会去除重复项。都可以迭代</p></li><li><p>实际上我们也可以设计有_iter_()和_next_()函数就可以生成迭代对象，如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define a Fib class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fib</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, max)</span>:</span></span><br><span class="line">        self.max = max</span><br><span class="line">        self.n, self.a, self.b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.n &lt; self.max:</span><br><span class="line">            r = self.b</span><br><span class="line">            self.a, self.b = self.b, self.a + self.b</span><br><span class="line">            self.n = self.n + <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> r</span><br><span class="line">        <span class="keyword">raise</span> StopIteration()</span><br><span class="line"></span><br><span class="line"><span class="comment"># using Fib object</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> Fib(<span class="number">5</span>):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>也可以用yield实现类似功能，yield执行的时候将立即返回结果给上层调用者，而当前的状态仍然保留。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(max)</span>:</span></span><br><span class="line">    a, b = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> max:</span><br><span class="line">        r = b</span><br><span class="line">        a, b = b, a+b</span><br><span class="line">        max -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line"><span class="comment"># using generator</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> fib(<span class="number">5</span>):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure><p>将同样生成1，1，2，3，5</p></li><li><p>python中有if-else行内表达式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var = var1 <span class="keyword">if</span> condition <span class="keyword">else</span> var2</span><br></pre></td></tr></table></figure><p>如果condition为真，把var1赋值给var,否则把var2赋值给var</p></li><li><p>python的else if是连在一起写的，记作elif</p></li><li><p>函数定义的时候可以设置默认参数，即参数有默认值。但是需要注意的是所有的默认参数不能出现在非默认参数的前面</p></li><li><p>python的自调用</p><p>当想对当前的子模块进行测试时，可以加上以下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">#code_here</span></span><br></pre></td></tr></table></figure></li><li><p>可变参数 接收的是tuple</p><p>当传入参数的数目不确定时候，可以用可变参数，但是注意可变参数不能出现在特点参数和默认参数前面</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">report</span><span class="params">(name, *grades)</span>:</span></span><br><span class="line">    total_grade = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> grade <span class="keyword">in</span> grades:</span><br><span class="line">        total_grade += grade</span><br><span class="line">    print(name, <span class="string">'total grade is '</span>, total_grade)</span><br></pre></td></tr></table></figure></li><li><p>关键字参数</p><p>关键字参数可以传入0个或者任意个含参数名的参数，这些参数在函数内部可以自动封装成一个字典dict.西藏形状为**参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">portrait</span><span class="params">(name, **kw)</span>:</span></span><br><span class="line">    print(<span class="string">'name is'</span>, name)</span><br><span class="line">    <span class="keyword">for</span> k,v <span class="keyword">in</span> kw.items():</span><br><span class="line">        print(k, v)</span><br></pre></td></tr></table></figure></li></ul><p>执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">portrait(<span class="string">'Mike'</span>, age=<span class="number">24</span>, country=<span class="string">'China'</span>, education=<span class="string">'bachelor'</span>)</span><br></pre></td></tr></table></figure><p>可以输出    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">name <span class="keyword">is</span> Mike</span><br><span class="line">age <span class="number">24</span></span><br><span class="line">country China</span><br><span class="line">education bachelor</span><br></pre></td></tr></table></figure><ul><li><p>定义变量的位置决定了是全局变量还是局部变量。可以在通过加global,在函数中修改全局参数</p></li><li><p>open（）能够读取或者写入一文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">my_file=open(<span class="string">'my file.txt'</span>,<span class="string">'w'</span>)   <span class="comment">#用法: open('文件名','形式'), 其中形式有'w':write;'r':read;'a':添加</span></span><br><span class="line">my_file.write(text)               <span class="comment">#该语句会写入先前定义好的 text</span></span><br><span class="line">my_file.close()                   <span class="comment">#关闭文件</span></span><br><span class="line">append_text=<span class="string">'\nThis is appended file.'</span>  <span class="comment"># 为这行文字提前空行 "\n"</span></span><br><span class="line">my_file=open(<span class="string">'my file.txt'</span>,<span class="string">'a'</span>)   <span class="comment"># 'a'=append 以增加内容的形式打开</span></span><br><span class="line">my_file.write(append_text)</span><br><span class="line">my_file.close()</span><br></pre></td></tr></table></figure></li><li><p>文件的读取有几种方式，如read(),readline()或者radlines()</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">file= open(<span class="string">'my file.txt'</span>,<span class="string">'r'</span>) </span><br><span class="line">content=file.read()  </span><br><span class="line">print(content)</span><br><span class="line"></span><br><span class="line"><span class="string">""""</span></span><br><span class="line"><span class="string">This is my first test.</span></span><br><span class="line"><span class="string">This is the second line.</span></span><br><span class="line"><span class="string">This the third line.</span></span><br><span class="line"><span class="string">This is appended file.    </span></span><br><span class="line"><span class="string">"""</span><span class="string">"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">file= open('my file.txt','r')  </span></span><br><span class="line"><span class="string">content=file.readline()  # 读取第一行</span></span><br><span class="line"><span class="string">print(content)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"</span><span class="string">"""</span></span><br><span class="line"><span class="string">This is my first test.</span></span><br><span class="line"><span class="string">"""</span><span class="string">"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">second_read_time=file.readline()  # 读取第二行</span></span><br><span class="line"><span class="string">print(second_read_time)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"</span><span class="string">""</span></span><br><span class="line">This <span class="keyword">is</span> the second line.</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">file= open('my file.txt','r') </span></span><br><span class="line"><span class="string">content=file.readlines() # python_list 形式</span></span><br><span class="line"><span class="string">print(content)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span><span class="string">"</span></span><br><span class="line"><span class="string">['This is my first test.\n', 'This is the second line.\n', 'This the third line.\n', 'This is appended file.']</span></span><br><span class="line"><span class="string">"</span><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 之后如果使用 for 来迭代输出:</span></span><br><span class="line"><span class="string">for item in content:</span></span><br><span class="line"><span class="string">    print(item)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">This <span class="keyword">is</span> my first test.</span><br><span class="line"></span><br><span class="line">This <span class="keyword">is</span> the second line.</span><br><span class="line"></span><br><span class="line">This the third line.</span><br><span class="line"></span><br><span class="line">This <span class="keyword">is</span> appended file.</span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><ul><li>用class定义一个类时，首字母要大写。想要类的内部属性不被外部访问，在属性名称前加上两个下划线，但是需要注意的是在变量前以双划线开头，并且以双划线结尾的是特殊变量，不是private变量。如__init__()。</li><li><p>大多数在类里定义的self代表的是类的实例。在定义函数的时候在参数中一般不能省略</p></li><li><h3 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h3><p>tuple,用小括号或者无括号来表示，是一串有顺序的数字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a_tuple = (<span class="number">12</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">15</span> , <span class="number">6</span>)</span><br><span class="line">another_tuple = <span class="number">12</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">15</span> , <span class="number">6</span></span><br></pre></td></tr></table></figure><p>tuple指向的位置不能变，当它指向基本类型时它的值自然不能变化，但是当它指向一个list的话，list里的值自然是能变的</p></li><li><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><p>list;用中括号来命名；不需要元素为同一类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a_list=[<span class="number">12</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">15</span>,<span class="number">6</span>]</span><br></pre></td></tr></table></figure><p>列表多了一些方法，如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a_list.append（a）<span class="comment">#在列表最后加上'a'</span></span><br><span class="line">a_list.insert(<span class="number">1</span>,<span class="number">2</span>)<span class="comment">#在列表位置1，插入常量2</span></span><br><span class="line">a_list.pop()<span class="comment">#删除列表最后一个元素 也可加入参数 删除指定位置的元素 这时候和remove（）一样了</span></span><br><span class="line">a_list.sort()<span class="comment">#对列表进行排序</span></span><br></pre></td></tr></table></figure><p>list可以拓展为多维列表</p></li><li><p>字典</p><p>dic;字典是无序的，有key和value两种元素，也不要求所有的key或者value有相同的形式。从一定角度来说，list是key为有序数列的dic。</p></li><li><p>错误处理</p><p>举例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file=open(<span class="string">'eeee.txt'</span>,<span class="string">'r'</span>)  <span class="comment">#会报错的代码</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:  <span class="comment"># 将报错存储在 e 中</span></span><br><span class="line">    print(e)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[Errno 2] No such file or directory: 'eeee.txt'</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></li><li><p>zip</p><p>接受任意个序列作为参数，合并之后返回一个tuple列表。例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b=[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">ab=zip(a,b)</span><br><span class="line">print(list(ab))  <span class="comment">#需要加list来可视化这个功能</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[(1, 4), (2, 5), (3, 6)]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></li><li><p>lambda</p><p>匿名函数，简化函数。如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fun= <span class="keyword">lambda</span> x,y:x+y</span><br><span class="line">x=int(input(<span class="string">'x='</span>))    <span class="comment">#这里要定义int整数，否则会默认为字符串</span></span><br><span class="line">y=int(input(<span class="string">'y='</span>))</span><br><span class="line">print(fun(x,y))</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">x=6</span></span><br><span class="line"><span class="string">y=6</span></span><br><span class="line"><span class="string">12</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></li><li><p>map</p><p>map是把函数和参数绑定在一起，第一个参数是 function ，以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(x,y)</span>:</span></span><br><span class="line"><span class="keyword">return</span> (x+y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(map(fun,[<span class="number">1</span>],[<span class="number">2</span>]))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[3]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(map(fun,[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[4,6]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></li><li><p>深拷贝和浅拷贝 import copy</p><p>deepcopy：对外围和内部元素都进行了拷贝对象本身，而不是对象的引用。也就是拷贝的和被拷贝的地址不一样了。</p><p>copy:拷贝了最外围的对象本身，内部的元素都只是拷贝了一个引用而已。就是对于复杂的元素只是拷贝他们的地址，变化会引起本来对象相应值的变化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=[<span class="number">1</span>,<span class="number">2</span>,[<span class="number">3</span>,<span class="number">4</span>]]  <span class="comment">#第三个值为列表[3,4],即内部元素</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d=copy.copy(a) <span class="comment">#浅拷贝a中的[3，4]内部元素的引用，非内部元素对象的本身</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>id(a)==id(d)</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>id(a[<span class="number">2</span>])==id(d[<span class="number">2</span>])</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">2</span>][<span class="number">0</span>]=<span class="number">3333</span>  <span class="comment">#改变a中内部原属列表中的第一个值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d             <span class="comment">#这时d中的列表元素也会被改变</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, [<span class="number">3333</span>, <span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#copy.deepcopy()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>e=copy.deepcopy(a) <span class="comment">#e为深拷贝了a</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">2</span>][<span class="number">0</span>]=<span class="number">333</span> <span class="comment">#改变a中内部元素列表第一个的值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>e</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, [<span class="number">3333</span>, <span class="number">4</span>]] <span class="comment">#因为时深拷贝，这时e中内部元素[]列表的值不会因为a中的值改变而改变</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>pickle</p><p>用来保存和提取文件的模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">a_dict = &#123;<span class="string">'da'</span>: <span class="number">111</span>, <span class="number">2</span>: [<span class="number">23</span>,<span class="number">1</span>,<span class="number">4</span>], <span class="string">'23'</span>: &#123;<span class="number">1</span>:<span class="number">2</span>,<span class="string">'d'</span>:<span class="string">'sad'</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># pickle a variable to a file</span></span><br><span class="line">file = open(<span class="string">'pickle_example.pickle'</span>, <span class="string">'wb'</span>)</span><br><span class="line">pickle.dump(a_dict, file)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># reload a file to a variable</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'pickle_example.pickle'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    a_dict1 =pickle.load(file)</span><br><span class="line"></span><br><span class="line">print(a_dict1)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>set</p><p>set最基本的功能就是来找list中或者句子中不同的元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">char_list = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'d'</span>, <span class="string">'d'</span>]</span><br><span class="line"></span><br><span class="line">print(set(char_list))</span><br><span class="line"><span class="comment"># &#123;'b', 'd', 'a', 'c'&#125;</span></span><br></pre></td></tr></table></figure><p>我们还能进行一些筛选操作, 比如对比另一个东西, 看看原来的 set 里有没有和他不同的 (<code>difference</code>). 或者对比另一个东西, 看看 set 里有没有相同的 (<code>intersection</code>).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">unique_char = set(char_list)</span><br><span class="line">print(unique_char.difference(&#123;<span class="string">'a'</span>, <span class="string">'e'</span>, <span class="string">'i'</span>&#125;))</span><br><span class="line"><span class="comment"># &#123;'b', 'd', 'c'&#125;</span></span><br><span class="line"></span><br><span class="line">print(unique_char.intersection(&#123;<span class="string">'a'</span>, <span class="string">'e'</span>, <span class="string">'i'</span>&#125;))</span><br><span class="line"><span class="comment"># &#123;'a'&#125;</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><p>  python中用来匹配字符的工具，广泛用于网页爬虫、文稿整理和数据筛选。包含于python的内置模块re中。</p><ul><li><p>简单匹配</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># regular expression</span></span><br><span class="line">pattern1 = <span class="string">"cat"</span></span><br><span class="line">pattern2 = <span class="string">"bird"</span></span><br><span class="line">string = <span class="string">"dog runs to cat"</span></span><br><span class="line">print(re.search(pattern1, string))  <span class="comment"># &lt;_sre.SRE_Match object; span=(12, 15), match='cat'&gt;</span></span><br><span class="line">print(re.search(pattern2, string))  <span class="comment"># None</span></span><br></pre></td></tr></table></figure></li><li><p>可以用[]来模糊匹配，建立规则需要在规则前加上一个r</p><p><code>[A-Z]</code> 表示的就是所有大写的英文字母. <code>[0-9a-z]</code> 表示可以是数字也可以是任何小写字母</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ptn = <span class="string">r"r[au]n"</span>       <span class="comment"># start with "r" means raw string</span></span><br><span class="line">print(re.search(ptn, <span class="string">"dog runs to cat"</span>))    <span class="comment"># &lt;_sre.SRE_Match object; span=(4, 7), match='run'&gt;</span></span><br><span class="line"></span><br><span class="line">print(re.search(<span class="string">r"r[A-Z]n"</span>, <span class="string">"dog runs to cat"</span>))     <span class="comment"># None</span></span><br><span class="line">print(re.search(<span class="string">r"r[a-z]n"</span>, <span class="string">"dog runs to cat"</span>))     <span class="comment"># &lt;_sre.SRE_Match object; span=(4, 7), match='run'&gt;</span></span><br><span class="line">print(re.search(<span class="string">r"r[0-9]n"</span>, <span class="string">"dog r2ns to cat"</span>))     <span class="comment"># &lt;_sre.SRE_Match object; span=(4, 7), match='r2n'&gt;</span></span><br><span class="line">print(re.search(<span class="string">r"r[0-9a-z]n"</span>, <span class="string">"dog runs to cat"</span>))  <span class="comment"># &lt;_sre.SRE_Match object; span=(4, 7), match='run'&gt;</span></span><br></pre></td></tr></table></figure><p>还有很多规则 有空回来补</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> python学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inception-V2(BN论文解读）</title>
      <link href="/2019/03/07/Inception-V2(BN%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%EF%BC%89/"/>
      <url>/2019/03/07/Inception-V2(BN%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>Batch Normalization是加速深度学习的重要方法，被广泛应用在各种网络中。简单来说，训练数据分布大致相同对训练来说很有帮助，而且BN就是在帮助数据归一化，提高泛化能力。</p><h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><ul><li><p>internal convariate shift</p><p>当底层的参数变化时，相应输出变化，并将作用在上层的参数并被放大。论文中表述，在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。</p></li><li><p>whitening</p><p>以前用以归一化的方法，让输入数据具有相同的方差，均值为0，同时去掉特征之间的相关性（具体怎么做，论文并没有提到）</p><p>但是白化有两个很大缺点，一是数据量太大，需要对每一轮训练的每一层都进行白化；二是改变了网络的每一次的分布</p><a id="more"></a></li></ul><h3 id="提出背景"><a href="#提出背景" class="headerlink" title="提出背景"></a>提出背景</h3><ul><li>训练模型需要仔细调整各个超参数，特别是初始学习率</li><li>训练的时候导致的梯度消失</li></ul><h3 id="算法做法"><a href="#算法做法" class="headerlink" title="算法做法"></a>算法做法</h3><ul><li><p>由于训练一般用mini-batch上训练，所以BN也自然是针对mini-batch</p></li><li><p>对于一次训练的一个batch的一层中的一个神经元节点（也就是当前层的一个输入端，或者是一个维度）</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/BN%E8%AE%A1%E7%AE%97%EF%BC%881%EF%BC%89.png" alt="bn计算图"></p><p>其中m是这个batch的大小  ε是避免方差为0 从而分母为0</p></li><li><p>归一化之后可能会破坏本来数据的表达能力，又引进了两个可学习的参数γ和β，表达式如下</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/BN%E8%AE%A1%E7%AE%97%EF%BC%882%EF%BC%89.png" alt="表达式"></p><p>当<img src="https://www.zhihu.com/equation?tex=%5Cgamma%5E2%3D%5Csigma%5E2%2C%5Cbeta%3D%5Cmu" alt="\gamma^2=\sigma^2,\beta=\mu"></p><p>显然Z又等于之前的Z了，如果效果不好就恢复成原始网络所要学习的特征分布。</p></li><li><p>测试时用到的μ和σ是用全部训练集的数据得来的，在训练batch时，把他们的相关参数存下来，然后测试的时候拿出来用（叫做无偏估计）</p><p><img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2_%7Btest%7D%3D%5Cfrac%7Bm%7D%7Bm-1%7D%5Cmathbb%7BE%7D%28%5Csigma%5E2_%7Bbatch%7D%29" alt></p></li></ul><p><img src="https://www.zhihu.com/equation?tex=%5Cmu_%7Btest%7D%3D%5Cmathbb%7BE%7D+%28%5Cmu_%7Bbatch%7D%29" alt="\mu_{test}=\mathbb{E} (\mu_{batch})"></p><ul><li>在实际训练中BN需要的参数还是很多，所以使用了权重共享，把一张特征图当做一个神经元进行处理</li><li>论文还解释了为什么是BN Wu+b,而不是直接BN u,也就是上一层的输出，简单来说，这时候的数据比较稳定，非稀疏。</li></ul><h3 id="BN的好处"><a href="#BN的好处" class="headerlink" title="BN的好处"></a>BN的好处</h3><ul><li><p>可以选择较大的初始学习率，从而加快学习</p></li><li><p>拥有一定正则化的功能，减少了使用dropout的需求</p></li><li><p>允许使用饱和性激活函数（sigmoif,tanh等），不一定是之前统一的RELU</p></li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3>]]></content>
      
      
      <categories>
          
          <category> 深度学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux服务器安装anaconda，cuda和pytorch</title>
      <link href="/2019/03/01/Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85anaconda,cuda,pytorch/"/>
      <url>/2019/03/01/Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85anaconda,cuda,pytorch/</url>
      
        <content type="html"><![CDATA[<p>之前用的是学姐的anaconda，安装一些包时候，很多文件权限老是访问不了。所以需要在自己的文件目录下安装anaconda。</p><h3 id="安装anaconda"><a href="#安装anaconda" class="headerlink" title="安装anaconda"></a>安装anaconda</h3><a id="more"></a><p>先去官网下载相应版本，放在指定位置后，cd进相应位置，执行以下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash 安装包名.sh</span><br></pre></td></tr></table></figure><p>然后看相应许可证，问你是否同意，之后默认路径如果不想改变，就yes，进入安装流程</p><p>之后询问是否加入系统路径 是否安装vscode</p><p>没有例外的可以都选yes</p><h3 id="安装cuda和cudnn"><a href="#安装cuda和cudnn" class="headerlink" title="安装cuda和cudnn"></a>安装cuda和cudnn</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/local/cuda/version.txt</span><br><span class="line">cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure><p>先查看是否安装了cuda和cudnn，实验室服务器装了我就不用装了</p><h3 id="安装pytorch"><a href="#安装pytorch" class="headerlink" title="安装pytorch"></a>安装pytorch</h3><p>在官网选择指定的cuda 和python的版本之后，会提示命令行是什么 运行命令行 就可以 就是我的</p><p>由于正常操作的话太慢了，需要给conda更换镜像源，首先运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config</span><br></pre></td></tr></table></figure><p>会产生.condarc文件，然后编辑该文件，增加几个镜像源，最后结果如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br></pre></td></tr></table></figure><p>不过这几个源好像没有我需要的pytorch版本，pytorch这个包还是下载超级慢</p><p>改用pip的话 会提示错误 类似于</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch-0.1.6.post22-cp27-cp27mu-linux_x86_64.whl is not a supported wheel on this platform</span><br></pre></td></tr></table></figure><p>是因为服务器上有多个python版本 需要指定版本 改为  就是在命令前指定用python几</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ python2 -m pip install torch-0.2.0.post3-cp27-cp27mu-manylinux1_x86_64.whl  </span><br><span class="line">$ python2 -m pip install torchvision-0.1.8-py2.py3-none-any.whl</span><br></pre></td></tr></table></figure><p>但是pip好像不如conda管理包来的方便 所有还是建议使用conda</p><p>使用conda的话 把网站给的参考中的把-c去掉 不使用指定的来源 这样就能使用我们之前导入的镜像地址，也就是把</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit=8.0 -c pytorch</span><br></pre></td></tr></table></figure><p>改成以下即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit=8.0</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># CUDA TEST</span><br><span class="line">import torch</span><br><span class="line">x = torch.Tensor([1.0])</span><br><span class="line">xx = x.cuda()</span><br><span class="line">print(xx)</span><br><span class="line"> </span><br><span class="line"># CUDNN TEST</span><br><span class="line">from torch.backends import cudnn</span><br><span class="line">print(cudnn.is_acceptable(xx))</span><br></pre></td></tr></table></figure><hr><h3 id="额外的尝试"><a href="#额外的尝试" class="headerlink" title="额外的尝试"></a>额外的尝试</h3><p>自己去清华镜像里找相应版本下载</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br></pre></td></tr></table></figure><p>然后把下载的文件放到anaconda/pkgs  在cd到相应位置执行以下命令 包的名字注意相应变化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install --use-local pytorch-0.4.0-py35_cuda8.0.61_cudnn7.1.2_1.tar.bz2</span><br></pre></td></tr></table></figure><p>解压完之后再执行 官网给的那句命令行 因为包已经下好了 就会进入安装步骤 不用龟速下载 这里下载的包必须的版本一样 不然它会让你再下一个最新的</p>]]></content>
      
      
      <categories>
          
          <category> linux学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>了解Anaconda和Conda</title>
      <link href="/2019/02/25/%E4%BA%86%E8%A7%A3Anaconda%E5%92%8CConda/"/>
      <url>/2019/02/25/%E4%BA%86%E8%A7%A3Anaconda%E5%92%8CConda/</url>
      
        <content type="html"><![CDATA[<p><strong>特别注意：在实验室服务器中装了anaconda2和anaconda3。可以在当前目录的根目录下.bashrc中改变。</strong></p><h3 id="Anaconda和conda的介绍"><a href="#Anaconda和conda的介绍" class="headerlink" title="Anaconda和conda的介绍"></a>Anaconda和conda的介绍</h3><p>Anaconda相当于一个包管理器，可以对环境进行统一管理，包含了包括python和conda在内的超过180个科学包及其依赖包。专业地说，叫做Anaconda是一个包含180+的科学包及其依赖项的发行版本。</p><p>conda是包及其依赖项和环境的管理工具，包含在Anaconda中。可以理解为一个工具，而anaconda是一个打包好的集合。</p><p>另外pip也是用于安装和管理软件包的包管理器。</p><p>简单来说，conda在诸多方面都比pip强</p><a id="more"></a><h3 id="使用conda管理环境"><a href="#使用conda管理环境" class="headerlink" title="使用conda管理环境"></a>使用conda管理环境</h3><h4 id="创建新环境"><a href="#创建新环境" class="headerlink" title="创建新环境"></a>创建新环境</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create --name &lt;env_name&gt; &lt;package_names&gt;</span><br><span class="line">例如： conda create --name python2 python=2.7</span><br><span class="line">多个包的话：conda create -n python3 python=3.5 numpy pandas</span><br></pre></td></tr></table></figure><h4 id="切换环境"><a href="#切换环境" class="headerlink" title="切换环境"></a>切换环境</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source activate &lt;env_name&gt;</span><br></pre></td></tr></table></figure><h4 id="退出环境到root"><a href="#退出环境到root" class="headerlink" title="退出环境到root"></a>退出环境到root</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">source deactivate</span><br><span class="line">或者 source activate base</span><br></pre></td></tr></table></figure><h4 id="显示已经创建的环境"><a href="#显示已经创建的环境" class="headerlink" title="显示已经创建的环境"></a>显示已经创建的环境</h4><p>结果中的*号，即当前所在环境</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda info --envs</span><br><span class="line">或：conda env list</span><br></pre></td></tr></table></figure><h4 id="复制环境"><a href="#复制环境" class="headerlink" title="复制环境"></a>复制环境</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create --name &lt;new_env_name&gt; --clone &lt;copied_env_name&gt;</span><br><span class="line">例如：conda create --name py2 --clone python2</span><br></pre></td></tr></table></figure><h4 id="删除环境"><a href="#删除环境" class="headerlink" title="删除环境"></a>删除环境</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove --name &lt;env_name&gt; --all</span><br></pre></td></tr></table></figure><h3 id="在环境中管理包"><a href="#在环境中管理包" class="headerlink" title="在环境中管理包"></a>在环境中管理包</h3><h4 id="精确查找包"><a href="#精确查找包" class="headerlink" title="精确查找包"></a>精确查找包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda search --full-name &lt;package_full_name&gt;</span><br></pre></td></tr></table></figure><h4 id="模糊查找"><a href="#模糊查找" class="headerlink" title="模糊查找"></a>模糊查找</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda search &lt;text&gt;</span><br></pre></td></tr></table></figure><h4 id="获取当前环境中的包"><a href="#获取当前环境中的包" class="headerlink" title="获取当前环境中的包"></a>获取当前环境中的包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure><h4 id="安装包"><a href="#安装包" class="headerlink" title="安装包"></a>安装包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">具体：conda install --name &lt;env_name&gt; &lt;package_name&gt;</span><br><span class="line">在当前环境安装：conda install &lt;package_name&gt;</span><br></pre></td></tr></table></figure><p>需要注意的是，有些包conda不能安装，只能通过pip,pip不能管理环境，所以需要切换到当前环境再安装</p><p>除了pip之外，还可以从Anaconda.org中下载</p><h4 id="卸载包"><a href="#卸载包" class="headerlink" title="卸载包"></a>卸载包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">严谨的：conda remove --name &lt;env_name&gt; &lt;package_name&gt;</span><br><span class="line">卸载当前环境的包：conda remove &lt;package_name&gt;</span><br></pre></td></tr></table></figure><h4 id="更新包"><a href="#更新包" class="headerlink" title="更新包"></a>更新包</h4><h5 id="更新所有包"><a href="#更新所有包" class="headerlink" title="更新所有包"></a>更新所有包</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda update --all</span><br><span class="line">或conda upgrade --all</span><br></pre></td></tr></table></figure><h5 id="更新指定包"><a href="#更新指定包" class="headerlink" title="更新指定包"></a>更新指定包</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda update &lt;package_name&gt;</span><br><span class="line">或</span><br><span class="line">conda upgrade &lt;package_name&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>莫烦_关于linux服务器的使用</title>
      <link href="/2019/02/25/%E8%8E%AB%E7%83%A6_%E5%85%B3%E4%BA%8Elinux%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2019/02/25/%E8%8E%AB%E7%83%A6_%E5%85%B3%E4%BA%8Elinux%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h3 id="如何从其他系统登录远程的Linux"><a href="#如何从其他系统登录远程的Linux" class="headerlink" title="如何从其他系统登录远程的Linux"></a>如何从其他系统登录远程的Linux</h3><a id="more"></a><h4 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h4><p>首先在linux上安装ssh的服务器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure><p>在连接中需要知道服务器的ip,可以在服务器通过ifconfig,看eno中的inet显示的网络是多少</p><h4 id="Putty（Windows）（和xshell差不多）"><a href="#Putty（Windows）（和xshell差不多）" class="headerlink" title="Putty（Windows）（和xshell差不多）"></a>Putty（Windows）（和xshell差不多）</h4><p>在windows对ssh的支持不太好，可以用putty软件来实现这个媒介功能</p><h4 id="安卓也有很多ssh的软件"><a href="#安卓也有很多ssh的软件" class="headerlink" title="安卓也有很多ssh的软件"></a>安卓也有很多ssh的软件</h4><p>使用与windows的差不多</p><h4 id="通过VNC远程操作"><a href="#通过VNC远程操作" class="headerlink" title="通过VNC远程操作"></a>通过VNC远程操作</h4><p>vnc只能连接局域网里的，但是也不会受网速影响。</p><p>teamviewer可以连外网但是受网速影响</p><p>在Linux上安装vnc服务器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install x11vnc</span><br></pre></td></tr></table></figure><p>设置密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x11vnc  -storepasswd</span><br></pre></td></tr></table></figure><p>启用密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x11vnc -usepw</span><br></pre></td></tr></table></figure><h5 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h5><p>在window上安装vnc的客户端</p><p>有tightvnc或者realvnc</p><h5 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h5><p>直接在软件中搜索VNC,会有个remmina。</p><h3 id="如何在远程计算机进行运算"><a href="#如何在远程计算机进行运算" class="headerlink" title="如何在远程计算机进行运算"></a>如何在远程计算机进行运算</h3><p>例如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh 用户名@云端ip python3 &lt; 文件在当前电脑的位置</span><br></pre></td></tr></table></figure><p>如果有多个文件的话，先复制过去</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp 当前位置及文件 用户名@ip地址：想粘贴的地址</span><br></pre></td></tr></table></figure><p>执行的时候，和最上面不一样了，这是在终端运行，而之前是用&lt;将文件推送到终端，这次是将命令推送到终端</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh 用户名@IP地址 “指令”</span><br><span class="line">例如： $ ssh jianhua@192.168.0.114 &quot;python3 ~/Desktop/a.py&quot;</span><br></pre></td></tr></table></figure><p>如果产生结果，同样可以用scp把结果复制回来</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例如 $ scp morvan@192.168.0.114:~/Desktop/b.py ~/Desktop/result</span><br></pre></td></tr></table></figure><p>流程总结</p><ul><li>本地有要运行的文件</li><li>单个文件的话可以直接 ssh 去云端运行</li><li>多个文件可以先复制去云端, 然后在 ssh 运行</li><li>如果在云端有产生文件, 可以用 scp 复制回来</li></ul><h3 id="共享Linux上的文件"><a href="#共享Linux上的文件" class="headerlink" title="共享Linux上的文件"></a>共享Linux上的文件</h3>]]></content>
      
      
      <categories>
          
          <category> linux学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 2_17-2_23周报</title>
      <link href="/2019/02/23/2019%202_17-2_23%E5%91%A8%E6%8A%A5/"/>
      <url>/2019/02/23/2019%202_17-2_23%E5%91%A8%E6%8A%A5/</url>
      
        <content type="html"><![CDATA[<h3 id="这周完成的事"><a href="#这周完成的事" class="headerlink" title="这周完成的事"></a>这周完成的事</h3><a id="more"></a><h4 id="看了AlexNet、ZFNet、VGG、GoogLenet、Resnet五篇框架的论文"><a href="#看了AlexNet、ZFNet、VGG、GoogLenet、Resnet五篇框架的论文" class="headerlink" title="看了AlexNet、ZFNet、VGG、GoogLenet、Resnet五篇框架的论文"></a>看了AlexNet、ZFNet、VGG、GoogLenet、Resnet五篇框架的论文</h4><h4 id="跑了一次五千米"><a href="#跑了一次五千米" class="headerlink" title="跑了一次五千米"></a>跑了一次五千米</h4><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E8%B7%91%E6%AD%A5%E8%AE%B0%E5%BD%95/Screenshot_2019-02-23-19-18-51-058_com.yuedong.sp.png" alt></p><h4 id="看了三部电影"><a href="#看了三部电影" class="headerlink" title="看了三部电影"></a>看了三部电影</h4><p><img src="https://img3.doubanio.com/view/photo/l/public/p1312700576.webp" alt="当幸福来敲门"></p><p><img src="https://img3.doubanio.com/view/photo/l/public/p2544987866.webp" alt="阿丽塔：战斗天使"></p><p><img src="https://img3.doubanio.com/view/photo/l/public/p2493480880.webp" alt="顽主"></p><h3 id="这周想完成却没有完成的事"><a href="#这周想完成却没有完成的事" class="headerlink" title="这周想完成却没有完成的事"></a>这周想完成却没有完成的事</h3><p>没有</p><h3 id="下周计划"><a href="#下周计划" class="headerlink" title="下周计划"></a>下周计划</h3><p>跑一跑AlexNet和VGG的代码 python还得需要学习并多实际操作</p><p>跑两次五千米</p><p>看电影 大腕 手机</p>]]></content>
      
      
      <categories>
          
          <category> 周报 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ResNet论文解读</title>
      <link href="/2019/02/23/ResNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/02/23/ResNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>经典深度卷积网络的深度至关重要，深度带来的梯度消失问题被标准初始化和中间标准化层解决。另一个问题就是随着深度增加，准确度反而下降。作者觉得理论上网络深度的增加至少不会导致准确度下降（因为后面的层都可以设置成恒等映射），从而引进了残差网络的概念，当准确度不如低层的时候，其他非线性连接的权重偏向于0，恒等映射的权重适当增加。保证准确度至少不会下降。</p><h3 id="贡献点"><a href="#贡献点" class="headerlink" title="贡献点"></a>贡献点</h3><ul><li>在卷积网络中引进旁路（$shortcut$)，也就是深度残差学习框架。简单形式如下图</li></ul><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/Resnet%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt></p><a id="more"></a><h3 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h3><ul><li><p>shortcut connections 旁路连接</p></li><li><p>identity mapping  恒等映射</p></li></ul><h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><ul><li><p>构建块可以被近似地估计为 $y = F(x,W_i) +x$,x和y分别是块的输入和输出向量，函数F就是要学习的残差映射 ，由于x和y必须维度相同才能相加，当维度不同的时候有两种方法来处理。</p><p>（1）零填充</p><p>（2）用投影矩阵改变维度</p></li><li><p>bottleneck design </p><p>由$1 \times 1$的卷积组实现，在GoogLeNet中有用到，能减少计算量和参数量，具体如下图右</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/ResNet%E6%94%B9%E5%8F%98%E7%BB%B4%E5%BA%A6.png" alt></p></li><li><p>残差块至少要有两层及以上，一层没有效果</p></li><li><p>架构图和具体参数如下两图，为了做对照试验，图一中的中和右的参数数目基本一致</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/Resnet%E6%9E%B6%E6%9E%84.png" alt></p></li></ul><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/Resnet%E5%8F%82%E6%95%B0.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GoogLeNet论文解读</title>
      <link href="/2019/02/22/GoogLeNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/02/22/GoogLeNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>提升网络性能一般都是提高网络的深度和宽度，这将带来参数过多，难以训练等问题。本文作者想着用稀疏结构来取代传统的密集连接。由于当下硬件更为支持密集数据的计算，作者提出了<strong>Inception</strong>这一架构近似的最优局部稀疏结构，将稀疏矩阵聚类为相对密集的子矩阵。</p><p><strong>注意：Inception的作用可以近似地解释为：替代了人工确定卷积层中过滤器的类型或者是否创建卷积层和池化层，让网络自己学习它具体需要什么参数。</strong></p><h3 id="贡献点"><a href="#贡献点" class="headerlink" title="贡献点"></a>贡献点</h3><ul><li><p>提出Inception这一全新架构，是稀疏连接的代表</p></li><li><p>多尺度卷积核，最后处理聚合</p></li><li><p>通过引进$1 \times 1$小卷积核来降低维度</p></li><li><p>为了避免梯度消失，在架构中增加了两个辅助的softmax用于向前传导梯度，并以较小的权重（0.3）加到最终分类结果，有些正则化的意思</p></li></ul><a id="more"></a><h3 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h3><p>网络的规模的增大，有两个主要缺点。一是更大的尺寸需要更多的参数，也有着更大的过拟合的可能性。第二个缺点是更大的网络自然需要更大的训练集，而训练集的获得需要很大的成本。深度网络的实用性和性能的增长都陷入了瓶颈。</p><p>作者引进稀疏性来解决以上问题，但是由于当下的计算架构不是很支持稀疏数据结构的数值计算。Inception就是利用滤波器水平的稀疏性，但又利用密集矩阵计算来利用目前的硬件。出于便利性，架构的滤波器仅有$1 \times 1$,$3 \times 3$,$5 \times 5$,由于池化操作的良好表现，又加入一个并行的池化路径。（玄学？）初始架构如下图（a),后加入$1 \times 1​$滤波器来降维，如下图（b).</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/Inception.png" alt="1"></p><h3 id="具体架构"><a href="#具体架构" class="headerlink" title="具体架构"></a>具体架构</h3><p>GoogLeNet是Inception的一个应用，前几层是常见的卷积网络，接下来就是以上所说的Inception结构，最后为了方便泛化到其他数据集还加上一个额外的线性层。虽然移除了全连接层，还是用了70%的dropout策略。（具体在哪用，论文没表述）</p><p>为了有效的训练，同时考虑到中间层产生的特征具有一定的是别离，在中间层加了一些softmax层，并以一定权重直接加到最后的结果中，在后面的控制训练中，发现这些辅助网络影响相对较小，只需要一个就能取得一样的效果。</p><p>训练方法就是正常的BP算法，图像块的采样是随机采样，在图像的$8\%-100\%$上，方向角限制在$[\frac{3}{4},\frac{4}{3}]​$之间，光度扭曲被使用。测试时采用多尺度多方位采样，在比赛中作者采样了144个，在实际中可能不需要这么多数据。</p><p>详细框架如下图</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/googLeNet.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VGG论文解读</title>
      <link href="/2019/02/21/VGG%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/02/21/VGG%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>VGG这篇文章通过增加卷积神经网络的深度和减小卷积核的方法，训练了一个更大的拥有更好效果的卷积神经网络。做了多组对照试验，对卷积网络的深度，训练时数据集的选择，测试时数据集的选择，LRN的效果等都做了探讨。</p><h3 id="贡献点"><a href="#贡献点" class="headerlink" title="贡献点"></a>贡献点</h3><ul><li><p>深度提升了性能</p></li><li><p>采用了小卷积核和小池化核</p></li><li><p>在测试的时候用卷积来代替全连接层</p></li></ul><h3 id="值得注意的一些做法以及解释"><a href="#值得注意的一些做法以及解释" class="headerlink" title="值得注意的一些做法以及解释"></a>值得注意的一些做法以及解释</h3><ul><li><p>小卷积的好处（与大卷积的对比）</p><p>在达到相同感受野的前提下，卷积核越小的卷积参数和计算量都越小。同时更小的卷积核拥有更多的非线性修正层，使决策函数更具有判别性。</p><a id="more"></a><p>另外在论文中，作者提到两个$3 \times 3​$的卷积的感受野大小等于一个$5 \times 5​$的感受野大小，三个$3 \times 3​$的卷积的感受野大小等于一个$7 \times 7​$的感受野大小。大概解释如下图</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/VGG_%E6%84%9F%E5%8F%97%E9%87%8E.png" alt="感受野"></p></li><li><p>预处理时为什么要减去RGB的均值</p><p>​    如果不处理的话，输入层的输入就相对较大，经过BP传播传回来的值就相对较大，这时候的学习率就不能太大，否则会错过局部最小。所以说学习率的选择需要参考输入层的数值，不如直接将数据归一化，就不用对学习率进行过多判断和处理。</p></li><li><p>预训练时能用的部分都用前面低层次网络训练所得的参数</p></li><li><p>多尺度训练时 参数怎么办</p><p>​    多尺度训练是说截取的图片是多尺度，之后还是要归一化为固定的大小如（$224 \times 224)$</p></li><li><p>训练集 验证集和测试集</p><p>​    训练集是用来训练模型内的参数的；验证集一开始还觉得和训练集差不多，其实它是用来调整超参数，不涉及网络权重的调整 ，也用来判断过拟合；测试集，评价模型的泛化能力。</p><p>​    有个比喻，训练集就是上课听老师讲课，验证集是平时做作业，测试集是最后的考试</p></li></ul><h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>设计了六个模型，具体如下图</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/VGG%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0.png" alt="VGG"></p><p>深度递增，共有五个池化层，每次池化之后，卷积核数目增加直到512.卷积大小基本都是$3 \times 3$，步长为1,填充像素为1.在C中验证最小卷积核$1 \times 1$的效果。池化层都是$2 \times 2$大小，步长为2。在A-LRN中，作者验证了AlexNet使用的LRN基本没有用，而且增加无用的计算。</p><p><strong>部分结果：</strong>C的结果不如D,虽然小卷积增加了非线性成分，但是感受域下降了。</p><h3 id="具体训练和测试的操作"><a href="#具体训练和测试的操作" class="headerlink" title="具体训练和测试的操作"></a>具体训练和测试的操作</h3><h4 id="选取训练图像的大小有两种方式。"><a href="#选取训练图像的大小有两种方式。" class="headerlink" title="选取训练图像的大小有两种方式。"></a>选取训练图像的大小有两种方式。</h4><p>第一种就是常用的固定大小尺度的训练，本文选择了两种边的大小（记为S)，一个是256.一个是384，而且为了加速，在训练S=384的网络用了S=256的权重来初始化。</p><p>第二种就是多尺度训练，对每张训练图片的大小在一定范围内采取随机采样S，在一定意义扩充了数据集，并且符合现实中物体大小不一。</p><p><strong>注意</strong>：网络的输入都是224，不管S是多少，最后都要裁剪成$224 \times 224$。（之前还在想随机S,网络结构不得一直变）</p><p><strong>实验结果：</strong>第二种训练方法在实践里有更好的表现</p><h4 id="测试图像的两个可变条件"><a href="#测试图像的两个可变条件" class="headerlink" title="测试图像的两个可变条件"></a>测试图像的两个可变条件</h4><p>第一种条件是是否测试多个尺度。需要决定测试图像的大小Q是否固定。若Q固定的话，在固定的S时，Q=S;不固定S的话，$Q=0.5*(S_{min}+S_{max})$。若Q不固定的话，在S固定时，$Q={S-32,S,S+32}$;S不固定时，$Q={S_{min},0.5 \times(S_{min}+S_{max},S_{max})}$.</p><p><strong>实验结果：</strong>测试时的尺度抖动有更好的结果。</p><p>第二种条件就是对测试图片多个采样或者采用全卷积网络，将全部的全连接层转化为卷积层，后者称为密集评估。不需要对测试图片进行采样。</p><p><strong>实验结果：</strong>多裁剪的处理方式比密集评估效果越好，两者的结合有更好的结果。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ZFNet论文解读</title>
      <link href="/2019/02/19/ZFNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/02/19/ZFNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>ZFNet这篇侧重点在于分析卷积神经网络的工作原理和试图改进，做了不少的对照实验，实现卷积网络可视化。改进了AlexNet，并有了更好的表现。</p><h3 id="1-贡献点"><a href="#1-贡献点" class="headerlink" title="1.贡献点"></a>1.贡献点</h3><ul><li>提出了ZFNet</li><li>用一套的步骤实现了卷积网络的可视化</li><li>利用可视化，分析了卷积神经工作网络的诸多工作原理</li><li>主要的工作有遮挡部分图片和删除部分层做比对实验</li></ul><h3 id="2-具体做法"><a href="#2-具体做法" class="headerlink" title="2.具体做法"></a>2.具体做法</h3><ul><li><p>可视化方法</p><p>一般的卷积层通过卷积，Relus和池化来提取特征，要想反卷积就要一步步实现逆过程</p><p>分别实现Unpooling.Rectification和Filtering</p><p>Unpooling:在池化时候记下当前最大值的位置，在反池化时将最大值标注回来，其他位置填0</p><p>Rectification:为了保证特征有效性，采用Relus,卷积时参数不为负，在反卷积时实行相同步骤即可</p><p>Filtering:现在的卷积核为之前卷积核的转置，就是讲之前的卷积矩阵进行垂直和水平的翻转。</p></li></ul><a id="more"></a><ul><li><p>数据集处理和网络架构</p><p>本文的数据集处理和AlexNet基本相同，但是这个是在一个GPU上训练的。</p><p>在第一层可视化中发现由于卷积步长过大产生了不少混乱的特征，所以将第一层的卷积核由11改为7，将步长由4改为2，其他基本和Alexnet差不多</p></li></ul><h3 id="3-做的对照试验"><a href="#3-做的对照试验" class="headerlink" title="3.做的对照试验"></a>3.做的对照试验</h3><p>首先关于特征可视化，输入图片差异很大但是输出的特征相差很少。在ZFNet中，第一卷积层偏重于颜色，第二卷积层展示了物体的边缘和轮廓，第三层展示相似的纹理，第四层开始体现类与类之间的差异，第五层展示姿态不同的同一类。可以得到结论，层次越高，不变性越强，原始输入图片具有强辨识度可以更好帮助学习。</p><p>特征在训练过程中一直发生变化，经过一定次数的迭代之后，特征才趋于稳定。</p><p>对图片做了平移，旋转和缩放，在一定层次之后，平移和尺度变化对最终结果的影响比较小。</p><p>对不同的部位进行遮挡，当遮挡关键部位时，分类性能和响应强度急剧下降。说明神经网络是通过学习关键部位来进行分类的</p><p>做了图片的部分遮挡探究物体部件之间的关系，发现了深度网络非显式地计算了这些关系。            </p><p>抽取了几个层看各个层的作用，抽取6、7层的全连接层，对整体影响不大，虽然他们有大量的参数；抽取了3、4层两层卷积层，影响也不大。如果抽取3、4、6、7层，效果就急剧下降。改变全连接层的节点个数对分类性能影响不大，改变中间卷积层的节点数对训练结果有帮助，但是加大了过拟合的可能。</p><p>之后仅用少数图片重新训练softmax层，在不同数据集上将imagenet训练所得的卷积网络当做特征提取器。都表现良好，有种迁移学习的感觉。除了PASCAL,因为当前网络只提供一个预测，而PASCAL数据库的图片都有多个物体</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AlexNet论文解读</title>
      <link href="/2019/02/17/AlexNet/"/>
      <url>/2019/02/17/AlexNet/</url>
      
        <content type="html"><![CDATA[<ol><li><h3 id="贡献点"><a href="#贡献点" class="headerlink" title="贡献点"></a>贡献点</h3></li></ol><ul><li><p>训练了一个最大的卷积神经网络（截止这篇文章发表时），并且在识别任务达到了史上最好成绩。</p></li><li><p>编写了一个2D卷积的高度优化的GPU应用</p></li><li><p>为了减少过拟合采用了dropout的方法</p></li></ul><ol><li><h3 id="一些重要名词"><a href="#一些重要名词" class="headerlink" title="一些重要名词"></a>一些重要名词</h3></li></ol><ul><li><p>Relus(Rectified Linear Units),，修正线性单元，代表为max(0,x).用于替代之前传统的激活函数，如tanh(x)或者</p><script type="math/tex; mode=display">f(x)=(1+e^{(-x)})</script> <a id="more"></a></li></ul><ul><li><p>LRN(Local Response Normalization),局部响应归一化。借鉴生物的侧抑制机制，对不同核的输出创造竞争，看公式就是对相邻几个核的结果进行归一化</p><script type="math/tex; mode=display">b^i_{x,y} = a_{x,y}^i / ( k + \alpha \sum _{j = max(0, i-n / 2)} ^{min(N-1, i+n / 2)} (a_{x,y}i)2 )^\beta</script></li><li><p>Overlapping Pooling ,重叠池化，池化是为了减少数据量，传统的池化方法是步长等于池化单元的边长，重叠就是步长比边长短些，可以减少信息损失。</p></li></ul><ul><li><p>卷积层输出大小计算</p><p>设定图片大小为$M \times M ​$,滤波器大小为$F \times F​$,步长为$S​$,padding的像素值$P​$,可得输出图片的大小$N \times N​$,</p><p>其中$N =(W-F+2P)/S+1$.另外卷积层的输出深度是卷积核的个数。</p></li><li><p>在早期训练中，提供正输入加速了学习的早期阶段。</p></li></ul><ol><li><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3></li></ol><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/alexnet%E6%9E%B6%E6%9E%84.png" alt="AlexNet整体架构"></p><p>   如图所示，共有八个参数层，五层卷积层，三层全连接层，最后输送到1000-way的softmax层（关于softmax将会专门写一篇文章）。<strong>注意</strong>：图里的一个个矩阵块层是经过各个层之后得到的中间结果，我们所说的八个层是两个模块之间的那些虚线或者实线。或者说是两块之间的操作。上图有九个层次，分别是原图，和经过八个层处理之后的结构。其中在第一第二层采用LRN,在第一第二第五层之后用了最大池化层。</p><p>本架构采用的激活函数是用Relus，可以加快训练的速度数倍。同时采用多个GPU并行的方法。以下给出整个结构分析（参考[结构详解]: <a href="https://blog.csdn.net/dcrmg/article/details/79241211" target="_blank" rel="noopener">https://blog.csdn.net/dcrmg/article/details/79241211</a> ）</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/alexnet%E7%BB%93%E6%9E%84%E7%B2%BE%E7%AE%80.jpg" alt="精简"></p><p>注意的是第二到第五个卷积采用的是same padding也就是卷积大小不变，这个在论文里好像没讲到</p><ol><li><h3 id="减少过拟合的手段"><a href="#减少过拟合的手段" class="headerlink" title="减少过拟合的手段"></a>减少过拟合的手段</h3></li></ol><ul><li><p>扩大数据集，文中给了两种方法</p><p>第一种方法是通过平移和翻转的方式，在$256 \times 256$的图片上任意截取$224 \times 224$大小的图片进行训练，在测试的时候，分别在四个角和中央取样，和他们的水平翻转，共十个图片进行测试并取平均值。</p><p>第二种方法是改变RGB的强度。有点模仿物体在不同光照下本质不变的性质。</p></li></ul><ul><li><p>dropout</p><p>在每次输入时候，以一定概率对随机的神经元置0，使之既不参加前向传播也不参加后向修正，减少神经元之间的相互协作，使每次输入都是不一样的神经结构。在测试的时候使用全部神经元，但是对于输出乘以一定权重。</p><p>在alexnet中，对前两个全连接层使用了该技术，使该网络收敛的迭代次数增加了一倍，有效地减少过拟合现象。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow基本概念</title>
      <link href="/2019/02/16/TensorFlow%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
      <url>/2019/02/16/TensorFlow%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="tensorflow基本概念"><a href="#tensorflow基本概念" class="headerlink" title="tensorflow基本概念"></a>tensorflow基本概念</h2><p>最近打算重新看一遍一些深度学习的模型，并跑一跑他们的代码，需要重新复习tensorflow并看一看，做此记录。</p><p>tensorflow这个名字包含了自身最重要的两个词，tensor和flow,tensor叫做张量，在tensorflow里可以被简单理解为多维数组，flow表达了张量之间通过计算相互转换。tensorflow是通过计算图的形式表述计算的编程系统，它的每一个计算都可以称为计算图上的一个节点，节点之间的边则描述了计算之间的依赖关系。计算图一般有两个阶段，第一个阶段是定义计算图中的计算，第二个是执行计算。可以设定多个计算图，用来隔离张量，计算甚至设备。</p><a id="more"></a><ul><li><h5 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h5></li></ul><p>实际上张量保存的是对结果的引用，而不是像数组一样，保存数字。一个张量主要保存了三个属性，名字、维度和类型。张量和计算图上所代表的计算结果是对应的。</p><p>张量的用途主要就两个方面，一个是对中间计算结果的引用，二是当计算图构造完成之后，张量可以用来获得计算结果。</p><ul><li><h5 id="会话（session"><a href="#会话（session" class="headerlink" title="会话（session)"></a>会话（session)</h5></li></ul><p>会话拥有并管理tensorflow程序运行时的所有资源，只有在会话里才能真正进行计算。会话需要创建和关闭，可以使用Python的上下文管理器就可以不用去手动关闭。通过ConfigProto可以配置会话的诸多性质。</p><ul><li><h5 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h5></li></ul><p>变量（tf.Variable)的作用是保存和更新神经网络中的参数，变量需要初始化，可以使用随机数，常数或者其他变量的值。一个变量的值在被使用之前，这个变量的初始化过程需要被明确的调用。初始化每一个变量是很麻烦的，如下可以初始化所有变量，当然如何初始化还是需要各自指定。</p><p><code>init_op = tf.global_variables_initializer()</code></p><p><code>sess.run(init_op)</code></p><p>本质来说变量是一种特殊的的张量，因为tf.Variable是一个运算，它的输出结果是一个张量。</p><p>另外可以通过placeholder和feed_dict的配合减少使用常量来表示的次数</p><ul><li><h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5></li></ul><p>交叉熵：常用于分类的损失函数的评估，它表征两个概率分布之间的距离</p><p>均方误差（MSE):常用于回归的损失函数</p><p>可以根据不同问题需求自定义损失函数，将损失函数写入相应位置即可</p><ul><li><h5 id="神经网络进一步优化"><a href="#神经网络进一步优化" class="headerlink" title="神经网络进一步优化"></a>神经网络进一步优化</h5></li></ul><p>可以通过指数衰减的方法设置梯度下降的学习率 <code>tf.train.exponential_decay</code></p><p>为了缓解过拟合问题，常用正则化，在损失函数中加上模型复杂度。实际中，模型复杂度是用所有权重的L1或者L2。相关函数为 <code>tf.contrib.layers.l1(l2)_regularizer()</code>.可以用集合（collection）来处理过大神经网络的参数的正则化。</p><p>滑动平均模型，就是使变量的变化更多的参考于之前的变量。</p><ul><li><h5 id="变量管理"><a href="#变量管理" class="headerlink" title="变量管理"></a>变量管理</h5></li></ul><p>创建变量除了tf.Variable,还有tf.get_variable，后者更注重变量的名称和位置。可通过tf.variable_scope生成一个上下文管理器，配合后者来创建或者获取变量。</p><ul><li><h5 id="模型持久化"><a href="#模型持久化" class="headerlink" title="模型持久化"></a>模型持久化</h5></li></ul><p><code>tf.train.Saver()</code>保存模型。</p>]]></content>
      
      
      <categories>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux私房菜第10章学习总结</title>
      <link href="/2019/02/11/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%AD%A6%E4%B9%A0BASH/"/>
      <url>/2019/02/11/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%AD%A6%E4%B9%A0BASH/</url>
      
        <content type="html"><![CDATA[<h2 id="第十章-认识与学习BASH"><a href="#第十章-认识与学习BASH" class="headerlink" title="第十章 认识与学习BASH"></a>第十章 认识与学习BASH</h2><p>1.什么是shell</p><p>shell的功能是提供用户操作系统的一个接口，要是用户直接操作系统难免有所风险，shell和一般的应用程序的功能应该是在同一层。全面一点说，只能能够操作应用程序的接口都能够称为壳程序。狭义的壳程序指的是指令列方面的软件，广义的可以包括各个图形接口的软件。</p><p>2.当前linux使用的shell版本是bash，是Bourne Shell（sh)的增强版。当前系统可用的shell都会被写入到/etc/shells。</p><p>3.bash的一些优点</p><ul><li>命令记录功能</li><li>命令和文件补全功能<a id="more"></a></li><li>命令别名设定功能</li><li>工作控制、前景背景控制</li><li>程序化脚本（shell scripts）</li><li>通配符</li></ul><p>4.查询指令是否来自bash  type [-tpa] name  例如： type -t ls</p><p>5 输入过长需要换行时，回车键是紧跟反斜杠的[\].</p><p>6.输出变量内容，echo,但是变量前要加上$.</p><p>7.双引号内的特殊字符如 $ 等,可以保有原本的特性,如下所示:<br>    『var=”lang is $LANG”』则『echo $var』可得『lang is zh_TW.UTF-8』，而单引号只能是一般字符，比如$就是$,而不会像前面一样去带入具体的值。</p><p>8.增加path,如『PATH=”$PATH”:/home/bin』或『PATH=${PATH}:/home/bin』</p><p>9.PS1是提示字符的设定，可以修改。就是进入命令行时每行前面有的那些信息</p><p>10.？也是变量，可通过echo $? 得到上一个指令的返回值，0的话一般是正确，否则会有错误代码</p><p>11.export 变量名称 可以把自定义变量转成环境变量</p><p>12.tty1~tty6是无法显示中文，需要加装一些中文化接口的软件.调整编码方式可通过locale.</p><p>13.read,可读用户由键盘读入的数据；declare，宣告变量的类型，可以设置成整数，字符串，环境变量，把环境变量设置为局部变量等。</p><p>14.ulimit 限制文件或者程序的打开数目和大小</p><p>15.可以用#、##、%、%%来改变变量中的一些值的，更具体如下</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E4%BF%AE%E6%94%B9%E5%8F%98%E9%87%8F.png" alt="修改变量"></p><p>16.alias可以设定命令别名，而unalias可以取消别名。</p><p>17.用history可以查看和编辑输入历史，用[!]来执行历史的命令。</p><p>18.由于命令常常有重名的，所以命令执行有顺序</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F.png" alt="命令执行顺序"></p><p>19.从tty登入的bash和从xwindows进入的bash读取的bash文件不一样</p><p>20.通配符</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/bash%E4%B8%80%E4%BA%9B%E9%80%9A%E9%85%8D%E7%AC%A6.png" alt="通配符"></p><p>21.数据流重导向</p><p>顾名思义就是把输入到屏幕的数据输入到相应文件中去</p><p>输入到一个文件中去&gt; ,输入且不覆盖 &gt;&gt;,把错误输出同理2&gt;或者2&gt;&gt;.特殊的是/dev/null是垃圾桶黑洞装置，可以将任何导向这个装置的信息吃掉！</p><p>&lt;和&lt;&lt;是将文件里东西来代替键盘输入。&lt;&lt;后面加字符，表示这个字符为结束输入符。</p><p>22.管线命令 |</p><p>|处理前面一个指令传来的正确信息。这部分内容很多，等用到再了解吧。不然看了印象也不深。</p>]]></content>
      
      
      <categories>
          
          <category> linux学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux私房菜第9章学习总结</title>
      <link href="/2019/02/05/%E7%AC%AC%E4%B9%9D%E7%AB%A0%20VIM%E7%A8%8B%E5%BA%8F%E7%BC%96%E8%BE%91%E5%99%A8/"/>
      <url>/2019/02/05/%E7%AC%AC%E4%B9%9D%E7%AB%A0%20VIM%E7%A8%8B%E5%BA%8F%E7%BC%96%E8%BE%91%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="第九章-VIM程序编辑器"><a href="#第九章-VIM程序编辑器" class="headerlink" title="第九章 VIM程序编辑器"></a>第九章 VIM程序编辑器</h2><p>1.vi有三种模式，一般指令模式，编辑模式，指令列命令模式。其中</p><p>一般指令模式：当打开一个文件的时候，可以进行删除，复制，粘贴等行为，却无法编辑文件内容</p><p>编辑模式：当输入i,I,O,o,A,a,R,r中一个时候，会进入编辑模式，同时画面左下方会出现insert或者replace字样，按下esc退出编辑模式。其中i,o,a都是插入但是插入位置不一样，r是替换。</p><p>指令列命令模式：在一般模式中，输入[:/?]中的一个时，光标将移动到最后一行，此时可以读取，查找，大规模取代，离开vi等大量功能。如：wq是存档离开，：q离开，：q!表示离开不存储。</p><p>2.vi中还有大量快捷键，如[ctrl]+[f]代表下一页,[ctrl]+[b]表示上一页。[30j]表示向下三十列。大写的G常被使用，作用是移动光标位置。<br><a id="more"></a><br>3.vim有暂存档，当文件被不正常终止时，再进入这个文件时，会询问你的下一步操作。</p><p>4.vim可通过v或者[ctrl]+[v]进行区块选择，从而进行一个区块的操作。</p><p>5.多文件编辑，[:n]编辑下一个文件,[:N]编辑上一个文件,[:file]列出当前这个vim的开启的所有文件</p><p>6.[:sp  filename] 可以打开多个窗口，方便比对</p><p>7.vim的补全功能</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/vim%E8%A1%A5%E5%85%A8%E5%8A%9F%E8%83%BD.png" alt="vim补全键"></p><p>8.vim的一些系统设定可以写入到 ～/.vimrc文件中</p>]]></content>
      
      
      <categories>
          
          <category> linux学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux私房菜第6章学习总结</title>
      <link href="/2019/01/31/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E6%96%87%E4%BB%B6%E4%B8%8E%E7%9B%AE%E5%BD%95%E7%AE%A1%E7%90%86/"/>
      <url>/2019/01/31/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E6%96%87%E4%BB%B6%E4%B8%8E%E7%9B%AE%E5%BD%95%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h3 id="第六章-文件与目录管理"><a href="#第六章-文件与目录管理" class="headerlink" title="第六章 文件与目录管理"></a>第六章 文件与目录管理</h3><p>1.根目录的./和../都是本身，.代表此层目录，-表示前一个工作记录</p><p>2.处理目录的常见指令</p><ul><li><p>cd:变换目录</p></li><li><p>pwd:显示当前目录   若 pwd  -P  则不会显示连结档 直接显示正确的完整路径</p></li><li><p>mkdir:建立一个新的目录      加参数-m可以指定权限    加参数-p可以递归建立多层目录</p></li><li><p>rmdir:删除一个空的目录    -p递归删除多层目录 但是这个只能删除空文件夹</p></li><li><p>rm -r ：删除相应目录，包括目录下的所有文件</p></li></ul><p>3.环境变量PATH</p><a id="more"></a><p>跟Windows的环境变量类似，在执行命令的时候会去环境变量中看是否有这个命令。不同身份的path不同。需要注意的是当前目录并没有被加入到系统变量，假如一个变量不在PATh中，你需要指定它的位置才可使用它。（为了安全起见）</p><p>把文件夹加入PATH: <code>PATH=&quot;${PATH}:文件夹位置&quot;</code> </p><p>4.ls 检视指令，同时具备相当多的参数。常用有 -al，-a,-l等等</p><p>5.文件的复制，删除与移动,</p><ul><li><p>复制 cp  cp也有许多参数，具体不表。注意的是一般复制的话权限也是照搬。</p></li><li><p>删除 rm   -f 强制删除 不会出现警告信息      -i  互动模式       -r 递归循环删除</p></li><li><p>移动 mv  -f 强制覆盖   -i 询问是否覆盖    -u只有移动过去的那个比较新才会更新（update），mv也可以用来更目录名 </p></li></ul><p>6.取得路径的文件名和目录名称</p><p>basename和dirname，示例如下</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/basename%E5%92%8Cdirname.png" alt="basename和dirname"></p><p>7.文件内容查阅</p><ul><li><p>cat 由第一行开始显示文件内容   加参数-A可以把一些特殊字符都列出来</p></li><li><p>tac 从最后一行开始显示,可以看出 tac 是 cat 的倒着写!</p></li><li><p>nl 顺道输出行号</p></li><li><p>more 一页一页的显示文件内容    空格键表示下一页 可以用 /字符串 向下搜索相应字符串</p></li><li><p>less 与 more 类似,但是比 more 更好的是,他可以往前翻页 </p></li><li><p>head 只看头几行显示的时候      head -n 加数字</p></li><li><p>tail 只看尾巴几行  比如取一个文件的第十一行到第二十行   head -n 20     /etc/man_db.conf | tail -n 10  </p><p>|是管线的意思 类似于进程吧</p></li><li><p>od以二进制的方式读取文件内容!</p></li></ul><p>8.文件的时间</p><ul><li>mtime(modification):文件内容改变时间</li><li>ctime(status): 状态改变时间，比如属性或者权限</li><li>atime(access): 读取时间<br>题外话：多条命令可用;隔开，会依次执行的 </li><li>touch可以用来改文件的各个时间，或者创建空文件</li></ul><p>9.文件预设权限 umask，默认文件的权限是666，目录的权限是777.而umask指的是减去的分，比如目录的umask=022代表拥有者什么权限都有，但是组和其他用户都不能写。比如出现3,33=2+1，就是去了w和x.设定umask直接设定 umask <em>*</em>就好。</p><p>10.文件隐藏属性 </p><p>配置属性  chattr  查看隐藏属性 lsattr</p><p>特殊属性有 +a  该文件只能增加 不能删除 也不能修改     +i 该文件完全不能改动    还有其他的</p><p>11.文件特殊权限  s和t</p><p>s出现在文件拥有者x的位置上时，称为Set UID,简称为SUID。SUID的具体权限如下：</p><ul><li><p>SUID 权限仅对二进制程序(binary program)有效;</p></li><li><p>执行者对于该程序需要具有 x 的可执行权限;</p></li><li><p>本权限仅在执行该程序的过程中有效 (run-time);</p></li><li><p>执行者将具有该程序拥有者 (owner) 的权限。</p><p>具体比如用户可以改自己的密码，passwd文件就有这个功能，可以帮助用户去修改/etc/shadow里的相应密码，由于passwd的拥有者是root,在一般用户使用passwd就暂时获得了root的权限</p></li></ul><p>当s出现在文件群组的x位置上时，称为SGID,大概与SUID功能相同，只是SGID还可以针对目录来设定，可以修改该目录中的文件</p><p> SBIT是只对目录有效，</p><p>12.设定SUID/SGID/SBIT 一样是通过数字，分别是421，比如在一个为755的文件加SUID功能，那就是4755.全加就是7755.</p><p>13.观察文件类型 file，判断文件类型权限等。</p><p>14.which 或者type 查找脚本放在哪里，其中which是在PATH中有的文件夹进行查找</p><p>15.whereis或者locate搜寻文件档名，相对于find比较快，因为这两个指令只是在一些相关文件夹里查找，当然也有可能找不到。find是搜索整个硬盘，相当慢。不过find的功能是最完善，可带参数是最多的。</p>]]></content>
      
      
      <categories>
          
          <category> linux学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux私房菜第5章学习总结</title>
      <link href="/2019/01/27/%E7%AC%AC%E4%BA%94%E7%AB%A0/"/>
      <url>/2019/01/27/%E7%AC%AC%E4%BA%94%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<h3 id="第五章"><a href="#第五章" class="headerlink" title="第五章"></a>第五章</h3><p>1.linux將文件的存取身份分成三个类别，分别是owner/group/others。操作的权限也有三个rwx(读写执行)</p><p>2.linux中帐号密码群组信息放在/etc/passwd,/etc/shadow和/etc/group中</p><p>3.切换到root,<code>su root</code>，退出<code>exit</code>;刚装完的系统可能没有设<code>root,</code>sudo passed root`:给root设密码</p><p>4.<code>ls</code>:查看文件， <code>ls- al</code>:看当前文件下所有文件 ，ls是list的意思，出来的第一档指令表示类型和三种类别对这个文件各自的权限（读、写、执行），接下来各档也有各自的含义</p><a id="more"></a><p>5.改变文件属性和权限</p><ul><li><p>chgrp :改变文件所属群组    <code>chgrp [-R] dirname/filename</code>: R是递归的意思，改变该文件夹下所有文件的群组  具体例子  <code>chgrp users initial-setup-ks.cfg</code></p></li><li><p>chown :改变文件拥有者   还可以顺带改组名</p><p>chown [-R] 账号名称   文件或目录 例子: <code>chown bin initial-setup-ks.cfg</code><br>chown  [-R] 账号名称:组名   文件或目录  例子: <code>chown root:root initial-setup-ks.cfg</code></p></li><li><p>chmod :改变文件的权限, SUID, SGID, SBIT 等等的特性，有两种方法，数字和符号。</p><p>数字：r:4 w:2 x:1 用它们的和，九个相应权限就只需要三个数字 例子:<code>chmod 777 .bashrc</code></p><p>符号：更具体的方法，u  g   o  a 表示四种身份（a表示all),+或-表示增删，=表示设定，rwx操作权限，例子1  <code>chmod   u=rwx,go=rx   .bashrc</code>    例子2  <code>chmod   a+w   .bashrc</code></p></li></ul><p>6.复制文件：cp 来源文件 目标文件</p><p>7.操作权限对于目录和文件不太一样，具体如下，其中修改文件内容不能对文件本身进行操作，如删除。</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/linux_rwx.png" alt="权限对文件和目录的不同含义"></p><p>8.linux文件种类和拓展名（[]内表示使用[ls -l]出现的第一个字符）</p><p>文件种类</p><p>[-] :正规文件 包括纯文本、二进制文件、数据格式文件<br>        [d] : 目录文件</p><p>[l] :连结档 类似于Windows的快捷方式</p><p>[b]或[c]: 硬件设备</p><p>[s] :数据接口，常被用在网络上的数据承接</p><p>文件扩展名</p><p>.sh  脚本或者批处理文件</p><p><em>Z, </em>.tar, <em>.tar.gz, </em>.zip, *.tgz: 经过打包的压缩文件</p><p><em>.html, </em>.php:网页相关文件</p><p>linux的后缀和Windows不大一样，能不能执行还是要看权限</p><p>9.linux目录配置</p><p>现在的目录配置方法基本有统一的标准，即FHS.代表性定义如下图</p><p><img src="http://guojianhua-site.oss-cn-shenzhen.aliyuncs.com/FHS.png" alt="FHS常用定义"></p><p>实际上具体就定义出以下三个目录</p><ul><li><p>/ (root, 根目录):与开机系统有关;</p></li><li><p>/usr (unix software resource):与软件安装/执行有关;</p></li><li><p>/var (variable):与系统运作过程有关。</p><p>/是一个系统的根目录，由于其重要性，所以不建议将其他无关文件放入/，FHS建议在/再进行不同的分目录实现各自的功能。</p></li></ul><p>10./usr和/var 这两个文件都是在/下</p><p>/usr不是user的缩写，是Unix Software Resource,也就是存放软件资源的地方。所有的系统软件都放在这</p><p>/var针对的变动的文件，某些软件运行产生的文件或者缓存都放在这</p><ol><li>./表示当前目录，../表示上一层目录</li></ol><p>​    </p>]]></content>
      
      
      <categories>
          
          <category> linux学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux私房菜第0到4章学习总结</title>
      <link href="/2019/01/23/linux%E7%A7%81%E6%88%BF%E8%8F%9C%E7%AC%AC0%E5%88%B04%E7%AB%A0%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
      <url>/2019/01/23/linux%E7%A7%81%E6%88%BF%E8%8F%9C%E7%AC%AC0%E5%88%B04%E7%AB%A0%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>CPU架构分类，精简指令集（RISC)和复杂指令集（CISC）。RISC多用在比较小的设备，如手机常用的ARM,CISC多用于电脑和工作站，如AMD和Inter的x86架构。x86是因为intel的前几代cpu代号都是86xx的。（鸟哥的0和1章讲的大多是体系结构的东西和发展史，大概过一下)</p><h3 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h3><p>1.各个组件或者装置在linux底下都是一个文件</p><p>2.在Linux中，所有的硬件裝置文件都在/dev這個目錄中，sata或usb常被识别为/dev/sd[a-p]</p><a id="more"></a><p>3.磁盘的分区格式有两种，MBR和GPT,分别对应boot方式，legacy和uefi，之前我装nvidia的驱动装两好久就是因爲做盘的方式就做错了。</p><p>4.MBR,管理开机管理程序（446bytes)和分区表(64bytes)，都放在磁盘的第一个扇区,由于分区表大小限制，一个盘所以只能分成四个区。但是可以通过延伸分区（extended）来增加分区（但是明显这样的话，延伸分区最多只能有四个，分区表指向他们，他们再指向逻辑分区），这些增加的分区称爲逻辑分区槽。由分区指定的叫主分区。</p><p>5.GPT(GUID)，升级，逻辑上没有分区数目的限制。grub2不认识gpt</p><p>6.如果要安装多重引导, 最好先安装 Windows 再安装 Linux。因为:Linux 在安装的时候,你可以选择将开机管理程序安装在 MBR 或各别分区槽的启动扇区, 而且 Linux 的loader 可以手动设定选单所以可以在 Linux 的 boot loader 里面加入 Windows 开机的选项。Windows 在安装的时候,他的安装程序会主动的覆盖掉 MBR 以及自己所在分区槽的启动扇区,你没有选择的机会, 而且他没有让我们自己选择选单的功能。</p><p>7.开机的流程由:BIOS—&gt;MBR—&gt;boot loader（引导啓动程序）—&gt;核心文件，boot loader 的功能主要有:提供选单、加载核心、转交控制权给其他 loaderboot loader 可以安装的地点有两个,分别是 MBR 与 boot sector</p><p>8.所谓的『挂载』就是利用一个目录当成进入点,将磁盘分区槽的数据放置在该目录下; 也就是说,<br>进入该目录就可以读取该分区槽的意思。把硬盘的东西挂载到目录下。</p><h3 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h3><p>1.给笔记本安装linux时需要叫入acpi=off(我装ubuntu的时候是在splash后加上 nomodest),因为笔记本和台式的电源管理模块不一样。</p><p>2.Windows 8.1 以前的版本,不能够在非 UEFI 的 BIOS 环境下使用 GPT 分区表的分区槽来开机。</p><p>3.装linux时，分区大概可以有/，/boot,/home,swap四个分区,其中/和swap是必须的</p><p>注：本章说的比较多的centos的安装，基本跳过。</p><h3 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h3><p>1.默认root的提示字符是#，一般用户的提示字符是$</p><p>2.[ctrl]+[alt]+[F1-f6]进入文本系统，但是不同电脑细节不一样，多试一试。exit退出当前tty的登录帐号</p><p>3.指令一般格式command   [-options]     parameter1      parameter2 。选项没有[]，而且一般带-或者—，如-h,—help。指令太长的时候,可以使用反斜杠 () 来跳脱[Enter]符号,使指令连续到下一行。</p><p>4.linux下大小写敏感。</p><p>5.指令列模式里面下达指令时,会有两种主要的情况:一种是该指令会直接显示结果然后回到命令提示字符等待下一个指令的输入;一种是进入到该指令的环境,直到结束该指令才回到命令提示字符的环境。</p><p>6.命令行的快捷键</p><p>​     (1)[tab]     命令补齐，文件补齐，甚至参数补齐。有单[tab]和双[tab].[Tab] </p><p>​     接在一串指令的第一个字的后面,则为『命令补全』   </p><p>​     接在一串指令的第二个字以后时,则为『文件补齐』 (在-或—后面)<br>​            (2)[ctrl]+[c]  中断目前程序</p><p>​    (3)[ctrl]+[d]  代表键盘输入结束，在文字接口相当于输入exit</p><p>​    (4)[shift]+{[PageUP]|[Page Down]}   在命令行的上一页或下一页 相当于鼠标滚轮</p><p>7.指令系统的提醒   命令加[—help]，可以得到这个命令的使用方法等等，</p><p> 8.man +命令 可以出现更具体的说明文档，空格键表示下一页 [q]表示退出 。其中在指令后面出现数字（1到9），代表不同指令的不同地位身份。在出现man (manual)时，按下[/] 可以输入字符在man page里搜索，按[n]表示搜索的下一个。（next应该是）</p><p>9.info+命令也是查找命令的详细信息，相对于man它有更多的节点，有点像网站，</p><p>10.nano一个简单文本编辑器，指数符号代表[crtl], [M]是代表[alt]</p><p>11.关机前准备  who 看谁在线        ps -aux 知道主机目前的使用状态  sync 将当前的数据写进磁盘</p><p>12.关机有多个指令 基本差不多    shutdown halt poweroff</p>]]></content>
      
      
      <categories>
          
          <category> linux学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开场白</title>
      <link href="/2019/01/22/%E5%BC%80%E5%9C%BA%E7%99%BD/"/>
      <url>/2019/01/22/%E5%BC%80%E5%9C%BA%E7%99%BD/</url>
      
        <content type="html"><![CDATA[<p>研一上结束了，囫囵吞枣地学了点东西，开个博客系统的汇报和总结自己做的东西吧。暂时定位为自己的记录博客。如果你恰巧找到这，并且发现对你有用的东西（这也太渺茫了吧），那也是我的荣幸。</p><p>先给自己的博客做个大概基调吧，学习为主。Hexo有分类和标签，分类的话，我打算大体分成自白与反思，精神生活（比如电影或书籍的观后感），周报（现在开始给自己定的小目标），linux学习，深度学习，计算机视觉，开发（这个先占个坑）。标签的话就具体一点比如tensorflow，电影，hexo等等。尽量做到每周写周报。</p><p>近期目标的话，回家前，把鸟哥的liunx私房菜看一看实际操作一下。回家的话，把那些经典的深度学习网络结构重新学习，有条件的话跑一些代码。</p>]]></content>
      
      
      <categories>
          
          <category> 自白与反思 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 废话 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
